{"config":{"indexing":"full","lang":["en"],"min_search_length":3,"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"(Image copyright Ann Althouse, licensed under Creative Commons, http://www.flickr.com/photos/althouse/141467660/ ) OSG User School 2017 \u00b6 Could you transform your research with vast amounts of computing? Come spend a week at the beautiful University of Wisconsin\u2013Madison and learn how During the school, 17\u201321 July, you will learn to use high-throughput computing (HTC) systems \u2014 at your own campus or using the national Open Science Grid (OSG) \u2014 to run large-scale computing applications that are at the heart of today\u2019s cutting-edge science. Through lectures, discussions, and lots of hands-on activities with experienced OSG staff, you will learn how HTC systems work, how to run and manage lots of jobs and huge datasets to implement a scientific computing workflow, and where to turn for more information and help. Take a look at the high-level curriculum and syllabus for more details. The school is ideal for graduate students in any science or research domain where large-scale computing is a vital part of the research process, plus we will consider applications from advanced undergraduates, post-doctoral students, faculty, and staff. Students accepted to this program will receive financial support for basic travel and local costs associated with the School. Applications \u00b6 Applications closed on 17 April 2017. We plan to notify applicants of our decisions by early May. Thanks for your interest! Information for Participants \u00b6 Curriculum High-level curriculum Detailed schedule Materials Evening work sessions (optional) Final assignment Logistics General information and travel schedule Visa requirements for non-resident aliens Travel planning to and from Madison Hotel information Local transportation within Madison School location Meals at the School Fun things to do in Madison and Wednesday activities Reimbursements OSG User School 2017 group photo Contact Us \u00b6 The OSG User School is part of the OSG Outreach Area \u2014 please visit that site to learn about other OSG Outreach activities. If you have any questions about the School, the application process, or anything else, feel free to email us: user-school@opensciencegrid.org OSGUserSchool","title":"User School 2017"},{"location":"#osg-user-school-2017","text":"Could you transform your research with vast amounts of computing? Come spend a week at the beautiful University of Wisconsin\u2013Madison and learn how During the school, 17\u201321 July, you will learn to use high-throughput computing (HTC) systems \u2014 at your own campus or using the national Open Science Grid (OSG) \u2014 to run large-scale computing applications that are at the heart of today\u2019s cutting-edge science. Through lectures, discussions, and lots of hands-on activities with experienced OSG staff, you will learn how HTC systems work, how to run and manage lots of jobs and huge datasets to implement a scientific computing workflow, and where to turn for more information and help. Take a look at the high-level curriculum and syllabus for more details. The school is ideal for graduate students in any science or research domain where large-scale computing is a vital part of the research process, plus we will consider applications from advanced undergraduates, post-doctoral students, faculty, and staff. Students accepted to this program will receive financial support for basic travel and local costs associated with the School.","title":"OSG User School 2017"},{"location":"#applications","text":"Applications closed on 17 April 2017. We plan to notify applicants of our decisions by early May. Thanks for your interest!","title":"Applications"},{"location":"#information-for-participants","text":"Curriculum High-level curriculum Detailed schedule Materials Evening work sessions (optional) Final assignment Logistics General information and travel schedule Visa requirements for non-resident aliens Travel planning to and from Madison Hotel information Local transportation within Madison School location Meals at the School Fun things to do in Madison and Wednesday activities Reimbursements OSG User School 2017 group photo","title":"Information for Participants"},{"location":"#contact-us","text":"The OSG User School is part of the OSG Outreach Area \u2014 please visit that site to learn about other OSG Outreach activities. If you have any questions about the School, the application process, or anything else, feel free to email us: user-school@opensciencegrid.org OSGUserSchool","title":"Contact Us"},{"location":"curriculum/assignment/","text":"OSG User School 2017 Final Assignment \u00b6 The School focused on using high-throughput computing (HTC) to support and transform scientific inquiry. Your final assignment is to apply your new knowledge to a challenge in your scientific domain that requires significant computation. The assignment is useful, because it: Reinforces and consolidates what you learned Prepares you to take real action on your large-scale computational challenge(s) Demonstrates the value of the School to our funding agencies and to your advisor, colleagues, etc. Guides us as we try to improve the School Content \u00b6 First, choose a challenge or project to present. An ideal topic: Is important to you and your advisor, team, department, or field Represents work that is in progress or is planned to start soon Requires significant computational resources Next, think about your topic and how to apply what you learned during the School. Think about how to approach the computational needs of the project using local HTC resources or the Open Science Grid (OSG). We are not asking you to implement the system! Just imagine how you would do it. One approach is fine, but more than one approach is fine, too. Imagine that you will run on the resources available to you at your own institution. If your institution does not have a HTC system available, then think about what kind of resources you would want or how you could get access to resources via the OSG. Your final assignment will be a written report (see below for more detail). Please address all of the following questions in your submission: The science challenge ( about 1/3 of the assignment ) \u2014 described in a way that smart people outside of your field can understand What science do you work on? What specific challenge do you (want to) work on? Why does that work require significant computing resources to solve? The computational plan ( about 2/3 of the assignment ) Summarize your approach and explain why you think it is good Estimate the resources (CPUs, time, memory, disk, network, etc.) that you need Describe in some detail your plan or proposal to use computing tools to work on your challenge (more than one plan is OK) Refer back to specific lectures and exercises in the School, when appropriate There are many possible questions your paper could address. Below are some suggestions \u2014 feel free to answer some (or all) of them, or create and answer your own interesting questions: What local resources do you have access to? Would you use just local resources or do you need remote resources, too? How would you turn your project into actual jobs? What are the resource needs of the jobs themselves? What sort of workflow, if any, would you use? Are there manual steps in your overall workflow? Could they be automated (e.g., with DAGMan)? How much data do you need to move around? Which type of data situation do you have? What is your plan for data management? Do you think your project is better suited for HTC or HPC? Why? What security or privacy concerns do you have with your project? Do you need to do anything special regarding security? How would your science be transformed by increasing the amount of computation you can use? Format \u00b6 You will write a short paper. You can think of the format as an informal whitepaper, research paper, or proposal, if that helps organize your paper. Please represent yourself and your work well! Your final assignment may be posted on the School website and hence be available to the public. There are no precise length requirements, but 1000\u20131500 words is a good length to aim for Pictures, charts, and diagrams are good, if they are appropriate and clear The paper does not need to be journal-ready, but should be good quality and ready for public display Submit your paper in as a PDF \u2014 no Word or LaTeX documents, please! If you are not sure how to make a PDF, consult your department or campus IT staff for help Email the PDF to the user-school@opensciencegrid.org list If the PDF is really huge, contact us and we will find another way to transfer the file (we should be able to manage large data, right??) Deadline \u00b6 The paper is due 31 August 2017. We will consider individual requests for a time extension, but you need a good reason. Talk to us about the deadline, if it seems like a problem. Questions? \u00b6 If you have any questions or comments about the assignment, please contact us at the user-school@opensciencegrid.org mailing list.","title":"Final assignment"},{"location":"curriculum/assignment/#osg-user-school-2017-final-assignment","text":"The School focused on using high-throughput computing (HTC) to support and transform scientific inquiry. Your final assignment is to apply your new knowledge to a challenge in your scientific domain that requires significant computation. The assignment is useful, because it: Reinforces and consolidates what you learned Prepares you to take real action on your large-scale computational challenge(s) Demonstrates the value of the School to our funding agencies and to your advisor, colleagues, etc. Guides us as we try to improve the School","title":"OSG User School 2017 Final Assignment"},{"location":"curriculum/assignment/#content","text":"First, choose a challenge or project to present. An ideal topic: Is important to you and your advisor, team, department, or field Represents work that is in progress or is planned to start soon Requires significant computational resources Next, think about your topic and how to apply what you learned during the School. Think about how to approach the computational needs of the project using local HTC resources or the Open Science Grid (OSG). We are not asking you to implement the system! Just imagine how you would do it. One approach is fine, but more than one approach is fine, too. Imagine that you will run on the resources available to you at your own institution. If your institution does not have a HTC system available, then think about what kind of resources you would want or how you could get access to resources via the OSG. Your final assignment will be a written report (see below for more detail). Please address all of the following questions in your submission: The science challenge ( about 1/3 of the assignment ) \u2014 described in a way that smart people outside of your field can understand What science do you work on? What specific challenge do you (want to) work on? Why does that work require significant computing resources to solve? The computational plan ( about 2/3 of the assignment ) Summarize your approach and explain why you think it is good Estimate the resources (CPUs, time, memory, disk, network, etc.) that you need Describe in some detail your plan or proposal to use computing tools to work on your challenge (more than one plan is OK) Refer back to specific lectures and exercises in the School, when appropriate There are many possible questions your paper could address. Below are some suggestions \u2014 feel free to answer some (or all) of them, or create and answer your own interesting questions: What local resources do you have access to? Would you use just local resources or do you need remote resources, too? How would you turn your project into actual jobs? What are the resource needs of the jobs themselves? What sort of workflow, if any, would you use? Are there manual steps in your overall workflow? Could they be automated (e.g., with DAGMan)? How much data do you need to move around? Which type of data situation do you have? What is your plan for data management? Do you think your project is better suited for HTC or HPC? Why? What security or privacy concerns do you have with your project? Do you need to do anything special regarding security? How would your science be transformed by increasing the amount of computation you can use?","title":"Content"},{"location":"curriculum/assignment/#format","text":"You will write a short paper. You can think of the format as an informal whitepaper, research paper, or proposal, if that helps organize your paper. Please represent yourself and your work well! Your final assignment may be posted on the School website and hence be available to the public. There are no precise length requirements, but 1000\u20131500 words is a good length to aim for Pictures, charts, and diagrams are good, if they are appropriate and clear The paper does not need to be journal-ready, but should be good quality and ready for public display Submit your paper in as a PDF \u2014 no Word or LaTeX documents, please! If you are not sure how to make a PDF, consult your department or campus IT staff for help Email the PDF to the user-school@opensciencegrid.org list If the PDF is really huge, contact us and we will find another way to transfer the file (we should be able to manage large data, right??)","title":"Format"},{"location":"curriculum/assignment/#deadline","text":"The paper is due 31 August 2017. We will consider individual requests for a time extension, but you need a good reason. Talk to us about the deadline, if it seems like a problem.","title":"Deadline"},{"location":"curriculum/assignment/#questions","text":"If you have any questions or comments about the assignment, please contact us at the user-school@opensciencegrid.org mailing list.","title":"Questions?"},{"location":"curriculum/detailed-schedule/","text":"table.schedule { border-collapse: collapse; margin: 1em 0 1em 2em; } th, td { padding: 2px 0; vertical-align: top; } th + th, th + td, td + td { padding-left: 1em; } th.side { text-align: right; } td.time { text-align: right; } .hi { color: \\#F60; /\\* font-weight: bold; \\*/ } tr.meal { background-color: \\#FFEEBF; } tr.break { background-color: \\#D9F0FF; } 2017 OSG User School Schedule \u00b6 The schedule is in progress at this time. All events happen in the Computer Sciences building (1210 West Dayton Street), Room 1240, except when shown in orange below. Sunday, 16 July 2017 \u00b6 Welcome Dinner for Students and Staff Location Fluno Center (601 University Avenue) , Skyview Room Time 6:30 p.m. We will have instructors at the hotel lobby by ~5:45pm. At about 6:00 p.m., we will walk to the Fluno Center, which will take 25 minutes. Join us for the walk if you want, or otherwise meet us in the dinner room by 6:30 p.m. Monday, 17 July 2017 \u00b6 Main Idea: High Throughput Computing at a Local Site Start End Event Instructor 8:00 8:45 Breakfast and group discussion 9:00 9:15 Welcome Tim Cartwright 9:15 9:45 Introduction to High Throughput Computing (lecture) Lauren Michael 9:45 10:30 Running jobs locally with HTCondor, Part 1 (hands-on) \" 10:30 10:45 Break 10:45 11:15 Intermediate HTCondor (lecture) Lauren Michael 11:15 12:15 Running jobs locally with HTCondor, Part 2 (hands-on) \" 12:15 1:15 Lunch 1:15 1:45 Introduction to workflows (lecture) Lauren Michael 1:45 3:15 Running workflows locally with HTCondor and DAGMan, Part 1 (hands-on) \" 3:15 3:30 Break 3:30 4:00 Intermediate workflows (lecture) Lauren Michael 4:00 5:00 Running workflows locally with HTCondor and DAGMan, Part 2 (hands-on) \" 5:00 \u2014 On your own 7:00 9:00 \\[Optional\\] [Evening work session](../logistics/wednesday-activities.md) Union South, 2nd floor, Wisconsin Idea room Bala, Derek, Lauren, Tim Tuesday, 18 July 2017 \u00b6 Main Ideas: Distributed High Throughput Computing; Security; Connecting to OSG Start End Event Instructor 8:00 8:45 Breakfast and group discussion 9:00 9:30 Introduction to DHTC and overlay systems (lecture) Brian Lin 9:30 10:30 Interacting with overlay systems (hands-on) \" 10:30 10:45 Break 10:45 11:15 What's different about overlay systems? (lecture) Brian Lin 11:15 12:15 Seeing differences with overlay systems (hands-on) \" 12:15 1:15 Lunch 1:15 2:25 High throughput computing in action (interactive) Group 2:25 2:40 Security in the OSG (lecture) Brian Lin 2:40 2:55 Break 2:55 3:25 Troubleshooting (lecture) Brian Lin 3:25 4:30 Troubleshooting (hands-on) \" 4:30 5:00 Ways to connect to OSG (lecture) Lauren Michael 5:00 \u2014 On your own 7:00 9:00 \\[Optional\\] Evening work session Union South, 2nd floor, Wisconsin Idea room Bala, Christina, Derek Wednesday, 19 July 2017 \u00b6 Main Ideas: Software Portability + take a break Start End Event Instructor 8:00 8:45 Breakfast and group discussion 9:00 9:30 Software portability considerations Christina Koch 9:30 10:30 Compiled software and wrapper scripts (hands-on) \" 10:30 10:45 Break 10:45 11:15 Licensing challenges and research software (lecture) Christina Koch 11:15 12:15 Matlab and Python portability (hands-on) \" 12:15 1:15 Lunch 1:15 5:00 [Free Choice](../materials/day3/part3-activities.md) (with options provided) Group 5:00 \u2014 On your own 7:00 9:00 \\[Optional\\] Evening work session Union South, 2nd floor, Wisconsin Idea room Derek, Tim Thursday, 20 July 2017 \u00b6 Main Ideas: OSG Connect; Data Handling Start End Event Instructor 8:00 8:45 Breakfast and group discussion 9:00 9:30 OSG Connect (lecture) Bala Desinghu 9:30 10:45 Using OSG Connect and OASIS \" 10:45 11:00 Break 11:00 11:30 HTC considerations for \"big data\"? (lecture) Derek Weitzel 11:30 12:25 Determining data requirements (hands-on) \" 12:25 1:15 Lunch 1:15 1:45 Solutions for large input data (lecture) Derek Weitzel 1:45 3:15 Handling large input data (hands-on) \" 3:15 3:30 Break 3:30 4:00 Large output and local file systems (lecture) Derek Weitzel 4:00 5:00 Leveraging shared file systems (hands-on) \" 5:00 \u2014 On your own 7:00 9:00 \\[Optional\\] Evening work session Memorial Union (800 Langdon St.), Terrace (outdoors) Backup location: Memorial Union, Der Rathskeller (room) Bala, Brian, Derek Friday, 21 July 2017 \u00b6 Main Ideas: Science Workflows; High-Throughput Computing; HTC Showcase Start End Event Instructor 8:00 8:45 Breakfast and group discussion 9:00 9:30 From science to workflow (lecture) Christina Koch 9:30 10:15 Workflow development (hands-on) \" 10:15 10:30 Break 10:30 10:50 From workflow to production (lecture) Christina Koch 10:50 11:55 Workflow execution (hands-on) \" 11:55 12:40 Lunch 12:40 1:05 Group photo; Tour of Wisconsin Institutes for Discovery Lauren/Christina 1:05 1:15 Walk back to Comp Sci **1:15** **2:55** **High-throughput computing showcase** 1:20 1:40 *HTC for Materials Databases and Materials Design* Dane Morgan , Engineering 1:45 2:05 *Using HTC in Genomic Ancestry Analysis* Megan Frayer , Genetics 2:10 2:30 *Building a Character Table Database: A Use of Condor in Pure Math.* William Cocke , Mathematics 2:35 2:55 *HTC As a Tool to Study How Plant Genomes Function* Edgar Spalding , Botany 2:55 3:25 Break and Q&A 3:25 4:40 Principles for high-throughput computing (lecture) Miron Livny 4:40 4:45 Short Break 4:45 5:15 Where to go and what to do next? Final assignment Tim Cartwright 5:15 6:30 On your own Closing Dinner for Students and Staff Location Union South (1208 Dayton Street) , search the \u201cToday In The Union\u201d (TITU) display for the exact room Time 6:30 p.m.","title":"Detailed schedule"},{"location":"curriculum/detailed-schedule/#2017-osg-user-school-schedule","text":"The schedule is in progress at this time. All events happen in the Computer Sciences building (1210 West Dayton Street), Room 1240, except when shown in orange below.","title":"2017 OSG User School Schedule"},{"location":"curriculum/detailed-schedule/#sunday-16-july-2017","text":"Welcome Dinner for Students and Staff Location Fluno Center (601 University Avenue) , Skyview Room Time 6:30 p.m. We will have instructors at the hotel lobby by ~5:45pm. At about 6:00 p.m., we will walk to the Fluno Center, which will take 25 minutes. Join us for the walk if you want, or otherwise meet us in the dinner room by 6:30 p.m.","title":"Sunday, 16 July 2017"},{"location":"curriculum/detailed-schedule/#monday-17-july-2017","text":"Main Idea: High Throughput Computing at a Local Site Start End Event Instructor 8:00 8:45 Breakfast and group discussion 9:00 9:15 Welcome Tim Cartwright 9:15 9:45 Introduction to High Throughput Computing (lecture) Lauren Michael 9:45 10:30 Running jobs locally with HTCondor, Part 1 (hands-on) \" 10:30 10:45 Break 10:45 11:15 Intermediate HTCondor (lecture) Lauren Michael 11:15 12:15 Running jobs locally with HTCondor, Part 2 (hands-on) \" 12:15 1:15 Lunch 1:15 1:45 Introduction to workflows (lecture) Lauren Michael 1:45 3:15 Running workflows locally with HTCondor and DAGMan, Part 1 (hands-on) \" 3:15 3:30 Break 3:30 4:00 Intermediate workflows (lecture) Lauren Michael 4:00 5:00 Running workflows locally with HTCondor and DAGMan, Part 2 (hands-on) \" 5:00 \u2014 On your own 7:00 9:00 \\[Optional\\] [Evening work session](../logistics/wednesday-activities.md) Union South, 2nd floor, Wisconsin Idea room Bala, Derek, Lauren, Tim","title":"Monday, 17 July 2017"},{"location":"curriculum/detailed-schedule/#tuesday-18-july-2017","text":"Main Ideas: Distributed High Throughput Computing; Security; Connecting to OSG Start End Event Instructor 8:00 8:45 Breakfast and group discussion 9:00 9:30 Introduction to DHTC and overlay systems (lecture) Brian Lin 9:30 10:30 Interacting with overlay systems (hands-on) \" 10:30 10:45 Break 10:45 11:15 What's different about overlay systems? (lecture) Brian Lin 11:15 12:15 Seeing differences with overlay systems (hands-on) \" 12:15 1:15 Lunch 1:15 2:25 High throughput computing in action (interactive) Group 2:25 2:40 Security in the OSG (lecture) Brian Lin 2:40 2:55 Break 2:55 3:25 Troubleshooting (lecture) Brian Lin 3:25 4:30 Troubleshooting (hands-on) \" 4:30 5:00 Ways to connect to OSG (lecture) Lauren Michael 5:00 \u2014 On your own 7:00 9:00 \\[Optional\\] Evening work session Union South, 2nd floor, Wisconsin Idea room Bala, Christina, Derek","title":"Tuesday, 18 July 2017"},{"location":"curriculum/detailed-schedule/#wednesday-19-july-2017","text":"Main Ideas: Software Portability + take a break Start End Event Instructor 8:00 8:45 Breakfast and group discussion 9:00 9:30 Software portability considerations Christina Koch 9:30 10:30 Compiled software and wrapper scripts (hands-on) \" 10:30 10:45 Break 10:45 11:15 Licensing challenges and research software (lecture) Christina Koch 11:15 12:15 Matlab and Python portability (hands-on) \" 12:15 1:15 Lunch 1:15 5:00 [Free Choice](../materials/day3/part3-activities.md) (with options provided) Group 5:00 \u2014 On your own 7:00 9:00 \\[Optional\\] Evening work session Union South, 2nd floor, Wisconsin Idea room Derek, Tim","title":"Wednesday, 19 July 2017"},{"location":"curriculum/detailed-schedule/#thursday-20-july-2017","text":"Main Ideas: OSG Connect; Data Handling Start End Event Instructor 8:00 8:45 Breakfast and group discussion 9:00 9:30 OSG Connect (lecture) Bala Desinghu 9:30 10:45 Using OSG Connect and OASIS \" 10:45 11:00 Break 11:00 11:30 HTC considerations for \"big data\"? (lecture) Derek Weitzel 11:30 12:25 Determining data requirements (hands-on) \" 12:25 1:15 Lunch 1:15 1:45 Solutions for large input data (lecture) Derek Weitzel 1:45 3:15 Handling large input data (hands-on) \" 3:15 3:30 Break 3:30 4:00 Large output and local file systems (lecture) Derek Weitzel 4:00 5:00 Leveraging shared file systems (hands-on) \" 5:00 \u2014 On your own 7:00 9:00 \\[Optional\\] Evening work session Memorial Union (800 Langdon St.), Terrace (outdoors) Backup location: Memorial Union, Der Rathskeller (room) Bala, Brian, Derek","title":"Thursday, 20 July 2017"},{"location":"curriculum/detailed-schedule/#friday-21-july-2017","text":"Main Ideas: Science Workflows; High-Throughput Computing; HTC Showcase Start End Event Instructor 8:00 8:45 Breakfast and group discussion 9:00 9:30 From science to workflow (lecture) Christina Koch 9:30 10:15 Workflow development (hands-on) \" 10:15 10:30 Break 10:30 10:50 From workflow to production (lecture) Christina Koch 10:50 11:55 Workflow execution (hands-on) \" 11:55 12:40 Lunch 12:40 1:05 Group photo; Tour of Wisconsin Institutes for Discovery Lauren/Christina 1:05 1:15 Walk back to Comp Sci **1:15** **2:55** **High-throughput computing showcase** 1:20 1:40 *HTC for Materials Databases and Materials Design* Dane Morgan , Engineering 1:45 2:05 *Using HTC in Genomic Ancestry Analysis* Megan Frayer , Genetics 2:10 2:30 *Building a Character Table Database: A Use of Condor in Pure Math.* William Cocke , Mathematics 2:35 2:55 *HTC As a Tool to Study How Plant Genomes Function* Edgar Spalding , Botany 2:55 3:25 Break and Q&A 3:25 4:40 Principles for high-throughput computing (lecture) Miron Livny 4:40 4:45 Short Break 4:45 5:15 Where to go and what to do next? Final assignment Tim Cartwright 5:15 6:30 On your own Closing Dinner for Students and Staff Location Union South (1208 Dayton Street) , search the \u201cToday In The Union\u201d (TITU) display for the exact room Time 6:30 p.m.","title":"Friday, 21 July 2017"},{"location":"curriculum/evening-sessions/","text":"Optional Evening Sessions \u00b6 The evening sections are dedicated to individual help from the instructors. Please, join our very knowledgeable instructors, work on catching up, extra optional exercises, or your own research projects. This is a great time to ask lots of questions, too! Details \u00b6 | Days: | Monday through Thursday | | Times: | 7\u20139 p.m. | | Location: | Mon, Tue, Wed: http://www.union.wisc.edu/visit-unionsouth.htm Union South , 1308 West Dayton Street , right next to the Computer Sciences building | Thur: The Terrace at Memorial Union , 800 Langdon St, , Outside if the weather is nice, inside else Room: Check the \u201cToday In The Union\u201d (TITU) electronic displays each day, as the exact room may vary. TITU: http://union.ems.wisc.edu/VirtualEMS/CustomBrowseEvents.aspx?data=MeN0rP%2FRPNoJS2WjBImM5NV%2B64hv0ZxW Map: https://apps.union.wisc.edu/materials/Visit_BuildingMap_US.pdf#Page=4","title":"Evening sessions"},{"location":"curriculum/evening-sessions/#optional-evening-sessions","text":"The evening sections are dedicated to individual help from the instructors. Please, join our very knowledgeable instructors, work on catching up, extra optional exercises, or your own research projects. This is a great time to ask lots of questions, too!","title":"Optional Evening Sessions"},{"location":"curriculum/evening-sessions/#details","text":"| Days: | Monday through Thursday | | Times: | 7\u20139 p.m. | | Location: | Mon, Tue, Wed: http://www.union.wisc.edu/visit-unionsouth.htm Union South , 1308 West Dayton Street , right next to the Computer Sciences building | Thur: The Terrace at Memorial Union , 800 Langdon St, , Outside if the weather is nice, inside else Room: Check the \u201cToday In The Union\u201d (TITU) electronic displays each day, as the exact room may vary. TITU: http://union.ems.wisc.edu/VirtualEMS/CustomBrowseEvents.aspx?data=MeN0rP%2FRPNoJS2WjBImM5NV%2B64hv0ZxW Map: https://apps.union.wisc.edu/materials/Visit_BuildingMap_US.pdf#Page=4","title":"Details"},{"location":"curriculum/overview/","text":"OSG User School 2017 : Curriculum and Syllabus \u00b6 Curriculum Goals \u00b6 At a high level, the goal of the School is to help students learn to: Describe the basic elements and architecture of a distributed computing system Use basic distributed computing tools to run jobs and manage data Select reasonable tools and methods to solve scientific computing problems using distributed computing Outline the role of distributed computing, its history, current state and issues, and hopes for the future Identify resources for support, further study, and development opportunities in distributed computing Syllabus \u00b6 The high-level syllabus for 2017 is below; a more detailed schedule will be published later as we get closer to the school. Morning Afternoon Monday Welcome Introduction to high-throughput computing Running jobs locally with Condor Introduction to high-throughput workflows Running workflows locally with Condor Tuesday Introduction to grid computing and overlays Basic troubleshooting Introduction to OSG Running jobs using OSG glideins Running on real resources OSG architecture Wednesday Dealing with real software (Especially R and MATLAB, but others as well) Free choice: \u2022 More details about how overlays systems work \u2022 Get one-on-one help with your computing work \u2022 Take a break and visit Madison Thursday Introduction to distributed storage Using remote storage systems Managing large, distributed data Introduction to grid security for end users Friday Turning scientific computing needs into HTC jobs Estimating resource needs, decomposing and running large jobs Strategies and technologies for handling large workflows Principles of high-throughput computing Scientific computing showcase Where to go and what to do next \u2014 resources, funding, etc.","title":"High-level overview"},{"location":"curriculum/overview/#osg-user-school-2017-curriculum-and-syllabus","text":"","title":"OSG User School 2017 : Curriculum and Syllabus"},{"location":"curriculum/overview/#curriculum-goals","text":"At a high level, the goal of the School is to help students learn to: Describe the basic elements and architecture of a distributed computing system Use basic distributed computing tools to run jobs and manage data Select reasonable tools and methods to solve scientific computing problems using distributed computing Outline the role of distributed computing, its history, current state and issues, and hopes for the future Identify resources for support, further study, and development opportunities in distributed computing","title":"Curriculum Goals"},{"location":"curriculum/overview/#syllabus","text":"The high-level syllabus for 2017 is below; a more detailed schedule will be published later as we get closer to the school. Morning Afternoon Monday Welcome Introduction to high-throughput computing Running jobs locally with Condor Introduction to high-throughput workflows Running workflows locally with Condor Tuesday Introduction to grid computing and overlays Basic troubleshooting Introduction to OSG Running jobs using OSG glideins Running on real resources OSG architecture Wednesday Dealing with real software (Especially R and MATLAB, but others as well) Free choice: \u2022 More details about how overlays systems work \u2022 Get one-on-one help with your computing work \u2022 Take a break and visit Madison Thursday Introduction to distributed storage Using remote storage systems Managing large, distributed data Introduction to grid security for end users Friday Turning scientific computing needs into HTC jobs Estimating resource needs, decomposing and running large jobs Strategies and technologies for handling large workflows Principles of high-throughput computing Scientific computing showcase Where to go and what to do next \u2014 resources, funding, etc.","title":"Syllabus"},{"location":"logistics/","text":"OSG User School 2017 Logistics \u00b6 The following pages describe some of the important information about your visit to Madison for the OSG User School. Please read them carefully. There will be other pages with local details soon. Non-Resident Aliens and documentation requirements Travel to and from Madison As always: If you have questions, email us at user-school@opensciencegrid.org . Use that email address for all emails about the organization of the OSG School. General Information About the School Schedule \u00b6 Travel Schedule \u00b6 For planning your trip: Arrive on Sunday, July 16, 2017, prior to 5 p.m. (if possible). Although classes begin on Monday morning, there is a welcome dinner on Sunday evening for students and instructors. This is a nice way to get to know each other and to start the week. Depart on Saturday, July 22, 2017, any time. The School finishes with another dinner on Friday evening, so it is best to stay that night. If we offered to pay for your hotel room, we will pay for the six nights of this schedule. School Hours \u00b6 The school is 8:00 a.m. to about 5:00 p.m., Monday through Friday. A detailed schedule will be available before the School begins. Contact Information \u00b6 If you have questions, do not wait to contact us! user-school@opensciencegrid.org","title":"General information"},{"location":"logistics/#osg-user-school-2017-logistics","text":"The following pages describe some of the important information about your visit to Madison for the OSG User School. Please read them carefully. There will be other pages with local details soon. Non-Resident Aliens and documentation requirements Travel to and from Madison As always: If you have questions, email us at user-school@opensciencegrid.org . Use that email address for all emails about the organization of the OSG School.","title":"OSG User School 2017 Logistics"},{"location":"logistics/#general-information-about-the-school-schedule","text":"","title":"General Information About the School Schedule"},{"location":"logistics/#travel-schedule","text":"For planning your trip: Arrive on Sunday, July 16, 2017, prior to 5 p.m. (if possible). Although classes begin on Monday morning, there is a welcome dinner on Sunday evening for students and instructors. This is a nice way to get to know each other and to start the week. Depart on Saturday, July 22, 2017, any time. The School finishes with another dinner on Friday evening, so it is best to stay that night. If we offered to pay for your hotel room, we will pay for the six nights of this schedule.","title":"Travel Schedule"},{"location":"logistics/#school-hours","text":"The school is 8:00 a.m. to about 5:00 p.m., Monday through Friday. A detailed schedule will be available before the School begins.","title":"School Hours"},{"location":"logistics/#contact-information","text":"If you have questions, do not wait to contact us! user-school@opensciencegrid.org","title":"Contact Information"},{"location":"logistics/fun/","text":"Fun Things to Do in Madison \u00b6 Memorial Union is a great place to hang out and a short walk from the Best Western; many people like to get a drink and relax on the Terrace, which looks out over Lake Mendota There is construction going on at Memorial Union this year, but it is still open Check the Wisconsin Union events calendar State Street is a student-friendly street and outdoor pedestrian mall, a couple blocks from the Best Western, with tons of great businesses \u2014 restaurants, stores, bars, and more Catch a show at the historic Orpheum Theatre Check out the Overture Center for the Arts Read The Isthumus Daily Page for the latest events Go to a free outdoor classical music \u201cConcert on the Square\u201d on Wednesday , by the State Capitol building and steps away from the Best Western Also Check Out Wednesday Afternoon Activities!! \u00b6 (click just above)","title":"Fun in Madison"},{"location":"logistics/fun/#fun-things-to-do-in-madison","text":"Memorial Union is a great place to hang out and a short walk from the Best Western; many people like to get a drink and relax on the Terrace, which looks out over Lake Mendota There is construction going on at Memorial Union this year, but it is still open Check the Wisconsin Union events calendar State Street is a student-friendly street and outdoor pedestrian mall, a couple blocks from the Best Western, with tons of great businesses \u2014 restaurants, stores, bars, and more Catch a show at the historic Orpheum Theatre Check out the Overture Center for the Arts Read The Isthumus Daily Page for the latest events Go to a free outdoor classical music \u201cConcert on the Square\u201d on Wednesday , by the State Capitol building and steps away from the Best Western","title":"Fun Things to Do in Madison"},{"location":"logistics/fun/#also-check-out-wednesday-afternoon-activities","text":"(click just above)","title":"Also Check Out Wednesday Afternoon Activities!!"},{"location":"logistics/hotel/","text":"Hotel Information \u00b6 We have reserved a block of rooms at the Best Western Premier Park Hotel , located on the Capitol Square near campus. They are located at: 22 South Carroll Street Madison, WI 53703 (608) 285-8000 The hotel has a shuttle that may be available for transporting School participants, and it is located near many bus routes that serve the School location plus all of Madison. We will reserve your room for you within the block, and we will pay for your basic room costs directly. There is one participant per room, so you will not have a roommate. NOTE: Even though we are paying for your hotel room, when you check in, you must provide the hotel with a credit card \u2014 it is only for optional expenses that you cause by yourself. We will pay for only the room charges and taxes.","title":"Hotel information"},{"location":"logistics/hotel/#hotel-information","text":"We have reserved a block of rooms at the Best Western Premier Park Hotel , located on the Capitol Square near campus. They are located at: 22 South Carroll Street Madison, WI 53703 (608) 285-8000 The hotel has a shuttle that may be available for transporting School participants, and it is located near many bus routes that serve the School location plus all of Madison. We will reserve your room for you within the block, and we will pay for your basic room costs directly. There is one participant per room, so you will not have a roommate. NOTE: Even though we are paying for your hotel room, when you check in, you must provide the hotel with a credit card \u2014 it is only for optional expenses that you cause by yourself. We will pay for only the room charges and taxes.","title":"Hotel Information"},{"location":"logistics/local-transportation/","text":"Travel in Madison \u00b6 We will help coordinate travel within Madison for those who do not have their own transportation. Travel Between the Madison Airport and the Best Western Hotel \u00b6 The Best Western hotel can provide free shuttle service from and back to the Madison airport, but there are limited seats available \u2014 therefore we will organize groups of participants who can take a shuttle together. Shuttle plans will be formed and communicated shortly before the School itself. The hotel shuttle is free and direct, so it is the best option between airport and hotel. If the shuttle does not work for you, you may take a taxi or ride-sharing service and be reimbursed after the School. But please use this option only if necessary. There are several good taxi companies in Madison, including http://www.greencabofmadison.com/ Green Cab , http://www.badgercab.com/ Badger Cab and http://www.unioncab.com/ Union Cab . Cabs are readily available at the Madison airport; elsewhere, you most likely will have to contact the company and request a ride or make a reservation. Note that we can reimburse participants only for taxi or ride-sharing rides between the airport and hotel/campus area. Travel On and Near Campus \u00b6 The Best Western hotel is about 1.3 miles away from the Computer Sciences building, where the School is held. There are many good options for getting between the hotel and School, plus other sights, restaurants, and so forth in Madison. Walking \u00b6 It is easy to walk to most places in and around the University of Wisconsin\u2013Madison campus. Use your favorite mapping tool to help, or ask the hotel for a map. In particular, State Street \u2014 which connects the Capitol Square with the UW campus \u2014 is full of great shops and restaurants and is worth strolling along some time while you are here. Hotel Shuttle \u00b6 The Best Western has a free shuttle that can be used to transport students between the hotel and the School\u2026 and other places. Just ask the hotel for more information and availability. City of Madison Metro Bus Service \u00b6 Many Madison Metro buses stop on the Capitol Square and pass through the University of Wisconsin\u2013Madison campus. Bus fare is $2.00, and transfers may be requested for free. If you plan to use buses to travel to and from the School every day, contact us, and we may be able to buy a bus pass for you or explain how to do so. The official Madison Metro Bus page provides route maps and times for all Madison buses. Google Maps and the Wisconsin phone app also do a good job. Several bus lines run within a couple of blocks of the Best Western and Computer Sciences, and the buses are a great way to get around town when walking or if using the hotel shuttle is not an option. Taxis and Ride-Sharing Services \u00b6 As listed above in the airport-hotel section, there are several good taxi companies in Madison; plus, Uber and Lyft are both active in Madison. Note: We cannot reimburse participants for ride services except between the airport and hotel, so this option is on your own. Madison BCycle \u00b6 Madison is a great city to bike in, and there is even a short-term bike rental system called BCycle here. For instance, it is possible to get a bike near the hotel, just at the top of State Street, and then drop off the bike at Union South, located immediately adjacent to the Computer Sciences building where the School is held. See the website for more details. While not terribly expensive, this is not a cost that we can reimburse.","title":"Local transportation"},{"location":"logistics/local-transportation/#travel-in-madison","text":"We will help coordinate travel within Madison for those who do not have their own transportation.","title":"Travel in Madison"},{"location":"logistics/local-transportation/#travel-between-the-madison-airport-and-the-best-western-hotel","text":"The Best Western hotel can provide free shuttle service from and back to the Madison airport, but there are limited seats available \u2014 therefore we will organize groups of participants who can take a shuttle together. Shuttle plans will be formed and communicated shortly before the School itself. The hotel shuttle is free and direct, so it is the best option between airport and hotel. If the shuttle does not work for you, you may take a taxi or ride-sharing service and be reimbursed after the School. But please use this option only if necessary. There are several good taxi companies in Madison, including http://www.greencabofmadison.com/ Green Cab , http://www.badgercab.com/ Badger Cab and http://www.unioncab.com/ Union Cab . Cabs are readily available at the Madison airport; elsewhere, you most likely will have to contact the company and request a ride or make a reservation. Note that we can reimburse participants only for taxi or ride-sharing rides between the airport and hotel/campus area.","title":"Travel Between the Madison Airport and the Best Western Hotel"},{"location":"logistics/local-transportation/#travel-on-and-near-campus","text":"The Best Western hotel is about 1.3 miles away from the Computer Sciences building, where the School is held. There are many good options for getting between the hotel and School, plus other sights, restaurants, and so forth in Madison.","title":"Travel On and Near Campus"},{"location":"logistics/local-transportation/#walking","text":"It is easy to walk to most places in and around the University of Wisconsin\u2013Madison campus. Use your favorite mapping tool to help, or ask the hotel for a map. In particular, State Street \u2014 which connects the Capitol Square with the UW campus \u2014 is full of great shops and restaurants and is worth strolling along some time while you are here.","title":"Walking"},{"location":"logistics/local-transportation/#hotel-shuttle","text":"The Best Western has a free shuttle that can be used to transport students between the hotel and the School\u2026 and other places. Just ask the hotel for more information and availability.","title":"Hotel Shuttle"},{"location":"logistics/local-transportation/#city-of-madison-metro-bus-service","text":"Many Madison Metro buses stop on the Capitol Square and pass through the University of Wisconsin\u2013Madison campus. Bus fare is $2.00, and transfers may be requested for free. If you plan to use buses to travel to and from the School every day, contact us, and we may be able to buy a bus pass for you or explain how to do so. The official Madison Metro Bus page provides route maps and times for all Madison buses. Google Maps and the Wisconsin phone app also do a good job. Several bus lines run within a couple of blocks of the Best Western and Computer Sciences, and the buses are a great way to get around town when walking or if using the hotel shuttle is not an option.","title":"City of Madison Metro Bus Service"},{"location":"logistics/local-transportation/#taxis-and-ride-sharing-services","text":"As listed above in the airport-hotel section, there are several good taxi companies in Madison; plus, Uber and Lyft are both active in Madison. Note: We cannot reimburse participants for ride services except between the airport and hotel, so this option is on your own.","title":"Taxis and Ride-Sharing Services"},{"location":"logistics/local-transportation/#madison-bcycle","text":"Madison is a great city to bike in, and there is even a short-term bike rental system called BCycle here. For instance, it is possible to get a bike near the hotel, just at the top of State Street, and then drop off the bike at Union South, located immediately adjacent to the Computer Sciences building where the School is held. See the website for more details. While not terribly expensive, this is not a cost that we can reimburse.","title":"Madison BCycle"},{"location":"logistics/location/","text":"School Location \u00b6 The school will be held at the http://www.wisc.edu University of Wisconsin\u2013Madison in the Computer Sciences Department , located at 1210 West Dayton Street, Madison, WI, 53706 . The main classroom is Room 1240. It is a 25 minute walk from the Best Western hotel. Getting From the Best Western to Computer Sciences \u00b6 Walking \u00b6 Exit the Best Western onto South Carroll Street and walk northwest for 1 block 2. Turn Left at West Washington Avenue, walk 2 blocks 3. Turn Right at North Henry Street, walk 2 blocks 4. Turn Left at West Dayton Street, continue west on Dayton for about 1 mile 5. The main entrance to the Computer Sciences building is on the right, midway down the 1200 block Hotel Shuttle \u00b6 The Best Western will provide free shuttle service between the hotel and Computer Sciences for those who need it or prefer not to walk. Madison Metro City Bus Service \u00b6 The official Madison Metro Bus page provides route maps and times for all Madison buses. Google Maps and the Wisconsin phone app also do a good job. Several bus lines run within a couple of blocks of the Best Western and Computer Sciences, and the buses are a great way to get around town when walking or if using the hotel shuttle is not an option. Computer Sciences Building, Room 1240 \u00b6 School sessions, breakfasts, and lunches are held in Room 1240 : Enter the Computer Sciences building through the glass doors on Dayton Street Turn left and go through two sets of double doors Walk down the hallway just a bit Room 1240 is on the right, up the ramp or a few stairs The Computer Sciences building is a bit confusing \u2014 we will post signs to help you find your way. You can always ask people for directions if you get lost.","title":"School location"},{"location":"logistics/location/#school-location","text":"The school will be held at the http://www.wisc.edu University of Wisconsin\u2013Madison in the Computer Sciences Department , located at 1210 West Dayton Street, Madison, WI, 53706 . The main classroom is Room 1240. It is a 25 minute walk from the Best Western hotel.","title":"School Location"},{"location":"logistics/location/#getting-from-the-best-western-to-computer-sciences","text":"","title":"Getting From the Best Western to Computer Sciences"},{"location":"logistics/location/#walking","text":"Exit the Best Western onto South Carroll Street and walk northwest for 1 block 2. Turn Left at West Washington Avenue, walk 2 blocks 3. Turn Right at North Henry Street, walk 2 blocks 4. Turn Left at West Dayton Street, continue west on Dayton for about 1 mile 5. The main entrance to the Computer Sciences building is on the right, midway down the 1200 block","title":"Walking"},{"location":"logistics/location/#hotel-shuttle","text":"The Best Western will provide free shuttle service between the hotel and Computer Sciences for those who need it or prefer not to walk.","title":"Hotel Shuttle"},{"location":"logistics/location/#madison-metro-city-bus-service","text":"The official Madison Metro Bus page provides route maps and times for all Madison buses. Google Maps and the Wisconsin phone app also do a good job. Several bus lines run within a couple of blocks of the Best Western and Computer Sciences, and the buses are a great way to get around town when walking or if using the hotel shuttle is not an option.","title":"Madison Metro City Bus Service"},{"location":"logistics/location/#computer-sciences-building-room-1240","text":"School sessions, breakfasts, and lunches are held in Room 1240 : Enter the Computer Sciences building through the glass doors on Dayton Street Turn left and go through two sets of double doors Walk down the hallway just a bit Room 1240 is on the right, up the ramp or a few stairs The Computer Sciences building is a bit confusing \u2014 we will post signs to help you find your way. You can always ask people for directions if you get lost.","title":"Computer Sciences Building, Room 1240"},{"location":"logistics/meals/","text":"Meals and Food During the School \u00b6 All catered, group meals that the School provides will include vegan and vegetarian options. If you indicated other dietary needs in the survey, we will address those on a case-by-case basis as well. Group Dinners \u00b6 On Sunday evening, there will be a welcome dinner at 6:30 p.m. (details announced later). Someone from the School will be in the lobby of the Best Western hotel in advance. Then, about 25 minutes before the start of dinner, the entire group will walk together to the dinner location. Or you can travel to the location on your own. On Friday evening, there will be a closing dinner at 6:30 p.m. (details announced later). School Meals \u00b6 The School pays for your meals during the School. First, we provide the following meals directly: Breakfasts on Monday through Friday \u2014 held in Computer Sciences Room 1240, the same room as the school sessions Lunches on Monday through Friday \u2014 held in or near the lecture hall The group dinners described above For dinners Monday through Thursday, if you are coming from out of town , we can reimburse you after the School. In that case: Keep receipts for your dinners \u2013 if anything so that you remember how much meals cost! We can reimburse up to $27.00, including tax and tip If it is not on the receipt, be sure to write your tip amount yourself, so you do not forget We cannot pay for any alcohol, but non-alcoholic drinks are OK \u2014 so pay for any alcohol on a separate bill We will explain the reimbursement process at the end of the School Again, if you live in or near Madison and are part of the UW\u2013Madison community, by University rules we cannot pay for your own dinners during the School.","title":"Meals at the School"},{"location":"logistics/meals/#meals-and-food-during-the-school","text":"All catered, group meals that the School provides will include vegan and vegetarian options. If you indicated other dietary needs in the survey, we will address those on a case-by-case basis as well.","title":"Meals and Food During the School"},{"location":"logistics/meals/#group-dinners","text":"On Sunday evening, there will be a welcome dinner at 6:30 p.m. (details announced later). Someone from the School will be in the lobby of the Best Western hotel in advance. Then, about 25 minutes before the start of dinner, the entire group will walk together to the dinner location. Or you can travel to the location on your own. On Friday evening, there will be a closing dinner at 6:30 p.m. (details announced later).","title":"Group Dinners"},{"location":"logistics/meals/#school-meals","text":"The School pays for your meals during the School. First, we provide the following meals directly: Breakfasts on Monday through Friday \u2014 held in Computer Sciences Room 1240, the same room as the school sessions Lunches on Monday through Friday \u2014 held in or near the lecture hall The group dinners described above For dinners Monday through Thursday, if you are coming from out of town , we can reimburse you after the School. In that case: Keep receipts for your dinners \u2013 if anything so that you remember how much meals cost! We can reimburse up to $27.00, including tax and tip If it is not on the receipt, be sure to write your tip amount yourself, so you do not forget We cannot pay for any alcohol, but non-alcoholic drinks are OK \u2014 so pay for any alcohol on a separate bill We will explain the reimbursement process at the end of the School Again, if you live in or near Madison and are part of the UW\u2013Madison community, by University rules we cannot pay for your own dinners during the School.","title":"School Meals"},{"location":"logistics/personal-info/","text":"Documentation Requirements for Non-Resident Aliens \u00b6 In order for the University of Wisconsin to pay for your travel, hotel, or meal expenses, we must have certain personal information from you. We will collect as little information as possible and not share it with anyone except University administrative staff who need it. When you come to the School in Madison, we will need to look at and verify your travel documents. Please bring all travel documents to the School! Note: If you are not being reimbursed for any meals and/or travel for the school or if you are from UW-Madison or the Madison area, these documents are not required. Tasks To Do Now \u00b6 Please check your passport and visa for travel in the United States now. Make sure that all documents are valid from now and until after the School ends. If any documents are expired or will expire before the end of the School: Tell us immediately, so that we can help you Begin the process for updating your documents immediately Do whatever you can to expedite the update process The University of Wisconsin cannot pay for or reimburse you for costs without valid travel documents. We have no control over this policy and there are no exceptions. %RED%If you are in the United States on a J-1 Scholar visa: Tell us immediately, so that we can help you You must provide a letter from your sponsoring organization giving us permission to reimburse you for travel expenses We cannot pay any of your expenses until we receive the letter The letter must come before any travel planning, so get it soon Documents to Bring to the School \u00b6 When you come to Madison, you must bring: Passport U.S. visa U.S. Customs and Border Protection form I-94 If you entered the U.S. before 30 April 2013, the I-94 should be stapled into your passport \u2014 do not remove it! If you entered the U.S. after 30 April 2013, the I-94 is stored electronically; you can request a copy to print from CBP If you are Canadian, you may use a second form of picture ID instead of the I-94 if you did not obtain an I-94. Additional forms specified in the table below: If you have this visa We will also need F-1 (Student) Form I-20 (original document, not a copy) J-1 (Visitor) Form DS-2019 (original document, not a copy) Visa Waiver Program Paper copy of ESTA Authorization Please bring all required information and documents to the School, especially on Monday, July 17. School staff will make copies of the documents and return them to you as quickly as possible.","title":"Visa requirements"},{"location":"logistics/personal-info/#documentation-requirements-for-non-resident-aliens","text":"In order for the University of Wisconsin to pay for your travel, hotel, or meal expenses, we must have certain personal information from you. We will collect as little information as possible and not share it with anyone except University administrative staff who need it. When you come to the School in Madison, we will need to look at and verify your travel documents. Please bring all travel documents to the School! Note: If you are not being reimbursed for any meals and/or travel for the school or if you are from UW-Madison or the Madison area, these documents are not required.","title":"Documentation Requirements for Non-Resident Aliens"},{"location":"logistics/personal-info/#tasks-to-do-now","text":"Please check your passport and visa for travel in the United States now. Make sure that all documents are valid from now and until after the School ends. If any documents are expired or will expire before the end of the School: Tell us immediately, so that we can help you Begin the process for updating your documents immediately Do whatever you can to expedite the update process The University of Wisconsin cannot pay for or reimburse you for costs without valid travel documents. We have no control over this policy and there are no exceptions. %RED%If you are in the United States on a J-1 Scholar visa: Tell us immediately, so that we can help you You must provide a letter from your sponsoring organization giving us permission to reimburse you for travel expenses We cannot pay any of your expenses until we receive the letter The letter must come before any travel planning, so get it soon","title":"Tasks To Do Now"},{"location":"logistics/personal-info/#documents-to-bring-to-the-school","text":"When you come to Madison, you must bring: Passport U.S. visa U.S. Customs and Border Protection form I-94 If you entered the U.S. before 30 April 2013, the I-94 should be stapled into your passport \u2014 do not remove it! If you entered the U.S. after 30 April 2013, the I-94 is stored electronically; you can request a copy to print from CBP If you are Canadian, you may use a second form of picture ID instead of the I-94 if you did not obtain an I-94. Additional forms specified in the table below: If you have this visa We will also need F-1 (Student) Form I-20 (original document, not a copy) J-1 (Visitor) Form DS-2019 (original document, not a copy) Visa Waiver Program Paper copy of ESTA Authorization Please bring all required information and documents to the School, especially on Monday, July 17. School staff will make copies of the documents and return them to you as quickly as possible.","title":"Documents to Bring to the School"},{"location":"logistics/reimbursements/","text":"Getting Reimbursed for Expenses \u00b6 If you paid for certain expenses to attend the OSG User School 2017, then we will reimburse you for them. This page explains what can be reimbursed and how to request reimbursement. Please read this whole page carefully before submitting your request. The deadline for all reimbursement submissions is Tuesday, 5 September 2017 . What Can Be Reimbursed? \u00b6 We offered to reimburse certain expenses that were part of attending the OSG School: Bus fare to/from Madison Taxi to/from Madison airport Driving mileage (if prearranged) Parking at the hotel (if prearranged) Dinners Monday\u2013Thursday (up to $27 per dinner) Hotel bill (in special, prearranged cases only) Unfortunately, we are not able to reimburse you for any other expenses (e.g., checked baggage fees, transportation to/from your home and local airport). What Information Is Needed? \u00b6 The University of Wisconsin\u2013Madison has strict requirements about reimbursements. See below for each type of expense and its rules. Expense Rules Bus fare For bus trips between Madison and a nearby airport, as planned with us in advance. The original receipt is required, including at least the total amount paid. Ride fare For taxi and ride-sharing trips between the Madison airport and your hotel. Rides in your home town are not covered. If you paid for other School participants, list their names. If the total (fare + tip) is more than $25, the original receipt is required, ideally including: Date of trip Fare and tip Driving mileage If we agreed in advance to cover your driving costs to/from the School, simply list the agreed-upon reimbursement amount ($ or miles) as a reminder. Parking If you drove here and parked at the Best Western, you may submit your parking receipt(s) for reimbursement. Dinners For dinners on Monday through Thursday of the School, we can reimburse you for up to $27 per evening. The total reimbursement per evening is limited to $27 Within the $27 total, you may include tax and tip (but no more than 15% tip) No alcohol is allowed, but non-alcoholic drinks are OK Original receipts are not required, but if you want to email us a scanned copy of your receipts, it may help us understand your request better If you do not have receipts, do your best to estimate your meal expenses accurately, up to the $27 per evening limit. Remember, all of your meal reimbursements are being paid by federal taxpayer funds, so be fair and honest! Hotel bill If we arranged in advance for you to pay your own hotel bill, we can reimburse you as agreed. The complete hotel bill, including the itemized invoice, is required. Note: If you must give us receipts (see above), electronic files are fine. You may scan or photograph any paper receipts and email them to us. How to Request Reimbursement \u00b6 Please follow these instructions carefully for fastest processing of your reimbursement request. Gather your reimbursement information (see above for what you need) Email your reimbursement request by Tuesday, 5 September 2017. \\ Write a clear email with your name, the exact expenses and amounts to be reimbursed, reimbursement total, and any scanned receipts or other documentation. Send the email to mailto:user-school@opensciencegrid.org user-school@opensciencegrid.org . Use the sample format below to get started, updating the subtotals and removing the sections that do not apply to you : \\ I attended the OSG User School 2017 in Madison, Wisconsin, and I request reimbursement for the following School-related expenses: STUDENT INFORMATION Name: Cartwright, Tim SSN: 1234 [LAST FOUR DIGITS ONLY] Mail: 1210 W Dayton St, Madison, WI 53706 Email: cat@cs.wisc.edu Phone: (608) 262-4002 EXPENSES Bus Fare => $60.00 * Roundtrip on Van Galder bus between Chicago O\u2019Hare and UW Madison * Original electronic receipt is attached Taxi Fare => $30.00 * One-way from Madison airport to Best Western hotel * Other OSG School passengers: Lauren Michael, Brian Lin * Scanned receipt is attached Driving Mileage => $200.00 * Roundtrip between home and Best Western hotel in Madison * Estimated total miles (from Google Maps): 420 Dinners => $72.24 * Mon, 17 July 2017: $20.00 * Tue, 18 July 2017: $15.80 * Wed, 19 July 2017: $19.84 * Thu, 20 July 2017: $16.60 Hotel => $630.00 * Best Western Premier Park Hotel * $115.00 per night [maximum reimburseable rate] for 6 nights * Scanned receipt is attached TOTAL => $1052.24 For receipts, just attach the electronic files (PDF) to your request email; or, email separately to mailto:user-school@opensciencegrid.org user-school@opensciencegrid.org . All receipts must be emailed by Tuesday, 5 September 2017, or the corresponding expenses will not be reimbursed. OSG School staff will review your request and either approve it or send it back for changes. Once approved, UW\u2013Madison administrative staff will process your reimbursement request. Your reimbursement check will be mailed to the postal address that you provided. When you receive your check, please email mailto:user-school@opensciencegrid.org user-school@opensciencegrid.org to say that the check arrived and the exact amount of the check. Maybe surprisingly, this is the only way that we can make sure that you got the reimbursement and that the amount was correct.","title":"Reimbursements"},{"location":"logistics/reimbursements/#getting-reimbursed-for-expenses","text":"If you paid for certain expenses to attend the OSG User School 2017, then we will reimburse you for them. This page explains what can be reimbursed and how to request reimbursement. Please read this whole page carefully before submitting your request. The deadline for all reimbursement submissions is Tuesday, 5 September 2017 .","title":"Getting Reimbursed for Expenses"},{"location":"logistics/reimbursements/#what-can-be-reimbursed","text":"We offered to reimburse certain expenses that were part of attending the OSG School: Bus fare to/from Madison Taxi to/from Madison airport Driving mileage (if prearranged) Parking at the hotel (if prearranged) Dinners Monday\u2013Thursday (up to $27 per dinner) Hotel bill (in special, prearranged cases only) Unfortunately, we are not able to reimburse you for any other expenses (e.g., checked baggage fees, transportation to/from your home and local airport).","title":"What Can Be Reimbursed?"},{"location":"logistics/reimbursements/#what-information-is-needed","text":"The University of Wisconsin\u2013Madison has strict requirements about reimbursements. See below for each type of expense and its rules. Expense Rules Bus fare For bus trips between Madison and a nearby airport, as planned with us in advance. The original receipt is required, including at least the total amount paid. Ride fare For taxi and ride-sharing trips between the Madison airport and your hotel. Rides in your home town are not covered. If you paid for other School participants, list their names. If the total (fare + tip) is more than $25, the original receipt is required, ideally including: Date of trip Fare and tip Driving mileage If we agreed in advance to cover your driving costs to/from the School, simply list the agreed-upon reimbursement amount ($ or miles) as a reminder. Parking If you drove here and parked at the Best Western, you may submit your parking receipt(s) for reimbursement. Dinners For dinners on Monday through Thursday of the School, we can reimburse you for up to $27 per evening. The total reimbursement per evening is limited to $27 Within the $27 total, you may include tax and tip (but no more than 15% tip) No alcohol is allowed, but non-alcoholic drinks are OK Original receipts are not required, but if you want to email us a scanned copy of your receipts, it may help us understand your request better If you do not have receipts, do your best to estimate your meal expenses accurately, up to the $27 per evening limit. Remember, all of your meal reimbursements are being paid by federal taxpayer funds, so be fair and honest! Hotel bill If we arranged in advance for you to pay your own hotel bill, we can reimburse you as agreed. The complete hotel bill, including the itemized invoice, is required. Note: If you must give us receipts (see above), electronic files are fine. You may scan or photograph any paper receipts and email them to us.","title":"What Information Is Needed?"},{"location":"logistics/reimbursements/#how-to-request-reimbursement","text":"Please follow these instructions carefully for fastest processing of your reimbursement request. Gather your reimbursement information (see above for what you need) Email your reimbursement request by Tuesday, 5 September 2017. \\ Write a clear email with your name, the exact expenses and amounts to be reimbursed, reimbursement total, and any scanned receipts or other documentation. Send the email to mailto:user-school@opensciencegrid.org user-school@opensciencegrid.org . Use the sample format below to get started, updating the subtotals and removing the sections that do not apply to you : \\ I attended the OSG User School 2017 in Madison, Wisconsin, and I request reimbursement for the following School-related expenses: STUDENT INFORMATION Name: Cartwright, Tim SSN: 1234 [LAST FOUR DIGITS ONLY] Mail: 1210 W Dayton St, Madison, WI 53706 Email: cat@cs.wisc.edu Phone: (608) 262-4002 EXPENSES Bus Fare => $60.00 * Roundtrip on Van Galder bus between Chicago O\u2019Hare and UW Madison * Original electronic receipt is attached Taxi Fare => $30.00 * One-way from Madison airport to Best Western hotel * Other OSG School passengers: Lauren Michael, Brian Lin * Scanned receipt is attached Driving Mileage => $200.00 * Roundtrip between home and Best Western hotel in Madison * Estimated total miles (from Google Maps): 420 Dinners => $72.24 * Mon, 17 July 2017: $20.00 * Tue, 18 July 2017: $15.80 * Wed, 19 July 2017: $19.84 * Thu, 20 July 2017: $16.60 Hotel => $630.00 * Best Western Premier Park Hotel * $115.00 per night [maximum reimburseable rate] for 6 nights * Scanned receipt is attached TOTAL => $1052.24 For receipts, just attach the electronic files (PDF) to your request email; or, email separately to mailto:user-school@opensciencegrid.org user-school@opensciencegrid.org . All receipts must be emailed by Tuesday, 5 September 2017, or the corresponding expenses will not be reimbursed. OSG School staff will review your request and either approve it or send it back for changes. Once approved, UW\u2013Madison administrative staff will process your reimbursement request. Your reimbursement check will be mailed to the postal address that you provided. When you receive your check, please email mailto:user-school@opensciencegrid.org user-school@opensciencegrid.org to say that the check arrived and the exact amount of the check. Maybe surprisingly, this is the only way that we can make sure that you got the reimbursement and that the amount was correct.","title":"How to Request Reimbursement"},{"location":"logistics/travel/","text":"Travel To and From Madison \u00b6 Whether we offered to pay your travel costs or not, please make sure that we get a copy of your travel plans so that we know when to expect you here and can plan accurately. (If we offered to pay for your hotel room, we will pay for the six nights of the schedule.) Find the numbered section below that applies to you: 1. We Offered to Pay for Your Travel \u00b6 We budgeted an average of $525 per participant for travel to and from Madison (except in cases of flying internationally or driving from nearby). Many travelers will find good options for less than that amount, and some will have to go over that amount. Our goal is to have everyone find a reasonable travel itinerary and not exceed our overall budget. When looking for a reasonable and cost-effective means to travel to and from Madison, here are a few ideas: Within about 5 hours driving of Madison, consider driving or taking a bus; we can support all kinds of travel If you fly, be flexible (within reason) about departure times \u2014 early and late flights are often the least expensive Consider flying into Milwaukee (1\u00bd hours away) or Chicago (2\u00bd hours away) and taking a direct bus to/from Madison (details below) In any case, try to complete your travel plans by early June, before rates go up significantly. See below for details about each travel mode: Travel by Airplane \u00b6 Do NOT buy your own airline tickets . University rules require the University\u2019s travel agency, Fox World Travel (FWT), to purchase your tickets. To arrange flights to/from Madison: Prepare your travel information, requirements, and preferences: Basic personal information, including full legal name (as on government IDs), date of birth, and possibly a phone number Travel requirements, including starting and ending airport(s), hard timing constraints \u2014 preferences, such as not liking early flights, are different than hard constraints! Travel preferences, including timing preferences, airplane seating locations, and so forth If you wish, you may search for ( but not purchase ) flights ahead of time for your own information \u2014 but note that FWT may find different flights or may see different costs, and all tickets must be purchased by FWT and follow UW rules. If flights to/from Madison are expensive, the FWT agent may check flights to/from Milwaukee or Chicago and bus schedules from those airports to Madison: Bus between Mitchell Airport (MKE) in Milwaukee and downtown Madison: Badger Bus (~$20 each way) Bus between O\u2019Hare Airport (ORD) in Chicago and downtown Madison: Van Galder Bus (~$30 each way) Bus between Midway Airport (MDW) in Chicago and downtown Madison: Van Galder Bus (~$30 each way) 2. Contact Fox World Travel By phone: Within the United States, use 866-230-8787 and select option 4; internationally, use +1 920-230-6467 and select option 4. Tell them that you are part of Group Code 09UW1028. Agents are available Monday through Friday, 7:00 a.m. until 5:30 p.m. U.S. Central Time. By email: Send to uwgroups@foxworldtravel.com and include Group Code 09UW1028 in the subject line. 3. In most cases, FWT will need to get our approval for your itinerary before completing the purchase, so it may take up to a day to get final confirmation. Airplane seats cannot be held without purchase over a weekend, so it is best to avoid contacting FWT late on Fridays. Note: If you are including bus tickets as part of your flying itinerary, you may purchase them through FWT when you arrange your flights, or for maximum flexibility purchase them from bus drivers on the day of travel. Travel by Bus \u00b6 Typically, reasonable bus options \u2014 i.e., those that do not take an excessive amount of time to reach Madison \u2014 cost much less than air options. Note that you may purchase bus tickets through Fox World Travel (see above in the Travel by Airplane section) or yourself before or on the day of travel. If you purchase your own tickets, you must get our approval for the estimated cost first, then request reimbursement from us after the School. When you purchase your own tickets, save the original receipt (even if by email). It would be best if the ticket contains the following information, but a regular ticket stub (e.g., without your name or date) should work fine. Just get what you can! If possible, we would like: Complete itinerary Your name Date of purchase Total amount paid Be sure to email us with your travel plans as soon as possible. Try to include: Transportation provider(s) (e.g., Van Galder bus) Arrival date and approximate time Departure date and approximate time Arrival and departure location within Madison Actual or estimated cost (indicate which) Travel by Personal Car \u00b6 If you are driving to Madison, you will be reimbursed the mileage rate of $0.535/mile for the round-trip distance (as calculated by Google Maps), plus tolls. Also, we will pay for parking costs for the week at the hotel in Madison (but not elsewhere). It may help to keep your receipts for tolls. Note: Due to the high mileage reimbursement rate, driving can be an expensive option! We reserve the right to limit your total driving reimbursement, so work with us on the details. To travel by personal car, please check with us first. We may search for comparable flight options, to make sure that driving is indeed the least expensive method. Be sure to email us with your travel plans as soon as possible. Try to include: Departure date from home, location (for mileage calculation), and approximate time of arrival in Madison Departure date and approximate time from Madison, and return location (for mileage calculation) if different than above 2. You Are Paying for Your Travel \u00b6 If you are paying for your own travel or if someone else is paying for it, go ahead and make your travel arrangements now! Just remember to arrive on Sunday before 6pm and depart on Saturday (or else check with us first). Be sure to email us with your travel plans as soon as possible. Try to include: Transportation provider(s) (e.g., airline) Arrival date and approximate time Departure date and approximate time Arrival and departure location within Madison (e.g., airport, bus station, etc.)","title":"Travel planning"},{"location":"logistics/travel/#travel-to-and-from-madison","text":"Whether we offered to pay your travel costs or not, please make sure that we get a copy of your travel plans so that we know when to expect you here and can plan accurately. (If we offered to pay for your hotel room, we will pay for the six nights of the schedule.) Find the numbered section below that applies to you:","title":"Travel To and From Madison"},{"location":"logistics/travel/#1-we-offered-to-pay-for-your-travel","text":"We budgeted an average of $525 per participant for travel to and from Madison (except in cases of flying internationally or driving from nearby). Many travelers will find good options for less than that amount, and some will have to go over that amount. Our goal is to have everyone find a reasonable travel itinerary and not exceed our overall budget. When looking for a reasonable and cost-effective means to travel to and from Madison, here are a few ideas: Within about 5 hours driving of Madison, consider driving or taking a bus; we can support all kinds of travel If you fly, be flexible (within reason) about departure times \u2014 early and late flights are often the least expensive Consider flying into Milwaukee (1\u00bd hours away) or Chicago (2\u00bd hours away) and taking a direct bus to/from Madison (details below) In any case, try to complete your travel plans by early June, before rates go up significantly. See below for details about each travel mode:","title":"1. We Offered to Pay for Your Travel"},{"location":"logistics/travel/#travel-by-airplane","text":"Do NOT buy your own airline tickets . University rules require the University\u2019s travel agency, Fox World Travel (FWT), to purchase your tickets. To arrange flights to/from Madison: Prepare your travel information, requirements, and preferences: Basic personal information, including full legal name (as on government IDs), date of birth, and possibly a phone number Travel requirements, including starting and ending airport(s), hard timing constraints \u2014 preferences, such as not liking early flights, are different than hard constraints! Travel preferences, including timing preferences, airplane seating locations, and so forth If you wish, you may search for ( but not purchase ) flights ahead of time for your own information \u2014 but note that FWT may find different flights or may see different costs, and all tickets must be purchased by FWT and follow UW rules. If flights to/from Madison are expensive, the FWT agent may check flights to/from Milwaukee or Chicago and bus schedules from those airports to Madison: Bus between Mitchell Airport (MKE) in Milwaukee and downtown Madison: Badger Bus (~$20 each way) Bus between O\u2019Hare Airport (ORD) in Chicago and downtown Madison: Van Galder Bus (~$30 each way) Bus between Midway Airport (MDW) in Chicago and downtown Madison: Van Galder Bus (~$30 each way) 2. Contact Fox World Travel By phone: Within the United States, use 866-230-8787 and select option 4; internationally, use +1 920-230-6467 and select option 4. Tell them that you are part of Group Code 09UW1028. Agents are available Monday through Friday, 7:00 a.m. until 5:30 p.m. U.S. Central Time. By email: Send to uwgroups@foxworldtravel.com and include Group Code 09UW1028 in the subject line. 3. In most cases, FWT will need to get our approval for your itinerary before completing the purchase, so it may take up to a day to get final confirmation. Airplane seats cannot be held without purchase over a weekend, so it is best to avoid contacting FWT late on Fridays. Note: If you are including bus tickets as part of your flying itinerary, you may purchase them through FWT when you arrange your flights, or for maximum flexibility purchase them from bus drivers on the day of travel.","title":"Travel by Airplane"},{"location":"logistics/travel/#travel-by-bus","text":"Typically, reasonable bus options \u2014 i.e., those that do not take an excessive amount of time to reach Madison \u2014 cost much less than air options. Note that you may purchase bus tickets through Fox World Travel (see above in the Travel by Airplane section) or yourself before or on the day of travel. If you purchase your own tickets, you must get our approval for the estimated cost first, then request reimbursement from us after the School. When you purchase your own tickets, save the original receipt (even if by email). It would be best if the ticket contains the following information, but a regular ticket stub (e.g., without your name or date) should work fine. Just get what you can! If possible, we would like: Complete itinerary Your name Date of purchase Total amount paid Be sure to email us with your travel plans as soon as possible. Try to include: Transportation provider(s) (e.g., Van Galder bus) Arrival date and approximate time Departure date and approximate time Arrival and departure location within Madison Actual or estimated cost (indicate which)","title":"Travel by Bus"},{"location":"logistics/travel/#travel-by-personal-car","text":"If you are driving to Madison, you will be reimbursed the mileage rate of $0.535/mile for the round-trip distance (as calculated by Google Maps), plus tolls. Also, we will pay for parking costs for the week at the hotel in Madison (but not elsewhere). It may help to keep your receipts for tolls. Note: Due to the high mileage reimbursement rate, driving can be an expensive option! We reserve the right to limit your total driving reimbursement, so work with us on the details. To travel by personal car, please check with us first. We may search for comparable flight options, to make sure that driving is indeed the least expensive method. Be sure to email us with your travel plans as soon as possible. Try to include: Departure date from home, location (for mileage calculation), and approximate time of arrival in Madison Departure date and approximate time from Madison, and return location (for mileage calculation) if different than above","title":"Travel by Personal Car"},{"location":"logistics/travel/#2-you-are-paying-for-your-travel","text":"If you are paying for your own travel or if someone else is paying for it, go ahead and make your travel arrangements now! Just remember to arrive on Sunday before 6pm and depart on Saturday (or else check with us first). Be sure to email us with your travel plans as soon as possible. Try to include: Transportation provider(s) (e.g., airline) Arrival date and approximate time Departure date and approximate time Arrival and departure location within Madison (e.g., airport, bus station, etc.)","title":"2. You Are Paying for Your Travel"},{"location":"logistics/wednesday-activities/","text":"table.schedule { border-collapse: collapse; margin: 1em 0 1em 2em; } th, td { padding: 2px 0; vertical-align: top; } th + th, th + td, td + td { padding-left: 1em; } th.side { text-align: right; } td.time { text-align: right; } .hi { color: \\#F60; /\\* font-weight: bold; \\*/ } tr.other { background-color: \\#FFEEBF; } tr.organized { background-color: \\#D9F0FF; } Wednesday Afternoon Options \u00b6 After Wednesday morning instruction and lunch, there are no planned instructional activities so you are free to explore Madison, rest, or stick around and talk to instructors or work on your projects. Transportation and Costs \u00b6 Please note that School-Organized Activities are all free and walkable. If you are interested in the On Your Own activities also listed, or prefer not to walk, you will be responsible for you own transportation costs and activity costs. We cannot reimburse transportation costs for these Wednesday afternoon activities. Map of Activity Locations \u00b6 School-Organized Activities \u00b6 Immediately following lunch, select School instructors will accompany interested students for the below activities, which are all free and walkable. You are welcome to arrange alternative transportation, but are responsible for any transportation costs. Activity Detail Logistics UW-Madison Campus Tour (A in map) 1:15 PM 60-minute tour gather at Union South after Lunch FREE Tour of Wisconsin State Capitol (B) 3:00 PM (or 2:00 PM on your own) 45-minute tours walk to Capitol after Campus Tour FREE (or meet at the Capitol main desk) Concert on the Square (C) 5:00 PM (music at 7:00 PM) Wisconsin Chamber Orchestra performs; vendors Meet-Up point (C in map) Christina with blankets in grass near Meet-Up point (C in map) On Your Own \u00b6 Though the OSG User School has organized some activities (above) you are also welcome to strike out on your own. Just remember that you'll be responsible for any/all costs, including transportation. We've listed some ideas below. FREE! \u00b6 Activity Detail Logistics Chazen Museum of Art (D in map) until 5:00 PM large collections of art from a variety of eras/styles campus (750 Univesity Avenue) donations welcome short walk from School/Hotels UW\u2013Madison Geology Museum (E) until 4:30 PM large collection of geological specimens Geosciences (1215 Dayton Street) donations welcome across the street from the School building L.R. Ingersoll Physics Museum (F) until 4:00 PM on-campus, kid-friendly museum Chamberlin Hall (Physics) donations welcome very short walk from School/Hotels Henry Vilas Zoo (G) until 5:00 PM 1 mile walk south of Computer Sciences 702 South Randall Avenue donations welcome Buses 4,44 Some Cost \u00b6 Activity Detail Logistics The Sett (A in map) varies per activity upstairs: sports pub and outdoor terraces Union South (East end) downstairs: rock climbing, pool, bowling adjacent to School Tour of First Unitarian Society\u2019s Meeting House (H) at 2:30 PM designed by Frank Lloyd Wright 900 University Bay Drive $10 each, ONLY UP TO 10 PEOPLE Buses 2, 10, 15, 56, 57, 70, 71, 72 Olbrich Botanical Gardens (I) until 4:00 PM 16 acres outdoor (FREE) 3330 Atwood Avenue indoor: $2 conservatory; $7 butterfly house Buses 3, 37, 38 Capital Brewery Tour (J) until 9:00 PM includes commemorative glass and 4 samples 7734 Terrace Ave., Middleton $7 walk-in Buses 70, 71, 72 Paddling Rentals on Lake Mendota until sunset up to $17 / hr Memorial Union Terrace","title":"Wednesday activities"},{"location":"logistics/wednesday-activities/#wednesday-afternoon-options","text":"After Wednesday morning instruction and lunch, there are no planned instructional activities so you are free to explore Madison, rest, or stick around and talk to instructors or work on your projects.","title":"Wednesday Afternoon Options"},{"location":"logistics/wednesday-activities/#transportation-and-costs","text":"Please note that School-Organized Activities are all free and walkable. If you are interested in the On Your Own activities also listed, or prefer not to walk, you will be responsible for you own transportation costs and activity costs. We cannot reimburse transportation costs for these Wednesday afternoon activities.","title":"Transportation and Costs"},{"location":"logistics/wednesday-activities/#map-of-activity-locations","text":"","title":"Map of Activity Locations"},{"location":"logistics/wednesday-activities/#school-organized-activities","text":"Immediately following lunch, select School instructors will accompany interested students for the below activities, which are all free and walkable. You are welcome to arrange alternative transportation, but are responsible for any transportation costs. Activity Detail Logistics UW-Madison Campus Tour (A in map) 1:15 PM 60-minute tour gather at Union South after Lunch FREE Tour of Wisconsin State Capitol (B) 3:00 PM (or 2:00 PM on your own) 45-minute tours walk to Capitol after Campus Tour FREE (or meet at the Capitol main desk) Concert on the Square (C) 5:00 PM (music at 7:00 PM) Wisconsin Chamber Orchestra performs; vendors Meet-Up point (C in map) Christina with blankets in grass near Meet-Up point (C in map)","title":"School-Organized Activities"},{"location":"logistics/wednesday-activities/#on-your-own","text":"Though the OSG User School has organized some activities (above) you are also welcome to strike out on your own. Just remember that you'll be responsible for any/all costs, including transportation. We've listed some ideas below.","title":"On Your Own"},{"location":"logistics/wednesday-activities/#free","text":"Activity Detail Logistics Chazen Museum of Art (D in map) until 5:00 PM large collections of art from a variety of eras/styles campus (750 Univesity Avenue) donations welcome short walk from School/Hotels UW\u2013Madison Geology Museum (E) until 4:30 PM large collection of geological specimens Geosciences (1215 Dayton Street) donations welcome across the street from the School building L.R. Ingersoll Physics Museum (F) until 4:00 PM on-campus, kid-friendly museum Chamberlin Hall (Physics) donations welcome very short walk from School/Hotels Henry Vilas Zoo (G) until 5:00 PM 1 mile walk south of Computer Sciences 702 South Randall Avenue donations welcome Buses 4,44","title":"FREE!"},{"location":"logistics/wednesday-activities/#some-cost","text":"Activity Detail Logistics The Sett (A in map) varies per activity upstairs: sports pub and outdoor terraces Union South (East end) downstairs: rock climbing, pool, bowling adjacent to School Tour of First Unitarian Society\u2019s Meeting House (H) at 2:30 PM designed by Frank Lloyd Wright 900 University Bay Drive $10 each, ONLY UP TO 10 PEOPLE Buses 2, 10, 15, 56, 57, 70, 71, 72 Olbrich Botanical Gardens (I) until 4:00 PM 16 acres outdoor (FREE) 3330 Atwood Avenue indoor: $2 conservatory; $7 butterfly house Buses 3, 37, 38 Capital Brewery Tour (J) until 9:00 PM includes commemorative glass and 4 samples 7734 Terrace Ave., Middleton $7 walk-in Buses 70, 71, 72 Paddling Rentals on Lake Mendota until sunset up to $17 / hr Memorial Union Terrace","title":"Some Cost"},{"location":"materials/","text":"2017 OSG User School Materials \u00b6 Monday \u00b6 Monday Morning: Introduction to HTC \u00b6 Lecture: Introduction to HTC ( PDF ) Exercise 1.1: Log in to the local submit machine and look around Exercise 1.2: Experiment with basic HTCondor commands Exercise 1.3: Run jobs! Exercise 1.4: Read and interpret log files Exercise 1.5: Determining Resource Needs Exercise 1.6: Remove jobs from the queue Bonus Exercise 1.7: Compile and run some C code Monday Morning: More HTCondor \u00b6 Lecture: More HTCondor ( PDF ) Exercise 2.1: Explore condor_q Exercise 2.2: Explore condor_status Exercise 2.3: Work with input and output files Exercise 2.4: Use queue <em>N</em> , $(Cluster) , and $(Process) Exercise 2.5: Use queue matching , and a custom variable Exercise 2.6: Use queue from , and custom variables Monday Afternoon: Retrying jobs and Workflows with DAGMan \u00b6 Lecture: Intermediate HTCondor: Workflows ( PDF ) Exercise 3.1: A job that needs retries Exercise 3.2: A brief detour through the Mandelbrot set Exercise 3.3: Coordinating set of jobs: A simple DAG Exercise 3.4: A more complex DAG Monday Afternoon: Intermediate Workflows with DAGMan \u00b6 Lecture: HTCondor: More on Workflows ( PDF ) Exercise 4.1: Handling jobs that fail with DAGMan Exercise 4.2: Simpler DAGs with variable substitutions Exercise 4.3: Using DAG SPLICE for node organization Bonus Exercise 4.4: HTCondor challenges (If and only if you have time) Tuesday \u00b6 Tuesday Morning: Introduction to Distributed HTC and Overlay Systems \u00b6 Lecture: Introduction to DHTC ( PDF ) Exercise 1.1: Refresher - Submitting multiple jobs Exercise 1.2: Log in to the OSG submit machine Exercise 1.3: Running jobs in the OSG Tuesday Morning: Comparing Local and Remote HTC \u00b6 Lecture: What is different about overlay systems? ( PDF ) Exercise 2.1: Hardware differences in the OSG Exercise 2.2: Software differences in the OSG Tuesday Afternoon: Security in OSG \u00b6 Lecture: Security in OSG ( PDF ) Tuesday Afternoon: Troubleshooting jobs \u00b6 Lecture: Troubleshooting jobs ( PDF ) Exercise 3.1: Troubleshooting a DAG Tuesday Afternoon: Connecting to OSG \u00b6 Lecture: Ways to Connect to OSG ( PDF ) Wednesday \u00b6 Wednesday Morning: Software Portability \u00b6 Lecture: Software Portability for DHTC ( PDF ) Exercise 1.1: Compiling programs for portability Exercise 1.2: Using a pre-compiled binary Exercise 1.3: Using a wrapper script Exercise 1.4: Pre-packaging code Bonus Exercise 1.5: Passing Arguments Through the Wrapper Script Wednesday Morning: Software Limitations \u00b6 Lecture: Considerations for licensing and programming packages ( PDF ) Exercise 1.6: Compile and run Matlab code Exercise 1.7: Pre-packaging Python Exercise 1.8: In-job installation of Python Bonus Exercise 1.9: Using containers Wednesday Afternoon: On Your Own \u00b6 Ideas for activities Thursday \u00b6 Thursday Morning: OSG Connect \u00b6 Lecture: OSG Connect ( PDF ) Exercise 1.1: Get acquainted with OSG Connect Exercise 1.2: Do the OSG Connect \"quickstart\" Exercise 1.3: Try an OSG Connect software module Exercise 1.4: Try Singularity Container Job on the OSG (Optional) Thursday Morning: Data Handling \u00b6 Lecture: Overall data considerations ( PDF ) Exercise 2.1: Understanding your data requirements Exercise 2.2: HTCondor file transfer and compression Exercise 2.3: Splitting large input data Thursday Afternoon: Data Handling (continued) \u00b6 Lecture: Solutions for large input data ( PDF ) Exercise 3.1: Using a web proxy for large, shared input Exercise 3.2: Using StashCache for large, shared input Exercise 3.3: Using StashCache for large, unique input Thursday Afternoon: Data Handling (continued) \u00b6 Lecture: Large output and shared file systems; Data summary ( PDF ) Exercise 4.1: Using a local shared filesystem for large input files Exercise 4.2: Using a local shared filesystem for large output files The submit host user-training.osgconnect.net will be active for about two weeks. For long term use, please sign up for OSG Connect . If you have any questions about the signup process, please email user-support@opensciencegrid.org Friday \u00b6 Friday Morning: From Science to Production Workflows \u00b6 Lecture: From Science to Real Workflow ( PDF ) Exercise 1.1: Learn about Joe\u2019s Desired Computing Work Exercise 1.2: Plan Overall Workflow Friday Morning: From Science to Production Workflows \u00b6 Lecture: From Workflow to Automated Production ( PDF ) Exercise 1.3: Execute Joe\u2019s Workflow Bonus Exercise 1.4: Further Optimization and Scaling Friday Afternoon: HTC Showcase \u00b6 Talk: Dane Morgan , Engineering: HTC for Engineering Better Materials ( PDF ) Talk: Megan Frayer , Genetics: Using HTC in Genomic Ancestry Analysis ( PDF ) Talk: William Cocke , Mathematics: Building a Character Table Database: A Use of Condor in Pure Math ( PDF ) Talk: Edgar Spalding , Botany: HTC As a Tool to Study How Plant Genomes Function ( PDF ) Friday Afternoon: Foundations of HTC \u00b6 Lecture: The Principles of HTC ( PDF ) Friday Afternoon: Wrap Up \u00b6 Lecture: Where to Go and What to Do Next ( PDF )","title":"All materials"},{"location":"materials/#2017-osg-user-school-materials","text":"","title":"2017 OSG User School Materials"},{"location":"materials/#monday","text":"","title":"Monday"},{"location":"materials/#monday-morning-introduction-to-htc","text":"Lecture: Introduction to HTC ( PDF ) Exercise 1.1: Log in to the local submit machine and look around Exercise 1.2: Experiment with basic HTCondor commands Exercise 1.3: Run jobs! Exercise 1.4: Read and interpret log files Exercise 1.5: Determining Resource Needs Exercise 1.6: Remove jobs from the queue Bonus Exercise 1.7: Compile and run some C code","title":"Monday Morning: Introduction to HTC"},{"location":"materials/#monday-morning-more-htcondor","text":"Lecture: More HTCondor ( PDF ) Exercise 2.1: Explore condor_q Exercise 2.2: Explore condor_status Exercise 2.3: Work with input and output files Exercise 2.4: Use queue <em>N</em> , $(Cluster) , and $(Process) Exercise 2.5: Use queue matching , and a custom variable Exercise 2.6: Use queue from , and custom variables","title":"Monday Morning: More HTCondor"},{"location":"materials/#monday-afternoon-retrying-jobs-and-workflows-with-dagman","text":"Lecture: Intermediate HTCondor: Workflows ( PDF ) Exercise 3.1: A job that needs retries Exercise 3.2: A brief detour through the Mandelbrot set Exercise 3.3: Coordinating set of jobs: A simple DAG Exercise 3.4: A more complex DAG","title":"Monday Afternoon: Retrying jobs and Workflows with DAGMan"},{"location":"materials/#monday-afternoon-intermediate-workflows-with-dagman","text":"Lecture: HTCondor: More on Workflows ( PDF ) Exercise 4.1: Handling jobs that fail with DAGMan Exercise 4.2: Simpler DAGs with variable substitutions Exercise 4.3: Using DAG SPLICE for node organization Bonus Exercise 4.4: HTCondor challenges (If and only if you have time)","title":"Monday Afternoon: Intermediate Workflows with DAGMan"},{"location":"materials/#tuesday","text":"","title":"Tuesday"},{"location":"materials/#tuesday-morning-introduction-to-distributed-htc-and-overlay-systems","text":"Lecture: Introduction to DHTC ( PDF ) Exercise 1.1: Refresher - Submitting multiple jobs Exercise 1.2: Log in to the OSG submit machine Exercise 1.3: Running jobs in the OSG","title":"Tuesday Morning: Introduction to Distributed HTC and Overlay Systems"},{"location":"materials/#tuesday-morning-comparing-local-and-remote-htc","text":"Lecture: What is different about overlay systems? ( PDF ) Exercise 2.1: Hardware differences in the OSG Exercise 2.2: Software differences in the OSG","title":"Tuesday Morning: Comparing Local and Remote HTC"},{"location":"materials/#tuesday-afternoon-security-in-osg","text":"Lecture: Security in OSG ( PDF )","title":"Tuesday Afternoon: Security in OSG"},{"location":"materials/#tuesday-afternoon-troubleshooting-jobs","text":"Lecture: Troubleshooting jobs ( PDF ) Exercise 3.1: Troubleshooting a DAG","title":"Tuesday Afternoon: Troubleshooting jobs"},{"location":"materials/#tuesday-afternoon-connecting-to-osg","text":"Lecture: Ways to Connect to OSG ( PDF )","title":"Tuesday Afternoon: Connecting to OSG"},{"location":"materials/#wednesday","text":"","title":"Wednesday"},{"location":"materials/#wednesday-morning-software-portability","text":"Lecture: Software Portability for DHTC ( PDF ) Exercise 1.1: Compiling programs for portability Exercise 1.2: Using a pre-compiled binary Exercise 1.3: Using a wrapper script Exercise 1.4: Pre-packaging code Bonus Exercise 1.5: Passing Arguments Through the Wrapper Script","title":"Wednesday Morning: Software Portability"},{"location":"materials/#wednesday-morning-software-limitations","text":"Lecture: Considerations for licensing and programming packages ( PDF ) Exercise 1.6: Compile and run Matlab code Exercise 1.7: Pre-packaging Python Exercise 1.8: In-job installation of Python Bonus Exercise 1.9: Using containers","title":"Wednesday Morning: Software Limitations"},{"location":"materials/#wednesday-afternoon-on-your-own","text":"Ideas for activities","title":"Wednesday Afternoon: On Your Own"},{"location":"materials/#thursday","text":"","title":"Thursday"},{"location":"materials/#thursday-morning-osg-connect","text":"Lecture: OSG Connect ( PDF ) Exercise 1.1: Get acquainted with OSG Connect Exercise 1.2: Do the OSG Connect \"quickstart\" Exercise 1.3: Try an OSG Connect software module Exercise 1.4: Try Singularity Container Job on the OSG (Optional)","title":"Thursday Morning: OSG Connect"},{"location":"materials/#thursday-morning-data-handling","text":"Lecture: Overall data considerations ( PDF ) Exercise 2.1: Understanding your data requirements Exercise 2.2: HTCondor file transfer and compression Exercise 2.3: Splitting large input data","title":"Thursday Morning: Data Handling"},{"location":"materials/#thursday-afternoon-data-handling-continued","text":"Lecture: Solutions for large input data ( PDF ) Exercise 3.1: Using a web proxy for large, shared input Exercise 3.2: Using StashCache for large, shared input Exercise 3.3: Using StashCache for large, unique input","title":"Thursday Afternoon: Data Handling (continued)"},{"location":"materials/#thursday-afternoon-data-handling-continued_1","text":"Lecture: Large output and shared file systems; Data summary ( PDF ) Exercise 4.1: Using a local shared filesystem for large input files Exercise 4.2: Using a local shared filesystem for large output files The submit host user-training.osgconnect.net will be active for about two weeks. For long term use, please sign up for OSG Connect . If you have any questions about the signup process, please email user-support@opensciencegrid.org","title":"Thursday Afternoon: Data Handling (continued)"},{"location":"materials/#friday","text":"","title":"Friday"},{"location":"materials/#friday-morning-from-science-to-production-workflows","text":"Lecture: From Science to Real Workflow ( PDF ) Exercise 1.1: Learn about Joe\u2019s Desired Computing Work Exercise 1.2: Plan Overall Workflow","title":"Friday Morning: From Science to Production Workflows"},{"location":"materials/#friday-morning-from-science-to-production-workflows_1","text":"Lecture: From Workflow to Automated Production ( PDF ) Exercise 1.3: Execute Joe\u2019s Workflow Bonus Exercise 1.4: Further Optimization and Scaling","title":"Friday Morning: From Science to Production Workflows"},{"location":"materials/#friday-afternoon-htc-showcase","text":"Talk: Dane Morgan , Engineering: HTC for Engineering Better Materials ( PDF ) Talk: Megan Frayer , Genetics: Using HTC in Genomic Ancestry Analysis ( PDF ) Talk: William Cocke , Mathematics: Building a Character Table Database: A Use of Condor in Pure Math ( PDF ) Talk: Edgar Spalding , Botany: HTC As a Tool to Study How Plant Genomes Function ( PDF )","title":"Friday Afternoon: HTC Showcase"},{"location":"materials/#friday-afternoon-foundations-of-htc","text":"Lecture: The Principles of HTC ( PDF )","title":"Friday Afternoon: Foundations of HTC"},{"location":"materials/#friday-afternoon-wrap-up","text":"Lecture: Where to Go and What to Do Next ( PDF )","title":"Friday Afternoon: Wrap Up"},{"location":"materials/connect/","text":"OSG Connect Sign-up Instructions \u00b6 On Tuesday afternoon, you will use OSG Connect to run jobs on OSG. Before then, please sign up for an OSG Connect account: Visit the OSG Connect sign up page ( https://osgconnect.net/signup ) and click on the Continue button. This takes you to a page which will let you create a Globus ID or log in if you already have one. After you have signed in or created a Globus ID, you will be asked to link an existing account or continue with the current Globus account. Click on the \u201cNo Thanks, continue\u201d button to continue the application process. To complete your sign up, submit an application to join the \"osg\" project. It is important that you choose your Globus ID in the username dropdown menu . (If not sure, take a close look at Figure 3 in the OSG Connect sign up page.) In the next few minutes, you should get an automated email message that acknowledges the receipt of your application. Questions or Need Help with Sign Up \u00b6 Please ask Bala (helper) or send an email to user-support@opensciencegrid.org .","title":"Set up OSG Connect"},{"location":"materials/connect/#osg-connect-sign-up-instructions","text":"On Tuesday afternoon, you will use OSG Connect to run jobs on OSG. Before then, please sign up for an OSG Connect account: Visit the OSG Connect sign up page ( https://osgconnect.net/signup ) and click on the Continue button. This takes you to a page which will let you create a Globus ID or log in if you already have one. After you have signed in or created a Globus ID, you will be asked to link an existing account or continue with the current Globus account. Click on the \u201cNo Thanks, continue\u201d button to continue the application process. To complete your sign up, submit an application to join the \"osg\" project. It is important that you choose your Globus ID in the username dropdown menu . (If not sure, take a close look at Figure 3 in the OSG Connect sign up page.) In the next few minutes, you should get an automated email message that acknowledges the receipt of your application.","title":"OSG Connect Sign-up Instructions"},{"location":"materials/connect/#questions-or-need-help-with-sign-up","text":"Please ask Bala (helper) or send an email to user-support@opensciencegrid.org .","title":"Questions or Need Help with Sign Up"},{"location":"materials/day1/part1-ex1-login/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Monday Exercise 1.1: Log In and Look Around \u00b6 The goal of this first exercise is simply to log in to the local submit machine and look around a little bit. This exercise should take only a few minutes. If you have trouble getting ssh access to the submit machine, ask the instructors right away! Gaining access is critical for all remaining exercises. Logging In \u00b6 Today, you will use the machine named learn.chtc.wisc.edu for all of your exercises. To log in, use a Secure Shell (SSH) client. On OS X, run the Terminal app and use the ssh command, like so: %UCL_PROMPT_SHORT% <strong>ssh username@learn.chtc.wisc.edu</strong> On Windows, we recommend a free client called PuTTY , but any SSH client should be fine On Linux, open a terminal window and use the ssh command, as exemplified above for Mac. If you need help finding or using an SSH client, ask the instructors for help right away! Once you have an SSH client, use it to log in to the submit machine. Some tips: The hostname of the submit machine is learn.chtc.wisc.edu Your username and initial password are located on the Accounts sheet of paper that you received this morning While the passwd command, your initial password will be automatically reset for you on an hourly basis. (So you probably don't want to change your password, in the first place, and definitely want to keep your sheet of paper or memorize the password). Running Commands \u00b6 In the exercises, we will show commands that you are supposed to type or copy into the command line, like this: %UCL_PROMPT_SHORT% <strong>hostname</strong> learn.chtc.wisc.edu Note: In the first line of the example above, the %UCL_PROMPT_SHORT% part is meant to show the Linux command-line prompt. You do not type this part! Further, your actual prompt probably is a bit different, and that is expected. So in the example above, the command that you type at your own prompt is just the eight characters hostname . The second line of the example, without the prompt, shows the output of the command; you do not type this part, either. Here are a few other commands that you can try, to learn a little bit about this machine (the examples below do not show the output from each command): %UCL_PROMPT_SHORT% <strong>date</strong> %UCL_PROMPT_SHORT% <strong>uname -a</strong> A suggestion for the day: Try typing into the command line as many of the commands as you can. Copy-and-paste is fine, of course, but you WILL learn more if you take the time to type each command, yourself. Organizing Your Workspace \u00b6 You will be doing many different exercises over the next few days, many of them on this submit machine. Each exercise may use many files, once finished. To avoid confusion, it may be useful to create a separate directory for each exercise. For instance, for the rest of this exercise, you may wish to create and use a directory named monday-1.1-login , or something like that. %UCL_PROMPT_SHORT% <strong>mkdir monday-1.1-login</strong> %UCL_PROMPT_SHORT% <strong>cd monday-1.1-login</strong> Showing the Version of HTCondor \u00b6 HTCondor is installed on this machine. But what version? You can ask HTCondor itself: %UCL_PROMPT_SHORT% <strong>condor_version</strong> $CondorVersion: 8.7.2 Jun 02 2017 BuildID: 407060 $ $CondorPlatform: x86_64_RedHat6 $ As you can see from the output, we are using HTCondor 8.7.2, which is the most recently released development version. Background information about HTCondor version numbers \u00b6 HTCondor always has two types of releases at one time: stable and development. HTCondor 8.4.x and 8.6.x are considered stable releases, and you can know they are stable because the second digits (e.g., 4 or 6 in these cases) are even numbers. Within one stable series, all versions have the same features (for example 8.4.0 and 8.4.8 have the same set of features) and differ only in bug and security fixes. HTCondor 8.7.2 is the current development release series of HTCondor. You know that it is a development release because the second digit (i.e., 7) is an odd number. Typically, the current development series (i.e., 8.7.x) has greater version numbers than the current stable series (i.e., 8.6.x), but that is not always true. In any case, development releases add new features and are more likely to have problems. For that reason, we typically do not use development releases for the School, but in this case, 8.7.2 is considered stable enough. References \u00b6 Here are a few links to reference materials that might be interesting now or later. HTCondor home page HTCondor manuals ; it is probably best to read the manual corresponding to the version of HTCondor that you use (8.7.2 for today) Center for High Throughput Computing , our local research computing organization","title":"1.1. Log in"},{"location":"materials/day1/part1-ex1-login/#monday-exercise-11-log-in-and-look-around","text":"The goal of this first exercise is simply to log in to the local submit machine and look around a little bit. This exercise should take only a few minutes. If you have trouble getting ssh access to the submit machine, ask the instructors right away! Gaining access is critical for all remaining exercises.","title":"Monday Exercise 1.1: Log In and Look Around"},{"location":"materials/day1/part1-ex1-login/#logging-in","text":"Today, you will use the machine named learn.chtc.wisc.edu for all of your exercises. To log in, use a Secure Shell (SSH) client. On OS X, run the Terminal app and use the ssh command, like so: %UCL_PROMPT_SHORT% <strong>ssh username@learn.chtc.wisc.edu</strong> On Windows, we recommend a free client called PuTTY , but any SSH client should be fine On Linux, open a terminal window and use the ssh command, as exemplified above for Mac. If you need help finding or using an SSH client, ask the instructors for help right away! Once you have an SSH client, use it to log in to the submit machine. Some tips: The hostname of the submit machine is learn.chtc.wisc.edu Your username and initial password are located on the Accounts sheet of paper that you received this morning While the passwd command, your initial password will be automatically reset for you on an hourly basis. (So you probably don't want to change your password, in the first place, and definitely want to keep your sheet of paper or memorize the password).","title":"Logging In"},{"location":"materials/day1/part1-ex1-login/#running-commands","text":"In the exercises, we will show commands that you are supposed to type or copy into the command line, like this: %UCL_PROMPT_SHORT% <strong>hostname</strong> learn.chtc.wisc.edu Note: In the first line of the example above, the %UCL_PROMPT_SHORT% part is meant to show the Linux command-line prompt. You do not type this part! Further, your actual prompt probably is a bit different, and that is expected. So in the example above, the command that you type at your own prompt is just the eight characters hostname . The second line of the example, without the prompt, shows the output of the command; you do not type this part, either. Here are a few other commands that you can try, to learn a little bit about this machine (the examples below do not show the output from each command): %UCL_PROMPT_SHORT% <strong>date</strong> %UCL_PROMPT_SHORT% <strong>uname -a</strong> A suggestion for the day: Try typing into the command line as many of the commands as you can. Copy-and-paste is fine, of course, but you WILL learn more if you take the time to type each command, yourself.","title":"Running Commands"},{"location":"materials/day1/part1-ex1-login/#organizing-your-workspace","text":"You will be doing many different exercises over the next few days, many of them on this submit machine. Each exercise may use many files, once finished. To avoid confusion, it may be useful to create a separate directory for each exercise. For instance, for the rest of this exercise, you may wish to create and use a directory named monday-1.1-login , or something like that. %UCL_PROMPT_SHORT% <strong>mkdir monday-1.1-login</strong> %UCL_PROMPT_SHORT% <strong>cd monday-1.1-login</strong>","title":"Organizing Your Workspace"},{"location":"materials/day1/part1-ex1-login/#showing-the-version-of-htcondor","text":"HTCondor is installed on this machine. But what version? You can ask HTCondor itself: %UCL_PROMPT_SHORT% <strong>condor_version</strong> $CondorVersion: 8.7.2 Jun 02 2017 BuildID: 407060 $ $CondorPlatform: x86_64_RedHat6 $ As you can see from the output, we are using HTCondor 8.7.2, which is the most recently released development version.","title":"Showing the Version of HTCondor"},{"location":"materials/day1/part1-ex1-login/#background-information-about-htcondor-version-numbers","text":"HTCondor always has two types of releases at one time: stable and development. HTCondor 8.4.x and 8.6.x are considered stable releases, and you can know they are stable because the second digits (e.g., 4 or 6 in these cases) are even numbers. Within one stable series, all versions have the same features (for example 8.4.0 and 8.4.8 have the same set of features) and differ only in bug and security fixes. HTCondor 8.7.2 is the current development release series of HTCondor. You know that it is a development release because the second digit (i.e., 7) is an odd number. Typically, the current development series (i.e., 8.7.x) has greater version numbers than the current stable series (i.e., 8.6.x), but that is not always true. In any case, development releases add new features and are more likely to have problems. For that reason, we typically do not use development releases for the School, but in this case, 8.7.2 is considered stable enough.","title":"Background information about HTCondor version numbers"},{"location":"materials/day1/part1-ex1-login/#references","text":"Here are a few links to reference materials that might be interesting now or later. HTCondor home page HTCondor manuals ; it is probably best to read the manual corresponding to the version of HTCondor that you use (8.7.2 for today) Center for High Throughput Computing , our local research computing organization","title":"References"},{"location":"materials/day1/part1-ex2-commands/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Monday Exercise 1.2: Experiment With Basic HTCondor Commands \u00b6 The goal of this exercise is to use the basic informational HTCondor commands, condor_status and condor_q . They will be useful for monitoring your jobs and available slots throughout the day. This exercise should take only a few minutes. Viewing Slots \u00b6 As discussed in the lecture, the condor_status command is used to view the current state of slots in an HTCondor pool. At its most basic, the command is very simple: %UCL_PROMPT_SHORT% <strong>condor_status</strong> This command, running on our (CHTC) pool, will produce a lot of output; there is one line per slot, and we typically have over 10,000 slots. TIP: You can widen your terminal window, which may help you to see all details of the output better. Here is some example output (what you see will be longer): slot1_31@e437.chtc.wisc.edu LINUX X86_64 Unclaimed Idle 0.000 8053 0+01:14:34 slot1_32@e437.chtc.wisc.edu LINUX X86_64 Unclaimed Idle 0.000 8053 0+03:57:00 slot1_33@e437.chtc.wisc.edu LINUX X86_64 Unclaimed Idle 0.000 8053 1+00:05:17 slot1@e438.chtc.wisc.edu LINUX X86_64 Owner Idle 0.300 250 7+03:22:21 <em>slot1_1@e438.chtc.wisc.edu LINUX X86_64 Claimed Busy 0.930 1024 0+02:42:08</em> slot1_2@e438.chtc.wisc.edu LINUX X86_64 Claimed Busy 3.530 1024 0+02:40:24 This output consists of 8 columns: Col Example Meaning Name slot1_1@e438.chtc.wisc.edu Slot name and hostname OpSys LINUX Operating system Arch X86_64 Machine architecture (e.g., Intel 64 bit) State Claimed State of the slot ( Unclaimed is available, Owner is being used by the machine owner, Claimed is matched to a job) Activity Busy Is there activity on the slot? LoadAv 0.930 Load average, a measure of CPU activity on the slot Mem 1024 Memory available to the slot, in MB ActvtyTime 0+02:42:08 Amount of time spent in current activity (days + hours:minutes:seconds) At the end of the slot listing, there is a summary. Here is an example: Machines Owner Claimed Unclaimed Matched Preempting Drain X86_64/LINUX 10831 0 10194 631 0 0 6 X86_64/WINDOWS 2 2 0 0 0 0 0 Total 10833 2 10194 631 0 0 6 There is one row of summary for each machine architecture/operating system combination. The columns are the different states that a slot can be in. The final row gives a summary of slot states for the whole pool. Questions: For the sample above, how many 64-bit Linux slots are available? (Hint: Unclaimed = available.) For the sample above, how many slots total are being used by their owner? Now, run condor_status yourself and try these: How many 64-bit Linux slots are in the pool right now? How does that compare to the sample above? How many of those 64-bit Linux slots are available now? Viewing Whole Machines, Only \u00b6 Also try out the -compact for a slightly different view of whole machines, without the individual slots shown. %UCL_PROMPT_SHORT% <strong>condor_status -compact</strong> How has the column information changed? (Below is an example of the top of the output.) Machine Platform Slots Cpus Gpus TotalGb FreCpu FreeGb CpuLoad ST Jobs/Min MaxSlotGb aci-005.chtc.wisc.edu x64/SL6 16 16 58.86 0 1.29 1.00 Cb 0.00 12.00 aci-017.chtc.wisc.edu x64/SL6 14 16 58.86 2 2.86 0.88 ** 0.00 4.00 aci-056.chtc.wisc.edu x64/SL6 7 16 58.86 9 0.86 0.42 ** 0.05 12.00 aci-057.chtc.wisc.edu x64/SL6 6 16 58.82 10 2.82 0.39 ** 0.00 12.00 aci-058.chtc.wisc.edu x64/SL6 6 16 58.86 10 2.86 0.38 ** 0.00 12.00 Viewing Jobs \u00b6 The condor_q command lists jobs that are on this submit machine and that are running or waiting to run. The _q part of the name is meant to suggest the word \u201cqueue\u201d, or list of jobs waiting to finish. Viewing Your Own Jobs \u00b6 The simplest form of the command lists only your jobs: %UCL_PROMPT_SHORT% <strong>condor_q</strong> The main part of the output (which will be empty, because you haven't submitted jobs yet) shows one job ID per line: -- Schedd: learn.chtc.wisc.edu : <128.104.100.43:9618?... @ 07/16/17 09:02:31 OWNER BATCH_NAME SUBMITTED DONE RUN IDLE TOTAL JOB_IDS aapohl CMD: run_ffmpeg.sh 7/17 09:58 _ _ 1 1 18801.0 This output consists of 8 (or 9) columns: Col Example Meaning OWNER aapohl The user ID of the user who submitted the job BATCH_NAME run_ffmpeg.sh The executable or the \"jobbatchname\" specified within submit file(s) SUBMITTED 7/17 09:58 The date and time when the job was submitted DONE _ Number of jobs in this batch that have completed RUN _ Number of jobs in this batch that are currently running IDLE 1 Number of jobs in this batch that are idle, waiting for a match HOLD _ Column will show up if there are jobs on \"hold\" because something about the submission/setup needs to be corrected by the user TOTAL 1 Total number of jobs in this batch JOB_IDS 18801.0 Job ID or range of Job IDs in this batch At the end of the job listing, there is a summary. Here is a sample: 1 jobs; 0 completed, 0 removed, 1 idle, 0 running, 0 held, 0 suspended It shows total counts of jobs in the different possible states. Questions: For the sample above, when was the job submitted? For the sample above, was the job running or not yet? How can you tell? Viewing Everyone\u2019s Jobs \u00b6 By default, the condor_q command shows your jobs only. To see everyone\u2019s jobs that are queued on the machine, add the -all option: %UCL_PROMPT_SHORT% <strong>condor_q -all</strong> Run that command now and use its output to answer the following questions: How many jobs are queued in total (i.e., running or waiting to run)? How many jobs from this submit machine are running right now? Viewing Jobs without the Default \"batch\" Mode \u00b6 The condor_q output, by default, groups \"batches\" of jobs together (if they were submitted with the same submit file as part of the same Cluster, and even for separately submitted Clusters that use the same exact executable). To see more information for EVERY job on a separate line of output, use condor_q -nobatch (or, to see everyone's jobs condor_q -all -nobatch ). %UCL_PROMPT_SHORT% <strong>condor_q -all -nobatch</strong> How has the column information changed? (Below is an example of the top of the output.) -- Schedd: learn.chtc.wisc.edu : <128.104.100.43:9618?... @ 07/17/17 09:58:44 ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 18203.0 s16_alirezakho 7/27 09:51 0+00:00:00 I 0 0.7 pascal 18204.0 s16_alirezakho 7/27 09:51 0+00:00:00 I 0 0.7 pascal 18801.0 aapohl 7/28 16:58 0+00:00:00 I 0 0.0 run_ffmpeg.sh 18997.0 s16_martincum 7/29 10:59 0+00:00:32 I 0 733.0 runR.pl 1_0 run_perm.R 1 0 10 19027.5 s16_martincum 7/29 11:06 0+00:09:20 I 0 2198.0 runR.pl 1_5 run_perm.R 1 5 1000 The -nobatch output shows a line for every job and consists of 8 columns: Col Example Meaning ID 18801.0 Job ID, which is the cluster , a dot character ( . ), and the process OWNER aapohl The user ID of the user who submitted the job SUBMITTED 7/17 09:58 The date and time when the job was submitted RUN_TIME 0+00:00:00 Total time spent running so far (days + hours:minutes:seconds) ST I Status of job: I is Idle (waiting to run), R is Running, H is Held, etc. PRI 0 Job priority (see next lecture) SIZE 0.0 Current run-time memory usage, in MB CMD run_ffmpeg.sh The executable command (with arguments) to be run In future exercises, you'll want to switch between condor_q and condor_q -nobatch to see different types of information about YOUR jobs. Extra Information \u00b6 Both condor_status and condor_q have many command-line options, some of which significantly change their output. You will explore a few of the most useful options in the next lecture and set of exercises, but if you want to experiment now, go ahead! There are a few ways to learn more about the commands: Use the (brief) built-in help for the commands, e.g.: condor_q -h Read the installed man(ual) pages for the commands, e.g.: man condor_q Find the command in the online manual ; note: the text online is the same as the man text, only formatted for the web","title":"1.2. Commands"},{"location":"materials/day1/part1-ex2-commands/#monday-exercise-12-experiment-with-basic-htcondor-commands","text":"The goal of this exercise is to use the basic informational HTCondor commands, condor_status and condor_q . They will be useful for monitoring your jobs and available slots throughout the day. This exercise should take only a few minutes.","title":"Monday Exercise 1.2: Experiment With Basic HTCondor Commands"},{"location":"materials/day1/part1-ex2-commands/#viewing-slots","text":"As discussed in the lecture, the condor_status command is used to view the current state of slots in an HTCondor pool. At its most basic, the command is very simple: %UCL_PROMPT_SHORT% <strong>condor_status</strong> This command, running on our (CHTC) pool, will produce a lot of output; there is one line per slot, and we typically have over 10,000 slots. TIP: You can widen your terminal window, which may help you to see all details of the output better. Here is some example output (what you see will be longer): slot1_31@e437.chtc.wisc.edu LINUX X86_64 Unclaimed Idle 0.000 8053 0+01:14:34 slot1_32@e437.chtc.wisc.edu LINUX X86_64 Unclaimed Idle 0.000 8053 0+03:57:00 slot1_33@e437.chtc.wisc.edu LINUX X86_64 Unclaimed Idle 0.000 8053 1+00:05:17 slot1@e438.chtc.wisc.edu LINUX X86_64 Owner Idle 0.300 250 7+03:22:21 <em>slot1_1@e438.chtc.wisc.edu LINUX X86_64 Claimed Busy 0.930 1024 0+02:42:08</em> slot1_2@e438.chtc.wisc.edu LINUX X86_64 Claimed Busy 3.530 1024 0+02:40:24 This output consists of 8 columns: Col Example Meaning Name slot1_1@e438.chtc.wisc.edu Slot name and hostname OpSys LINUX Operating system Arch X86_64 Machine architecture (e.g., Intel 64 bit) State Claimed State of the slot ( Unclaimed is available, Owner is being used by the machine owner, Claimed is matched to a job) Activity Busy Is there activity on the slot? LoadAv 0.930 Load average, a measure of CPU activity on the slot Mem 1024 Memory available to the slot, in MB ActvtyTime 0+02:42:08 Amount of time spent in current activity (days + hours:minutes:seconds) At the end of the slot listing, there is a summary. Here is an example: Machines Owner Claimed Unclaimed Matched Preempting Drain X86_64/LINUX 10831 0 10194 631 0 0 6 X86_64/WINDOWS 2 2 0 0 0 0 0 Total 10833 2 10194 631 0 0 6 There is one row of summary for each machine architecture/operating system combination. The columns are the different states that a slot can be in. The final row gives a summary of slot states for the whole pool. Questions: For the sample above, how many 64-bit Linux slots are available? (Hint: Unclaimed = available.) For the sample above, how many slots total are being used by their owner? Now, run condor_status yourself and try these: How many 64-bit Linux slots are in the pool right now? How does that compare to the sample above? How many of those 64-bit Linux slots are available now?","title":"Viewing Slots"},{"location":"materials/day1/part1-ex2-commands/#viewing-whole-machines-only","text":"Also try out the -compact for a slightly different view of whole machines, without the individual slots shown. %UCL_PROMPT_SHORT% <strong>condor_status -compact</strong> How has the column information changed? (Below is an example of the top of the output.) Machine Platform Slots Cpus Gpus TotalGb FreCpu FreeGb CpuLoad ST Jobs/Min MaxSlotGb aci-005.chtc.wisc.edu x64/SL6 16 16 58.86 0 1.29 1.00 Cb 0.00 12.00 aci-017.chtc.wisc.edu x64/SL6 14 16 58.86 2 2.86 0.88 ** 0.00 4.00 aci-056.chtc.wisc.edu x64/SL6 7 16 58.86 9 0.86 0.42 ** 0.05 12.00 aci-057.chtc.wisc.edu x64/SL6 6 16 58.82 10 2.82 0.39 ** 0.00 12.00 aci-058.chtc.wisc.edu x64/SL6 6 16 58.86 10 2.86 0.38 ** 0.00 12.00","title":"Viewing Whole Machines, Only"},{"location":"materials/day1/part1-ex2-commands/#viewing-jobs","text":"The condor_q command lists jobs that are on this submit machine and that are running or waiting to run. The _q part of the name is meant to suggest the word \u201cqueue\u201d, or list of jobs waiting to finish.","title":"Viewing Jobs"},{"location":"materials/day1/part1-ex2-commands/#viewing-your-own-jobs","text":"The simplest form of the command lists only your jobs: %UCL_PROMPT_SHORT% <strong>condor_q</strong> The main part of the output (which will be empty, because you haven't submitted jobs yet) shows one job ID per line: -- Schedd: learn.chtc.wisc.edu : <128.104.100.43:9618?... @ 07/16/17 09:02:31 OWNER BATCH_NAME SUBMITTED DONE RUN IDLE TOTAL JOB_IDS aapohl CMD: run_ffmpeg.sh 7/17 09:58 _ _ 1 1 18801.0 This output consists of 8 (or 9) columns: Col Example Meaning OWNER aapohl The user ID of the user who submitted the job BATCH_NAME run_ffmpeg.sh The executable or the \"jobbatchname\" specified within submit file(s) SUBMITTED 7/17 09:58 The date and time when the job was submitted DONE _ Number of jobs in this batch that have completed RUN _ Number of jobs in this batch that are currently running IDLE 1 Number of jobs in this batch that are idle, waiting for a match HOLD _ Column will show up if there are jobs on \"hold\" because something about the submission/setup needs to be corrected by the user TOTAL 1 Total number of jobs in this batch JOB_IDS 18801.0 Job ID or range of Job IDs in this batch At the end of the job listing, there is a summary. Here is a sample: 1 jobs; 0 completed, 0 removed, 1 idle, 0 running, 0 held, 0 suspended It shows total counts of jobs in the different possible states. Questions: For the sample above, when was the job submitted? For the sample above, was the job running or not yet? How can you tell?","title":"Viewing Your Own Jobs"},{"location":"materials/day1/part1-ex2-commands/#viewing-everyones-jobs","text":"By default, the condor_q command shows your jobs only. To see everyone\u2019s jobs that are queued on the machine, add the -all option: %UCL_PROMPT_SHORT% <strong>condor_q -all</strong> Run that command now and use its output to answer the following questions: How many jobs are queued in total (i.e., running or waiting to run)? How many jobs from this submit machine are running right now?","title":"Viewing Everyone\u2019s Jobs"},{"location":"materials/day1/part1-ex2-commands/#viewing-jobs-without-the-default-batch-mode","text":"The condor_q output, by default, groups \"batches\" of jobs together (if they were submitted with the same submit file as part of the same Cluster, and even for separately submitted Clusters that use the same exact executable). To see more information for EVERY job on a separate line of output, use condor_q -nobatch (or, to see everyone's jobs condor_q -all -nobatch ). %UCL_PROMPT_SHORT% <strong>condor_q -all -nobatch</strong> How has the column information changed? (Below is an example of the top of the output.) -- Schedd: learn.chtc.wisc.edu : <128.104.100.43:9618?... @ 07/17/17 09:58:44 ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 18203.0 s16_alirezakho 7/27 09:51 0+00:00:00 I 0 0.7 pascal 18204.0 s16_alirezakho 7/27 09:51 0+00:00:00 I 0 0.7 pascal 18801.0 aapohl 7/28 16:58 0+00:00:00 I 0 0.0 run_ffmpeg.sh 18997.0 s16_martincum 7/29 10:59 0+00:00:32 I 0 733.0 runR.pl 1_0 run_perm.R 1 0 10 19027.5 s16_martincum 7/29 11:06 0+00:09:20 I 0 2198.0 runR.pl 1_5 run_perm.R 1 5 1000 The -nobatch output shows a line for every job and consists of 8 columns: Col Example Meaning ID 18801.0 Job ID, which is the cluster , a dot character ( . ), and the process OWNER aapohl The user ID of the user who submitted the job SUBMITTED 7/17 09:58 The date and time when the job was submitted RUN_TIME 0+00:00:00 Total time spent running so far (days + hours:minutes:seconds) ST I Status of job: I is Idle (waiting to run), R is Running, H is Held, etc. PRI 0 Job priority (see next lecture) SIZE 0.0 Current run-time memory usage, in MB CMD run_ffmpeg.sh The executable command (with arguments) to be run In future exercises, you'll want to switch between condor_q and condor_q -nobatch to see different types of information about YOUR jobs.","title":"Viewing Jobs without the Default \"batch\" Mode"},{"location":"materials/day1/part1-ex2-commands/#extra-information","text":"Both condor_status and condor_q have many command-line options, some of which significantly change their output. You will explore a few of the most useful options in the next lecture and set of exercises, but if you want to experiment now, go ahead! There are a few ways to learn more about the commands: Use the (brief) built-in help for the commands, e.g.: condor_q -h Read the installed man(ual) pages for the commands, e.g.: man condor_q Find the command in the online manual ; note: the text online is the same as the man text, only formatted for the web","title":"Extra Information"},{"location":"materials/day1/part1-ex3-jobs/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Monday Exercise 1.3: Run Jobs! \u00b6 The goal of this exercise is to submit jobs to HTCondor and have them run on the local pool (CHTC). This is a huge step in learning to use an HTC system! This exercise will take longer than the first two, short ones. It is the essential part of this exercise time. If you are having any problems getting the jobs to run, please ask the instructors! It is very important that you know how to run simple jobs. Running a Simple Command Using a Submit File \u00b6 Nearly all of the time, when you want to run an HTCondor job, you first write an HTCondor submit file for it. In this section, you will run the same hostname command as in the last exercise, but using a submit file. Here is a simple submit file for the hostname command: universe = vanilla executable = /bin/hostname output = simple.out error = simple.err log = simple.log request_cpus = 1 request_memory = 1MB request_disk = 1MB queue Write those lines of text in a file named simple.sub . Note: There is nothing magic about the name of an HTCondor submit file. It can be any filename you want. It's a good practice to always include the .sub extension, but it is not required. Ultimately, a submit file is a text file The lines of the submit file have the following meanings: universe The type of job this is. The default vanilla universe is a normal job. Later on, we will discuss other, special universes. executable The name of the program to run (relative to the directory from which you submit). output The filename where HTCondor will write the standard output from your job. error The filename where HTCondor will write the standard error from your job. This particular job is not likely to have any, but it is best to include this line for every job. log The filename where HTCondor will write information about your job run. Technically not required, it is a really good idea to have a log file for every job. request_* Tells HTCondor how many cpus and how much memory and disk we want, which is not much, because the 'hostname' executable is pretty simple queue Tells HTCondor to run your job with the settings above. Note that we are not using the arguments lines or transfer_input_files because the hostname program is all that needs to be transferred from the submit server, and we want to run it without any additional options. Double-check your submit file, so that it matches the text above. Then, tell HTCondor to run your job: %UCL_PROMPT_SHORT% <strong>condor_submit simple.sub</strong> Submitting job(s). 1 job(s) submitted to cluster <em>NNNN</em>. The actual cluster number will be shown instead of NNNN . If, instead of the text above, there are error messages, read them carefully and then try to correct your submit file. Notice that condor_submit returns back to the shell prompt right away. It does not wait for your job to run. Instead, as soon as it has finished submitting your job into the queue, the submit command finishes. Now, use condor_q and condor_q -nobatch to watch for your job in the queue. You probably may not even catch the job in the R running state, because the hostname command runs very quickly. When the job itself is finished, it will no longer be listed in the condor_q output. The output from your job is written to the filename given in the output line of your submit file. Thus, after the job finishes, you should be able to see the hostname output in simple.out , since this information is usually printed to the terminal by the hostname program, and not to a special file of it's own. %UCL_PROMPT_SHORT% <strong>cat simple.out</strong> e171.chtc.wisc.edu The simple.err file should be empty, unless there were issues running the hostname executable after it was transferred to the slot. The simple.log is more complex and will be the focus of a later exercise. Running a Job With Arguments \u00b6 Very often, when you run a command on the command line, it includes arguments after the command name itself: %UCL_PROMPT_SHORT% <strong>cat <em>simple.out</em></strong> %UCL_PROMPT_SHORT% <strong>sleep <em>60</em></strong> %UCL_PROMPT_SHORT% <strong>dc <em>-e '6 7 * p'</em></strong> In an HTCondor submit file, the command (or \"program\") name itself goes in the executable statement and all remaining arguments go into an arguments statement. For example, if the full command is: %UCL_PROMPT_SHORT% <strong>sleep <em>60</em></strong> Then in the submit file, we put: executable = /bin/sleep arguments = \"60\" Note: Put the entire list of arguments inside one pair of double-quotes. For the command-line command: %UCL_PROMPT_SHORT% <strong>dc <em>-e '6 7 * p'</em></strong> Then in the submit file, we put: executable = /usr/bin/dc arguments = \"-e '6 7 * p'\" Let\u2019s try a job submission with arguments. We will use the sleep command shown above, which simply does nothing for the specified number of seconds, then exits normally. It is convenient for simulating a job that takes a while to run. Create a new submit file (you name it this time) and save the following text in it. universe = vanilla executable = <em>/bin/sleep</em> <em>arguments = \"60\"</em> output = sleep.out error = sleep.err log = sleep.log request_cpus = 1 request_memory = 1MB request_disk = 1MB queue Except for changing a few filenames, this submit file is nearly identical to the last one. But, see the extra arguments line? Submit this new job. Again, watch for it to run using condor_q and condor_q -nobatch=; check once every 15 seconds or so. Once the job starts running, it will take about 1 minute to run (because of the =sleep command, right?), so you should be able to see it running for a bit. When the job finishes, it will disappear from the queue, but there will be no output in the output or error files, because sleep does not produce any output. Running a Script Job From the Submit Directory \u00b6 So far, we have been running programs (executables) that come with the standard Linux system. But you are not limited to standard programs. In this example, you will write a simple shell script (command line) executable in the submit directory, then write a submit file to run it. Put the following contents into a file named test-script.sh :\\ #/bin/sh echo 'Date: ' `date` echo 'Host: ' `hostname` echo 'System: ' `uname -spo` echo \"Program: $0\" echo \"Args: $*\" Make the file itself executable:\\ chmod +x test-script.sh Test your script from the command line:\\ ./test-script.sh hello 42 Date: Mon Jul 17 10:02:20 CDT 2017 Host: learn.chtc.wisc.edu System: Linux x86_64 GNU/Linux Program: ./test-script.sh Args: hello 42 \\ This step is really important! If you cannot run your executable from the command-line, HTCondor probably cannot run it on another machine, either. And debugging simple problems like this one is surprisingly difficult. So, if possible, test your executable and arguments as a command at the command-line first. Write the submit file (this should be getting easier by now):\\ universe = vanilla executable = test-script.sh arguments = \"foo bar baz\" output = script.out error = script.err log = script.log request_cpus = 1 request_memory = 1 request_disk = 1 queue \\ Note: As this example shows, blank lines and spaces around the = sign do not matter to HTCondor. Use whitespace to make things clear to you . What format do you prefer to read? Submit the job, wait for it to finish, check the output. (Are you surprised by the Program: line in the output? Why is it like that? Google for it, or ask an instructor if you are curious, although the answer is not that exciting.) In this example, the executable that was named in the submit file did not start with a / , so the location of the file is relative to the submit directory itself. In other words, in this format the executable must be in the same directory as the submit file. Extra Challenge 1 \u00b6 Below is a simple Python script that does something similar to the shell script above. Run this Python script using HTCondor. #!/usr/bin/env python \"\"\"Extra Challenge for OSG User School 2015 Monday, 27 July 2015 Written by Tim Cartwright Submitted to CHTC by #YOUR_NAME# \"\"\" import getpass import os import platform import socket import sys import time arguments = None if len(sys.argv) > 1: arguments = '\"' + ' '.join(sys.argv[1:]) + '\"' print >> sys.stderr, __doc__ print 'Time :', time.strftime('%Y-%m-%d (%a) %H:%M:%S %Z') print 'Host :', getpass.getuser(), '@', socket.gethostname() uname = platform.uname() print \"System :\", uname[0], uname[2], uname[4] print \"Version :\", platform.python_version() print \"Program :\", sys.executable print 'Script :', os.path.abspath(__file__) print 'Args :', arguments Note: For the Python script, above, you'll want to increase the memory request. We will talk about tuning resource requests, later, but you should be able to set the following to have the job run: request_memory = 64 MB Extra Challenge 1 \u00b6 Do you have any software of your own that Is written in a common scripting language (e.g., shell, Perl, Python, Ruby) or is it a compiled binary? Does not require other files (e.g., input files, libraries) Runs on common Linux systems (our machines are mostly compatible with Red Hat Enterprise Linux 6)? If so, try to run your program with HTCondor now, as you'll only need the same/similar submit file lines that we've covered so far.","title":"1.3. Run jobs"},{"location":"materials/day1/part1-ex3-jobs/#monday-exercise-13-run-jobs","text":"The goal of this exercise is to submit jobs to HTCondor and have them run on the local pool (CHTC). This is a huge step in learning to use an HTC system! This exercise will take longer than the first two, short ones. It is the essential part of this exercise time. If you are having any problems getting the jobs to run, please ask the instructors! It is very important that you know how to run simple jobs.","title":"Monday Exercise 1.3: Run Jobs!"},{"location":"materials/day1/part1-ex3-jobs/#running-a-simple-command-using-a-submit-file","text":"Nearly all of the time, when you want to run an HTCondor job, you first write an HTCondor submit file for it. In this section, you will run the same hostname command as in the last exercise, but using a submit file. Here is a simple submit file for the hostname command: universe = vanilla executable = /bin/hostname output = simple.out error = simple.err log = simple.log request_cpus = 1 request_memory = 1MB request_disk = 1MB queue Write those lines of text in a file named simple.sub . Note: There is nothing magic about the name of an HTCondor submit file. It can be any filename you want. It's a good practice to always include the .sub extension, but it is not required. Ultimately, a submit file is a text file The lines of the submit file have the following meanings: universe The type of job this is. The default vanilla universe is a normal job. Later on, we will discuss other, special universes. executable The name of the program to run (relative to the directory from which you submit). output The filename where HTCondor will write the standard output from your job. error The filename where HTCondor will write the standard error from your job. This particular job is not likely to have any, but it is best to include this line for every job. log The filename where HTCondor will write information about your job run. Technically not required, it is a really good idea to have a log file for every job. request_* Tells HTCondor how many cpus and how much memory and disk we want, which is not much, because the 'hostname' executable is pretty simple queue Tells HTCondor to run your job with the settings above. Note that we are not using the arguments lines or transfer_input_files because the hostname program is all that needs to be transferred from the submit server, and we want to run it without any additional options. Double-check your submit file, so that it matches the text above. Then, tell HTCondor to run your job: %UCL_PROMPT_SHORT% <strong>condor_submit simple.sub</strong> Submitting job(s). 1 job(s) submitted to cluster <em>NNNN</em>. The actual cluster number will be shown instead of NNNN . If, instead of the text above, there are error messages, read them carefully and then try to correct your submit file. Notice that condor_submit returns back to the shell prompt right away. It does not wait for your job to run. Instead, as soon as it has finished submitting your job into the queue, the submit command finishes. Now, use condor_q and condor_q -nobatch to watch for your job in the queue. You probably may not even catch the job in the R running state, because the hostname command runs very quickly. When the job itself is finished, it will no longer be listed in the condor_q output. The output from your job is written to the filename given in the output line of your submit file. Thus, after the job finishes, you should be able to see the hostname output in simple.out , since this information is usually printed to the terminal by the hostname program, and not to a special file of it's own. %UCL_PROMPT_SHORT% <strong>cat simple.out</strong> e171.chtc.wisc.edu The simple.err file should be empty, unless there were issues running the hostname executable after it was transferred to the slot. The simple.log is more complex and will be the focus of a later exercise.","title":"Running a Simple Command Using a Submit File"},{"location":"materials/day1/part1-ex3-jobs/#running-a-job-with-arguments","text":"Very often, when you run a command on the command line, it includes arguments after the command name itself: %UCL_PROMPT_SHORT% <strong>cat <em>simple.out</em></strong> %UCL_PROMPT_SHORT% <strong>sleep <em>60</em></strong> %UCL_PROMPT_SHORT% <strong>dc <em>-e '6 7 * p'</em></strong> In an HTCondor submit file, the command (or \"program\") name itself goes in the executable statement and all remaining arguments go into an arguments statement. For example, if the full command is: %UCL_PROMPT_SHORT% <strong>sleep <em>60</em></strong> Then in the submit file, we put: executable = /bin/sleep arguments = \"60\" Note: Put the entire list of arguments inside one pair of double-quotes. For the command-line command: %UCL_PROMPT_SHORT% <strong>dc <em>-e '6 7 * p'</em></strong> Then in the submit file, we put: executable = /usr/bin/dc arguments = \"-e '6 7 * p'\" Let\u2019s try a job submission with arguments. We will use the sleep command shown above, which simply does nothing for the specified number of seconds, then exits normally. It is convenient for simulating a job that takes a while to run. Create a new submit file (you name it this time) and save the following text in it. universe = vanilla executable = <em>/bin/sleep</em> <em>arguments = \"60\"</em> output = sleep.out error = sleep.err log = sleep.log request_cpus = 1 request_memory = 1MB request_disk = 1MB queue Except for changing a few filenames, this submit file is nearly identical to the last one. But, see the extra arguments line? Submit this new job. Again, watch for it to run using condor_q and condor_q -nobatch=; check once every 15 seconds or so. Once the job starts running, it will take about 1 minute to run (because of the =sleep command, right?), so you should be able to see it running for a bit. When the job finishes, it will disappear from the queue, but there will be no output in the output or error files, because sleep does not produce any output.","title":"Running a Job With Arguments"},{"location":"materials/day1/part1-ex3-jobs/#running-a-script-job-from-the-submit-directory","text":"So far, we have been running programs (executables) that come with the standard Linux system. But you are not limited to standard programs. In this example, you will write a simple shell script (command line) executable in the submit directory, then write a submit file to run it. Put the following contents into a file named test-script.sh :\\ #/bin/sh echo 'Date: ' `date` echo 'Host: ' `hostname` echo 'System: ' `uname -spo` echo \"Program: $0\" echo \"Args: $*\" Make the file itself executable:\\ chmod +x test-script.sh Test your script from the command line:\\ ./test-script.sh hello 42 Date: Mon Jul 17 10:02:20 CDT 2017 Host: learn.chtc.wisc.edu System: Linux x86_64 GNU/Linux Program: ./test-script.sh Args: hello 42 \\ This step is really important! If you cannot run your executable from the command-line, HTCondor probably cannot run it on another machine, either. And debugging simple problems like this one is surprisingly difficult. So, if possible, test your executable and arguments as a command at the command-line first. Write the submit file (this should be getting easier by now):\\ universe = vanilla executable = test-script.sh arguments = \"foo bar baz\" output = script.out error = script.err log = script.log request_cpus = 1 request_memory = 1 request_disk = 1 queue \\ Note: As this example shows, blank lines and spaces around the = sign do not matter to HTCondor. Use whitespace to make things clear to you . What format do you prefer to read? Submit the job, wait for it to finish, check the output. (Are you surprised by the Program: line in the output? Why is it like that? Google for it, or ask an instructor if you are curious, although the answer is not that exciting.) In this example, the executable that was named in the submit file did not start with a / , so the location of the file is relative to the submit directory itself. In other words, in this format the executable must be in the same directory as the submit file.","title":"Running a Script Job From the Submit Directory"},{"location":"materials/day1/part1-ex3-jobs/#extra-challenge-1","text":"Below is a simple Python script that does something similar to the shell script above. Run this Python script using HTCondor. #!/usr/bin/env python \"\"\"Extra Challenge for OSG User School 2015 Monday, 27 July 2015 Written by Tim Cartwright Submitted to CHTC by #YOUR_NAME# \"\"\" import getpass import os import platform import socket import sys import time arguments = None if len(sys.argv) > 1: arguments = '\"' + ' '.join(sys.argv[1:]) + '\"' print >> sys.stderr, __doc__ print 'Time :', time.strftime('%Y-%m-%d (%a) %H:%M:%S %Z') print 'Host :', getpass.getuser(), '@', socket.gethostname() uname = platform.uname() print \"System :\", uname[0], uname[2], uname[4] print \"Version :\", platform.python_version() print \"Program :\", sys.executable print 'Script :', os.path.abspath(__file__) print 'Args :', arguments Note: For the Python script, above, you'll want to increase the memory request. We will talk about tuning resource requests, later, but you should be able to set the following to have the job run: request_memory = 64 MB","title":"Extra Challenge 1"},{"location":"materials/day1/part1-ex3-jobs/#extra-challenge-1_1","text":"Do you have any software of your own that Is written in a common scripting language (e.g., shell, Perl, Python, Ruby) or is it a compiled binary? Does not require other files (e.g., input files, libraries) Runs on common Linux systems (our machines are mostly compatible with Red Hat Enterprise Linux 6)? If so, try to run your program with HTCondor now, as you'll only need the same/similar submit file lines that we've covered so far.","title":"Extra Challenge 1"},{"location":"materials/day1/part1-ex4-logs/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Monday Exercise 1.4: Read and Interpret Log Files \u00b6 The goal of this exercise is quite simple: Learn to understand the contents of a job log file. When things go wrong with your job, it is usually the first place you should look for important messages. Plus, there is other useful information there. This exercise is short. If you do not have time for it now, come back and visit it later. Reading a Log File \u00b6 For this exercise, we can reuse any previous job that you have run. The example output below is based on the sleep 60 job. A job log file is updated throughout the life of a job, usually at key events. Each event starts with a heading that indicates what happened and when. Here are all of the event headings from the sleep job log (detailed output in between headings has been omitted here): 000 (5739.000.000) 07/25 10:44:20 Job submitted from host: <128.104.100.43:9618?addrs=...> 001 (5739.000.000) 07/25 10:45:11 Job executing on host: <128.104.55.42:9618?addrs=...> 006 (5739.000.000) 07/25 10:45:20 Image size of job updated: 72 006 (5739.000.000) 07/25 10:46:11 Image size of job updated: 4072 005 (5739.000.000) 07/25 10:46:11 Job terminated. There is a lot of extra information in those lines, but you can see: The job ID: cluster 5739, process 0 (written 000 ) The date and local time of each event A brief description of the event: submission, execution, some information updates, and termination Some events provide no information in addition to the heading. For example: 000 (5739.000.000) 07/25 10:44:20 Job submitted from host: <128.104.100.43:9618?addrs=...> ... and 001 (5739.000.000) 07/25 10:45:11 Job executing on host: <128.104.55.42:9618?addrs=...> ... Note: Each event ends with a line that contains only 3 dots: ... But the periodic information update event contains some additional information: 006 (5739.000.000) 07/25 10:45:20 Image size of job updated: 72 1 - MemoryUsage of job (MB) 72 - ResidentSetSize of job (KB) ... These updates record the amount of memory that the job is using on the execute machine. This can be helpful information, so that in future runs of the job, you can tell HTCondor how much memory you will need. More on that topic in the next lecture. The job termination event includes a great deal of additional information: 005 (5739.000.000) 07/25 10:46:11 Job terminated. (1) Normal termination (return value 0) Usr 0 00:00:00, Sys 0 00:00:00 - Run Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Run Local Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Local Usage 0 - Run Bytes Sent By Job 27848 - Run Bytes Received By Job 0 - Total Bytes Sent By Job 27848 - Total Bytes Received By Job Partitionable Resources : Usage Request Allocated Cpus : 1 1 Disk (KB) : 40 30 4203309 Memory (MB) : 1 1 1 ... Probably the most interesting information is: The return value ( 0 here, which is success; non-zero usually means failure) The total number of bytes transferred each way, which could be useful if your network is slow The Partitionable Resources table, especially disk and memory usage \u2014 again, more on that in the next lecture There are many other kinds of events, but the ones above will occur in almost every job log. Understanding When Job Log Events Are Written \u00b6 When are events written to the job log file? Let\u2019s find out. Read through the entire procedure below before starting, because some parts of the process are time sensitive. Change the sleep job submit file, so that the job sleeps for 2 minutes (= 120 seconds) Submit the updated sleep job As soon as the condor_submit command finishes, hit the return key a few times, to create some blank lines Right away, run a command to show the log file and keep showing updates as they occur:\\ tail -f sleep.log \\ Be sure to use the correct filename for your log file, as named in your submit file. Watch the output carefully. When do events appear in the log file? After the termination event appears, press Control-C to end the tail command and return to the shell prompt. Understanding How HTCondor Writes Files \u00b6 When HTCondor writes the output, error, and log files, does it erase the previous contents of the file or does it add new lines onto the end? Let\u2019s find out! For this exercise, we can use the hostname job from earlier. Edit the hostname submit file so that it uses new and unique filenames for output, error, and log files\\ Alternatively, delete any existing output, error, and log files from previous runs of the hostname job. Submit the job three separate times in a row (there are better ways to do this, which we will cover in the next lecture) Wait for all three jobs to finish Examine the output file: How many hostnames are there? Did HTCondor erase the previous contents for each job, or add new lines? Examine the log file\u2026 carefully: What happened there? Pay close attention to the times and job IDs of the events. If you have questions about how HTCondor handles these files, you could try finding relevant sections of the manual (this is hard and not as useful as one would hope), discuss it with neighbors or instructors, or ask questions at the end of this session.","title":"1.4. Log files"},{"location":"materials/day1/part1-ex4-logs/#monday-exercise-14-read-and-interpret-log-files","text":"The goal of this exercise is quite simple: Learn to understand the contents of a job log file. When things go wrong with your job, it is usually the first place you should look for important messages. Plus, there is other useful information there. This exercise is short. If you do not have time for it now, come back and visit it later.","title":"Monday Exercise 1.4: Read and Interpret Log Files"},{"location":"materials/day1/part1-ex4-logs/#reading-a-log-file","text":"For this exercise, we can reuse any previous job that you have run. The example output below is based on the sleep 60 job. A job log file is updated throughout the life of a job, usually at key events. Each event starts with a heading that indicates what happened and when. Here are all of the event headings from the sleep job log (detailed output in between headings has been omitted here): 000 (5739.000.000) 07/25 10:44:20 Job submitted from host: <128.104.100.43:9618?addrs=...> 001 (5739.000.000) 07/25 10:45:11 Job executing on host: <128.104.55.42:9618?addrs=...> 006 (5739.000.000) 07/25 10:45:20 Image size of job updated: 72 006 (5739.000.000) 07/25 10:46:11 Image size of job updated: 4072 005 (5739.000.000) 07/25 10:46:11 Job terminated. There is a lot of extra information in those lines, but you can see: The job ID: cluster 5739, process 0 (written 000 ) The date and local time of each event A brief description of the event: submission, execution, some information updates, and termination Some events provide no information in addition to the heading. For example: 000 (5739.000.000) 07/25 10:44:20 Job submitted from host: <128.104.100.43:9618?addrs=...> ... and 001 (5739.000.000) 07/25 10:45:11 Job executing on host: <128.104.55.42:9618?addrs=...> ... Note: Each event ends with a line that contains only 3 dots: ... But the periodic information update event contains some additional information: 006 (5739.000.000) 07/25 10:45:20 Image size of job updated: 72 1 - MemoryUsage of job (MB) 72 - ResidentSetSize of job (KB) ... These updates record the amount of memory that the job is using on the execute machine. This can be helpful information, so that in future runs of the job, you can tell HTCondor how much memory you will need. More on that topic in the next lecture. The job termination event includes a great deal of additional information: 005 (5739.000.000) 07/25 10:46:11 Job terminated. (1) Normal termination (return value 0) Usr 0 00:00:00, Sys 0 00:00:00 - Run Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Run Local Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Local Usage 0 - Run Bytes Sent By Job 27848 - Run Bytes Received By Job 0 - Total Bytes Sent By Job 27848 - Total Bytes Received By Job Partitionable Resources : Usage Request Allocated Cpus : 1 1 Disk (KB) : 40 30 4203309 Memory (MB) : 1 1 1 ... Probably the most interesting information is: The return value ( 0 here, which is success; non-zero usually means failure) The total number of bytes transferred each way, which could be useful if your network is slow The Partitionable Resources table, especially disk and memory usage \u2014 again, more on that in the next lecture There are many other kinds of events, but the ones above will occur in almost every job log.","title":"Reading a Log File"},{"location":"materials/day1/part1-ex4-logs/#understanding-when-job-log-events-are-written","text":"When are events written to the job log file? Let\u2019s find out. Read through the entire procedure below before starting, because some parts of the process are time sensitive. Change the sleep job submit file, so that the job sleeps for 2 minutes (= 120 seconds) Submit the updated sleep job As soon as the condor_submit command finishes, hit the return key a few times, to create some blank lines Right away, run a command to show the log file and keep showing updates as they occur:\\ tail -f sleep.log \\ Be sure to use the correct filename for your log file, as named in your submit file. Watch the output carefully. When do events appear in the log file? After the termination event appears, press Control-C to end the tail command and return to the shell prompt.","title":"Understanding When Job Log Events Are Written"},{"location":"materials/day1/part1-ex4-logs/#understanding-how-htcondor-writes-files","text":"When HTCondor writes the output, error, and log files, does it erase the previous contents of the file or does it add new lines onto the end? Let\u2019s find out! For this exercise, we can use the hostname job from earlier. Edit the hostname submit file so that it uses new and unique filenames for output, error, and log files\\ Alternatively, delete any existing output, error, and log files from previous runs of the hostname job. Submit the job three separate times in a row (there are better ways to do this, which we will cover in the next lecture) Wait for all three jobs to finish Examine the output file: How many hostnames are there? Did HTCondor erase the previous contents for each job, or add new lines? Examine the log file\u2026 carefully: What happened there? Pay close attention to the times and job IDs of the events. If you have questions about how HTCondor handles these files, you could try finding relevant sections of the manual (this is hard and not as useful as one would hope), discuss it with neighbors or instructors, or ask questions at the end of this session.","title":"Understanding How HTCondor Writes Files"},{"location":"materials/day1/part1-ex5-request/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Monday Exercise 1.5: Declare Resource Needs \u00b6 The goal of this exercise is to demonstrate how to test and tune the request_<i>X</i> statements in a submit file for when you don't know what resources your job needs. There are three special resource request statements that you can use (optionally) in an HTCondor submit file: * request_cpus for the number of CPUs your job will use (most softwares will take an argument to control this number, and it's usually otherwise \"1\") * request_memory for the maximum amount of run-time memory your job may use * request_disk for the maximum amount of disk space your job may use (including the executable and all other data that may show up during the job) HTCondor defaults to certain reasonable values for these request settings, so you do not need to use them to get small jobs to run. However, on some HTCondor pools, if your job goes over the request values, it may be removed from the execute machine and either held (awaiting action on your part) or rerun later. So it can be a disadvantage to you if you do not declare your resource needs or if you underestimate them. If you overestimate them, your jobs will match to fewer slots (and with a longer average wait time) and you'll be hogging up resources that you don't need, but that could be used for the jobs of other users. In the long run, it works better for all users of the pool if you declare what you really need. But how do you know what to request? In particular, we are concerned with memory and disk here; requesting multiple CPUs and using them is covered a bit in later school materials, but true HTC splits work up into jobs that each use as few CPU cores as possible (one CPU core is always best to have the most jobs running soonest). Determining Resource Needs Before Running Any Jobs \u00b6 It can be very difficult to determine the memory needs of your running program. Typically, the memory size of a job changes over time, making the task even trickier. If you have knowledge ahead of time about your job\u2019s maximum memory needs, use that, or a maybe a number that's just a bit higher, to be safe. If not, then it's best to run your program in a single test job, first, and let HTCondor tell you in the log file (or in the condor_q -nobatch output, if you're able to watch it), which is covered in the next section on \"Determining Resource Needs by Running Test Jobs\". Running a Job Locally On our shared submit server learn.chtc.wisc.edu , you should not run computationally-intensive work because it can use resources needed by HTCondor to manage the queue for all uses. However, you may have access to other computers where you can observe the memory usage of a program. The downside is that you'll have to watch a program run for essentially the entire time, to make sure you catch the maximum memory usage. \u00b6 For Memory: On Mac and Windows, for example, the \"Activity Monitor\" and \"Task Manager\" applications may be useful. On a Mac or Linux system, you can use the ps command or the top command in the Terminal to watch a running program and see (roughly) how much memory it is using. Full coverage of these tools is beyond the scope of this exercise, but here are two quick examples: Using ps : %UCL_PROMPT_SHORT% <strong>ps ux</strong> USER PID %CPU %MEM VSZ <em>RSS</em> TTY STAT START TIME COMMAND cat 24342 0.0 0.0 90224 <em>1864</em> ? S 13:39 0:00 sshd: cat@pts/0 cat 24343 0.0 0.0 66096 <em>1580</em> pts/0 Ss 13:39 0:00 -bash cat 25864 0.0 0.0 65624 <em>996</em> pts/0 R+ 13:52 0:00 ps ux cat 30052 0.0 0.0 90720 <em>2456</em> ? S Jun22 0:00 sshd: cat@pts/2 cat 30053 0.0 0.0 66096 <em>1624</em> pts/2 Ss+ Jun22 0:00 -bash The Resident Set Size ( RSS ) column, highlighted above, gives a rough indication of the memory usage (in KB) of each running process. If your program runs long enough, you can run this command several times and note the greatest value. Using top : %UCL_PROMPT_SHORT% <strong>top -u <em>userid</em></strong> top - 13:55:31 up 11 days, 20:59, 5 users, load average: 0.12, 0.12, 0.09 Tasks: 198 total, 1 running, 197 sleeping, 0 stopped, 0 zombie Cpu(s): 1.2%us, 0.1%sy, 0.0%ni, 98.5%id, 0.2%wa, 0.0%hi, 0.1%si, 0.0%st Mem: 4001440k total, 3558028k used, 443412k free, 258568k buffers Swap: 4194296k total, 148k used, 4194148k free, 2960760k cached PID USER PR NI VIRT <em>RES</em> SHR S %CPU %MEM TIME+ COMMAND 24342 cat 15 0 90224 <em>1864</em> 1096 S 0.0 0.0 0:00.26 sshd 24343 cat 15 0 66096 <em>1580</em> 1232 S 0.0 0.0 0:00.07 bash 25927 cat 15 0 12760 <em>1196</em> 836 R 0.0 0.0 0:00.01 top 30052 cat 16 0 90720 <em>2456</em> 1112 S 0.0 0.1 0:00.69 sshd 30053 cat 18 0 66096 <em>1624</em> 1236 S 0.0 0.0 0:00.37 bash The top command (shown here with an option to limit the output to a single user ID) also shows information about running processes, but updates periodically by itself. Type the letter q to quit the interactive display. Again, the highlighted RES column shows an approximation of memory usage. For Disk: Determining disk needs may be a bit simpler, because you can check on the size of files that a program is using while it runs. However, it is important to count all files that HTCondor counts to get an accurate size. HTCondor counts everything in your job sandbox toward your job\u2019s disk usage: The executable itself All \"input\" files (anything else that gets transferred TO the job, even if you don't think of it as \"input\") All files created during the job (broadly defined as \"output\"), including the captured standard output and error files that you list in the submit file. All temporary files created in the sandbox, even if they get deleted by the executable before it's done. If you can run your program within a single directory on a local computer (not on the submit server), you should be able to view files and their sizes with the ls command. Determining Resource Needs By Running Test Jobs (BEST) \u00b6 Despite the techniques mentioned above, by far the easiest approach to measuring your job\u2019s resource needs is to run one or a small number of sample jobs and have HTCondor itself tell you about the resources used during the runs. For example, here is a strange Python script that does not do anything useful, but consumes some real resources while running: #!/usr/bin/env python import time import os size = 1000000 numbers = [] for i in xrange(size): numbers.append(str(i)) tempfile = open('temp', 'w') tempfile.write(' '.join(numbers)) tempfile.close() time.sleep(60) os.remove('temp') Without trying to figure out what this code does or how many resources it uses, just create a submit file for it, and run it once with HTCondor, starting with somewhat high memory requests (\"1GB\" for memory and disk is a good starting point, unless you think the job will use far more, and will still match quickly). When it is done, examine the log file. In particular, we care about these lines: Partitionable Resources : <em>Usage</em> Request Allocated Cpus : 1 1 Disk (<em>KB</em>) : <em>6739</em> 1048576 8022934 Memory (<em>MB</em>) : <em>3</em> 1024 1024 So, now we know that the job used 6,739 KB of disk (= about 6.5 MB) and 3 MB of memory! This is a great technique for determining the real resource needs of your job. If you think resource needs vary from run to run, submit a few sample jobs and look at all the results. And it never hurts to round up your resource requests a little, just in case your job occasionally uses more resources. Setting Resource Requirements \u00b6 Once you know your job\u2019s resource requirements, it is easy to declare them in your submit file. For example, taking our results above as an example, we might slightly increase our requests above what was used, just to be safe: request_memory = 4MB <span style=\"font-style: italic; color: blue;\"># rounded up from 3 MB</span> request_disk = 7MB <span style=\"font-style: italic; color: blue;\"># rounded up from 6.5 MB</span> Pay close attention to units: Without explicit units, request_memory is in MB (megabytes) Without explicit units, request_disk is in KB (kilobytes) Allowable units are KB (kilobytes), MB (megabytes), GB (gigabytes), and TB (terabytes) HTCondor translates these requirements into expressions that become part of the requirements expression. However, do not put your CPU, memory, and disk requirements directly into the requirements expression; use the request_<i>XXX</i> statements instead. Add these requirements to your submit file for the Python script, rerun the job, and confirm in the log file that your requests were used.","title":"1.5. Resource needs"},{"location":"materials/day1/part1-ex5-request/#monday-exercise-15-declare-resource-needs","text":"The goal of this exercise is to demonstrate how to test and tune the request_<i>X</i> statements in a submit file for when you don't know what resources your job needs. There are three special resource request statements that you can use (optionally) in an HTCondor submit file: * request_cpus for the number of CPUs your job will use (most softwares will take an argument to control this number, and it's usually otherwise \"1\") * request_memory for the maximum amount of run-time memory your job may use * request_disk for the maximum amount of disk space your job may use (including the executable and all other data that may show up during the job) HTCondor defaults to certain reasonable values for these request settings, so you do not need to use them to get small jobs to run. However, on some HTCondor pools, if your job goes over the request values, it may be removed from the execute machine and either held (awaiting action on your part) or rerun later. So it can be a disadvantage to you if you do not declare your resource needs or if you underestimate them. If you overestimate them, your jobs will match to fewer slots (and with a longer average wait time) and you'll be hogging up resources that you don't need, but that could be used for the jobs of other users. In the long run, it works better for all users of the pool if you declare what you really need. But how do you know what to request? In particular, we are concerned with memory and disk here; requesting multiple CPUs and using them is covered a bit in later school materials, but true HTC splits work up into jobs that each use as few CPU cores as possible (one CPU core is always best to have the most jobs running soonest).","title":"Monday Exercise 1.5: Declare Resource Needs"},{"location":"materials/day1/part1-ex5-request/#determining-resource-needs-before-running-any-jobs","text":"It can be very difficult to determine the memory needs of your running program. Typically, the memory size of a job changes over time, making the task even trickier. If you have knowledge ahead of time about your job\u2019s maximum memory needs, use that, or a maybe a number that's just a bit higher, to be safe. If not, then it's best to run your program in a single test job, first, and let HTCondor tell you in the log file (or in the condor_q -nobatch output, if you're able to watch it), which is covered in the next section on \"Determining Resource Needs by Running Test Jobs\".","title":"Determining Resource Needs Before Running Any Jobs"},{"location":"materials/day1/part1-ex5-request/#running-a-job-locally-on-our-shared-submit-server-learnchtcwiscedu-you-should-not-run-computationally-intensive-work-because-it-can-use-resources-needed-by-htcondor-to-manage-the-queue-for-all-uses-however-you-may-have-access-to-other-computers-where-you-can-observe-the-memory-usage-of-a-program-the-downside-is-that-youll-have-to-watch-a-program-run-for-essentially-the-entire-time-to-make-sure-you-catch-the-maximum-memory-usage","text":"For Memory: On Mac and Windows, for example, the \"Activity Monitor\" and \"Task Manager\" applications may be useful. On a Mac or Linux system, you can use the ps command or the top command in the Terminal to watch a running program and see (roughly) how much memory it is using. Full coverage of these tools is beyond the scope of this exercise, but here are two quick examples: Using ps : %UCL_PROMPT_SHORT% <strong>ps ux</strong> USER PID %CPU %MEM VSZ <em>RSS</em> TTY STAT START TIME COMMAND cat 24342 0.0 0.0 90224 <em>1864</em> ? S 13:39 0:00 sshd: cat@pts/0 cat 24343 0.0 0.0 66096 <em>1580</em> pts/0 Ss 13:39 0:00 -bash cat 25864 0.0 0.0 65624 <em>996</em> pts/0 R+ 13:52 0:00 ps ux cat 30052 0.0 0.0 90720 <em>2456</em> ? S Jun22 0:00 sshd: cat@pts/2 cat 30053 0.0 0.0 66096 <em>1624</em> pts/2 Ss+ Jun22 0:00 -bash The Resident Set Size ( RSS ) column, highlighted above, gives a rough indication of the memory usage (in KB) of each running process. If your program runs long enough, you can run this command several times and note the greatest value. Using top : %UCL_PROMPT_SHORT% <strong>top -u <em>userid</em></strong> top - 13:55:31 up 11 days, 20:59, 5 users, load average: 0.12, 0.12, 0.09 Tasks: 198 total, 1 running, 197 sleeping, 0 stopped, 0 zombie Cpu(s): 1.2%us, 0.1%sy, 0.0%ni, 98.5%id, 0.2%wa, 0.0%hi, 0.1%si, 0.0%st Mem: 4001440k total, 3558028k used, 443412k free, 258568k buffers Swap: 4194296k total, 148k used, 4194148k free, 2960760k cached PID USER PR NI VIRT <em>RES</em> SHR S %CPU %MEM TIME+ COMMAND 24342 cat 15 0 90224 <em>1864</em> 1096 S 0.0 0.0 0:00.26 sshd 24343 cat 15 0 66096 <em>1580</em> 1232 S 0.0 0.0 0:00.07 bash 25927 cat 15 0 12760 <em>1196</em> 836 R 0.0 0.0 0:00.01 top 30052 cat 16 0 90720 <em>2456</em> 1112 S 0.0 0.1 0:00.69 sshd 30053 cat 18 0 66096 <em>1624</em> 1236 S 0.0 0.0 0:00.37 bash The top command (shown here with an option to limit the output to a single user ID) also shows information about running processes, but updates periodically by itself. Type the letter q to quit the interactive display. Again, the highlighted RES column shows an approximation of memory usage. For Disk: Determining disk needs may be a bit simpler, because you can check on the size of files that a program is using while it runs. However, it is important to count all files that HTCondor counts to get an accurate size. HTCondor counts everything in your job sandbox toward your job\u2019s disk usage: The executable itself All \"input\" files (anything else that gets transferred TO the job, even if you don't think of it as \"input\") All files created during the job (broadly defined as \"output\"), including the captured standard output and error files that you list in the submit file. All temporary files created in the sandbox, even if they get deleted by the executable before it's done. If you can run your program within a single directory on a local computer (not on the submit server), you should be able to view files and their sizes with the ls command.","title":"Running a Job Locally On our shared submit server learn.chtc.wisc.edu, you should not run computationally-intensive work because it can use resources needed by HTCondor to manage the queue for all uses. However, you may have access to other computers where you can observe the memory usage of a program. The downside is that you'll have to watch a program run for essentially the entire time, to make sure you catch the maximum memory usage."},{"location":"materials/day1/part1-ex5-request/#determining-resource-needs-by-running-test-jobs-best","text":"Despite the techniques mentioned above, by far the easiest approach to measuring your job\u2019s resource needs is to run one or a small number of sample jobs and have HTCondor itself tell you about the resources used during the runs. For example, here is a strange Python script that does not do anything useful, but consumes some real resources while running: #!/usr/bin/env python import time import os size = 1000000 numbers = [] for i in xrange(size): numbers.append(str(i)) tempfile = open('temp', 'w') tempfile.write(' '.join(numbers)) tempfile.close() time.sleep(60) os.remove('temp') Without trying to figure out what this code does or how many resources it uses, just create a submit file for it, and run it once with HTCondor, starting with somewhat high memory requests (\"1GB\" for memory and disk is a good starting point, unless you think the job will use far more, and will still match quickly). When it is done, examine the log file. In particular, we care about these lines: Partitionable Resources : <em>Usage</em> Request Allocated Cpus : 1 1 Disk (<em>KB</em>) : <em>6739</em> 1048576 8022934 Memory (<em>MB</em>) : <em>3</em> 1024 1024 So, now we know that the job used 6,739 KB of disk (= about 6.5 MB) and 3 MB of memory! This is a great technique for determining the real resource needs of your job. If you think resource needs vary from run to run, submit a few sample jobs and look at all the results. And it never hurts to round up your resource requests a little, just in case your job occasionally uses more resources.","title":"Determining Resource Needs By Running Test Jobs (BEST)"},{"location":"materials/day1/part1-ex5-request/#setting-resource-requirements","text":"Once you know your job\u2019s resource requirements, it is easy to declare them in your submit file. For example, taking our results above as an example, we might slightly increase our requests above what was used, just to be safe: request_memory = 4MB <span style=\"font-style: italic; color: blue;\"># rounded up from 3 MB</span> request_disk = 7MB <span style=\"font-style: italic; color: blue;\"># rounded up from 6.5 MB</span> Pay close attention to units: Without explicit units, request_memory is in MB (megabytes) Without explicit units, request_disk is in KB (kilobytes) Allowable units are KB (kilobytes), MB (megabytes), GB (gigabytes), and TB (terabytes) HTCondor translates these requirements into expressions that become part of the requirements expression. However, do not put your CPU, memory, and disk requirements directly into the requirements expression; use the request_<i>XXX</i> statements instead. Add these requirements to your submit file for the Python script, rerun the job, and confirm in the log file that your requests were used.","title":"Setting Resource Requirements"},{"location":"materials/day1/part1-ex6-remove/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Monday Exercise 1.6: Remove Jobs From the Queue \u00b6 The goal of this exercise is to show you how to remove jobs from the queue. This is helpful if you make a mistake, do not want to wait for a job to complete, or otherwise need to fix things. For example, if some test jobs go on hold for using too much memory or disk, you may want to just remove them, edit the submit files, and then submit again. NOTE: Please remember to remove any jobs from the queue that you have given up on. Otherwise, the queue will start to get very long with jobs that will waste resources (and decrease your priority), or that may never run (if they're on hold, or have other issues keeping them from matching). This exercise is very short, but if you are out of time, you can come back to it later. Removing a Job From the Queue \u00b6 To practice removing jobs from the queue, you need a job in the queue! Submit a job from an earlier exercise Determine the job ID ( cluster.process ) from the condor_submit output or from condor_q Remove the job:\\ condor_rm job_id \\ Use the full job ID this time, e.g., 5759.0 . Did the job leave the queue immediately? If not, about how long did it take? So far, we have created job clusters that contain only one job process (the .0 part of the job ID). That will change soon, so it is good to know how to remove a specific job ID. However, it is possible to remove all jobs that are part of a cluster at once. Simply omit the job process (the .0 part of the job ID) in the condor_rm command: %UCL_PROMPT_SHORT% <strong>condor_rm <em>cluster</em></strong> Finally, you can include many job clusters and full job IDs in a single condor_rm command. For example: %UCL_PROMPT_SHORT% <strong>condor_rm 5768 5769 5770.0 5771.2</strong> Removing All of Your Jobs \u00b6 If you really want to remove all of your jobs at once, you can do that. Quickly submit several jobs from past exercises View the jobs in the queue with condor_q Remove them all at once\\ condor_rm userid \\ Use your login user ID for userid . Use condor_q to track progress In case you are wondering, you can remove only your own jobs. HTCondor administrators can remove anyone\u2019s jobs, so be nice to them.","title":"1.6. Remove jobs"},{"location":"materials/day1/part1-ex6-remove/#monday-exercise-16-remove-jobs-from-the-queue","text":"The goal of this exercise is to show you how to remove jobs from the queue. This is helpful if you make a mistake, do not want to wait for a job to complete, or otherwise need to fix things. For example, if some test jobs go on hold for using too much memory or disk, you may want to just remove them, edit the submit files, and then submit again. NOTE: Please remember to remove any jobs from the queue that you have given up on. Otherwise, the queue will start to get very long with jobs that will waste resources (and decrease your priority), or that may never run (if they're on hold, or have other issues keeping them from matching). This exercise is very short, but if you are out of time, you can come back to it later.","title":"Monday Exercise 1.6: Remove Jobs From the Queue"},{"location":"materials/day1/part1-ex6-remove/#removing-a-job-from-the-queue","text":"To practice removing jobs from the queue, you need a job in the queue! Submit a job from an earlier exercise Determine the job ID ( cluster.process ) from the condor_submit output or from condor_q Remove the job:\\ condor_rm job_id \\ Use the full job ID this time, e.g., 5759.0 . Did the job leave the queue immediately? If not, about how long did it take? So far, we have created job clusters that contain only one job process (the .0 part of the job ID). That will change soon, so it is good to know how to remove a specific job ID. However, it is possible to remove all jobs that are part of a cluster at once. Simply omit the job process (the .0 part of the job ID) in the condor_rm command: %UCL_PROMPT_SHORT% <strong>condor_rm <em>cluster</em></strong> Finally, you can include many job clusters and full job IDs in a single condor_rm command. For example: %UCL_PROMPT_SHORT% <strong>condor_rm 5768 5769 5770.0 5771.2</strong>","title":"Removing a Job From the Queue"},{"location":"materials/day1/part1-ex6-remove/#removing-all-of-your-jobs","text":"If you really want to remove all of your jobs at once, you can do that. Quickly submit several jobs from past exercises View the jobs in the queue with condor_q Remove them all at once\\ condor_rm userid \\ Use your login user ID for userid . Use condor_q to track progress In case you are wondering, you can remove only your own jobs. HTCondor administrators can remove anyone\u2019s jobs, so be nice to them.","title":"Removing All of Your Jobs"},{"location":"materials/day1/part1-ex7-compile/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Monday Bonus Exercise 1.6: Compile and Run Some C Code \u00b6 The goal of this exercise is to show that compiled code works just fine in HTCondor. It is mainly of interest to people who have their own C code to run (or C++, or really any compiled code, although Java would be handled a bit differently). Preparing a C Executable \u00b6 When preparing a C program for HTCondor, it is best to compile and link the executable statically, so that it does not depend on external libraries and their particular versions. Why is this important? When your compiled C program is sent to another machine for execution, that machine may not have the same libraries that you have on your submit machine (or wherever you compile the program). If the libraries are not available or are the wrong versions, your program may fail or, perhaps worse, silently produce the wrong results. Here is a simple C program to try using (thanks, Alain Roy): #include <stdio.h> int main(int argc, char **argv) { int sleep_time; int input; int failure; if (argc != 3) { printf(\"Usage: simple <sleep-time> <integer>\\n\"); failure = 1; } else { sleep_time = atoi(argv[1]); input = atoi(argv[2]); printf(\"Thinking really hard for %d seconds...\\n\", sleep_time); sleep(sleep_time); printf(\"We calculated: %d\\n\", input * 2); failure = 0; } return failure; } Save that code to a file, for example, simple.c . Compile the program with static linking: %UCL_PROMPT_SHORT% <strong>gcc -static -o simple simple.c</strong> As always, test that you can run your command from the command line first. First, without arguments to make sure it fails correctly: %UCL_PROMPT_SHORT% <strong>./simple</strong> and then with valid arguments: %UCL_PROMPT_SHORT% <strong>./simple 5 21</strong> Running a Compiled C Program \u00b6 The rest is simple. In fact, it is no different than running any other program. Here is a basic submit file for the C program (call it simple.sub): universe = vanilla executable = simple arguments = \"60 64\" output = c-program.out error = c-program.err log = c-program.log should_transfer_files = YES when_to_transfer_output = ON_EXIT queue Then submit the job as usual! In summary, it is easy to work with statically linked compiled code. It is possible to handle dynamically linked compiled code, but it is trickier. We will only mention this topic briefly on Wednesday.","title":"1.7. (opt) Compiling"},{"location":"materials/day1/part1-ex7-compile/#monday-bonus-exercise-16-compile-and-run-some-c-code","text":"The goal of this exercise is to show that compiled code works just fine in HTCondor. It is mainly of interest to people who have their own C code to run (or C++, or really any compiled code, although Java would be handled a bit differently).","title":"Monday Bonus Exercise 1.6: Compile and Run Some C Code"},{"location":"materials/day1/part1-ex7-compile/#preparing-a-c-executable","text":"When preparing a C program for HTCondor, it is best to compile and link the executable statically, so that it does not depend on external libraries and their particular versions. Why is this important? When your compiled C program is sent to another machine for execution, that machine may not have the same libraries that you have on your submit machine (or wherever you compile the program). If the libraries are not available or are the wrong versions, your program may fail or, perhaps worse, silently produce the wrong results. Here is a simple C program to try using (thanks, Alain Roy): #include <stdio.h> int main(int argc, char **argv) { int sleep_time; int input; int failure; if (argc != 3) { printf(\"Usage: simple <sleep-time> <integer>\\n\"); failure = 1; } else { sleep_time = atoi(argv[1]); input = atoi(argv[2]); printf(\"Thinking really hard for %d seconds...\\n\", sleep_time); sleep(sleep_time); printf(\"We calculated: %d\\n\", input * 2); failure = 0; } return failure; } Save that code to a file, for example, simple.c . Compile the program with static linking: %UCL_PROMPT_SHORT% <strong>gcc -static -o simple simple.c</strong> As always, test that you can run your command from the command line first. First, without arguments to make sure it fails correctly: %UCL_PROMPT_SHORT% <strong>./simple</strong> and then with valid arguments: %UCL_PROMPT_SHORT% <strong>./simple 5 21</strong>","title":"Preparing a C Executable"},{"location":"materials/day1/part1-ex7-compile/#running-a-compiled-c-program","text":"The rest is simple. In fact, it is no different than running any other program. Here is a basic submit file for the C program (call it simple.sub): universe = vanilla executable = simple arguments = \"60 64\" output = c-program.out error = c-program.err log = c-program.log should_transfer_files = YES when_to_transfer_output = ON_EXIT queue Then submit the job as usual! In summary, it is easy to work with statically linked compiled code. It is possible to handle dynamically linked compiled code, but it is trickier. We will only mention this topic briefly on Wednesday.","title":"Running a Compiled C Program"},{"location":"materials/day1/part2-ex1-queue/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Monday Exercise 2.1: Explore condor_q \u00b6 The goal of this exercise is try out some of the most common options to the condor_q command, so that you can view jobs effectively. The main part of this exercise should take just a few minutes, but if you have more time later, come back and work on the extension ideas at the end to become a condor_q expert! Selecting Jobs \u00b6 The condor_q program has many options for selecting which jobs are listed. You have already seen that the default mode (as of version 8.5) is to show only your jobs in \"batch\" mode: %UCL_PROMPT_SHORT% <strong>condor_q</strong> You've seen that you can view all jobs (all users) in the submit node's queue by using the -all argument: %UCL_PROMPT_SHORT% <strong>condor_q -all</strong> And you've seen that you can view more details about queued jobs, with each separate job on a single line using the -nobatch option: %UCL_PROMPT_SHORT% <strong>condor_q -nobatch</strong> %UCL_PROMPT_SHORT% <strong>condor_q -all -nobatch</strong> Did you know you can also name one or more user IDs on the command line, in which case jobs for all of the named users are listed at once? %UCL_PROMPT_SHORT% <strong>condor_q <em><i>username1 username2 username3</i></em></strong> There are two other, simple selection criteria that you can use. To list just the jobs associated with a single cluster number: %UCL_PROMPT_SHORT% <strong>condor_q <em><i>CLUSTER</i></em></strong> For example, if you want to see the jobs in cluster 5678 (i.e., 5678.0 , 5678.1 , etc.), you use condor_q 5678 . To list a specific job (i.e., cluster.process, as in 5678.0): %UCL_PROMPT_SHORT% <strong>condor_q <em><i>JOB_ID</i></em></strong> For example, to see job ID 5678.1, you use condor_q 5678.1 . Note: You can name more than one cluster, job ID, or combination thereof on the command line, in which case jobs for all of the named clusters and/or job IDs are listed. Let\u2019s get some practice using condor_q selections! Using a previous exercise, submit several sleep jobs List all jobs in the queue \u2014 are there others besides your own? Practice using all forms of condor_q that you have learned: List just your jobs, with and without batching List a specific cluster List a specific job ID Try listing several users at once Try listing several clusters and job IDs at once When there are a variety of jobs in the queue, try combining a user ID and a different user's cluster or job ID in the same command \u2014 what happens? Viewing a Job ClassAd \u00b6 You may have wondered why it is useful to be able to list a single job ID using condor_q . By itself, it may not be that useful. But, in combination with another option, it is very useful! If you add the -long option to condor_q (or its short form, -l ), it will show the complete ClassAd for each selected job, instead of the one-line summary that you have seen so far. Because job ClassAds may have 80\u201390 attributes (or more), it probably makes the most sense to show the ClassAd for a single job at a time. And you know how to show just one job! Here is what the command looks like: %UCL_PROMPT_SHORT% <strong>condor_q -long <em>job-id</em></strong> The output from this command is long and complex. Most of the attributes that HTCondor adds to a job are arcane and uninteresting for us now. But here are some examples of common, interesting attributes taken directly from condor_q output (except with some line breaks added to the Requirements attribute): MyType = \"Job\" Err = \"sleep.err\" UserLog = \"/home/cat/1-monday-2.1-queue/sleep.log\" JobUniverse = 5 Requirements = ( IsOSGSchoolSlot =?= true ) && ( TARGET.Arch == \"X86_64\" ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ) ClusterId = 2420 WhenToTransferOutput = \"ON_EXIT\" Owner = \"cat\" CondorVersion = \"$CondorVersion: 8.5.5 May 03 2016 BuildID: 366162 $\" Out = \"sleep.out\" Cmd = \"/bin/sleep\" Arguments = \"120\" Note: Attributes are listed in no particular order and may change from time to time. Do not assume anything about the order of attributes in condor_q output. See what you can find in a job ClassAd from your own job. Using a previous exercise, submit a sleep job Before the job executes, capture its ClassAd and save to a file:\\ condor_q -l job-id > classad-1.txt After the job starts execution but before it finishes, capture its ClassAd again and save to a file:\\ condor_q -l job-id > classad-2.txt Now examine each saved ClassAd file. Here are a few things to look for: Can you find attributes that came from your submit file? (E.g., JobUniverse, Cmd, Arguments, Out, Err, UserLog, and so forth) Can you find attributes that could have come from your submit file, but that HTCondor added for you? (E.g., Requirements) How many of the following attributes can you guess the meaning of? DiskUsage ImageSize BytesSent JobStartDate \u2014 what format is the value in? (Hint: The HTCondor developers are primarily trained in the Unix way of doing things.) JobStatus Why Is My Job Not Running? \u00b6 Sometimes, you submit a job and it just sits in the queue in Idle state, never running. It can be difficult to figure out why a job never matches and runs. Fortunately, HTCondor can give you some help. To ask HTCondor why you job is not running, add the -better-analyze option to condor_q for the specific job. For example, for job ID 2423.0, the command is: %UCL_PROMPT_SHORT% <strong>condor_q -better-analyze <em>2423.0</em></strong> Of course, replace the job ID with your own. Let\u2019s submit a job that will never run and see what happens. Here is the submit file to use: universe = vanilla executable = /bin/hostname output = norun.out error = norun.err log = norun.log should_transfer_files = YES when_to_transfer_output = ON_EXIT request_memory = 2TB queue (Do you see what I did?) Save and submit this file Run condor_q -analyze on the job ID There is a lot of output, but a few items are worth highlighting. Here is a sample from my own job (with many lines left out): -- Submitter: learn.chtc.wisc.edu : .... ... --- 2423.000: Run analysis summary. <em>Of 12388 machines,</em> <em> 12388 are rejected by your job's requirements </em> ... WARNING: Be advised: <em> No resources matched request's constraints</em> The Requirements expression for your job is: ... Suggestions: Condition Machines Matched Suggestion --------- ---------------- ---------- <em>1 ( TARGET.Memory >= 2097152 ) 0 MODIFY TO 1000064</em> 2 ( ... ) 12145 3 ( TARGET.Arch == \"X86_64\" ) 12388 4 ( TARGET.OpSys == \"LINUX\" ) 12386 5 ( TARGET.Disk >= 20 ) 12387 6 ( TARGET.HasFileTransfer ) 12388 Toward the top, condor_q said that it considered 12388 \u201cmachines\u201d (really, slots) and all 12388 of them were rejected by my job\u2019s requirements . In other words, I am asking for something that is not available. But what? The real clue comes from the breakdown of the Requirements expression, at the end of the output. Note the highlighted line: My job asked for 2 terabytes of memory (2,097,152 MB) and no machines matched that part of the expression. Well, of course! 2 TB is a lot of memory on today\u2019s machines. And finally, note the suggestion: If I reduce my memory request to 1,000,064 MB (about 1 TB), there will be at least one slot in the pool that will match that expression. The output from condor_q -analyze (and condor_q -better-analyze ) may be helpful or it may not be, depending on your exact case. The example above was constructed so that it would be obvious what the problem was. But in many cases, this is a good place to start looking if you are having problems matching. Automatic Formatting Output (Optional) \u00b6 There is a way to format output from condor_q with the -autoformat or -af option. In this case, HTCondor decides for you how to format the data you ask for from job ClassAd(s). (To tell HTCondor how to format this information, yourself, you could use the -format option, which we're not covering.) To use autoformatting, use the -af option followed by the attribute name, for each attribute that you want to output: %UCL_PROMPT_SHORT% <strong>condor_q -af Owner -af ClusterId -af Cmd</strong> moate 2418 /share/test.sh cat 2421 /bin/sleep cat 2422 /bin/sleep Note that the -af option saves some typing but provides less control over the formatting of the output. Bonus Question : If you wanted to print out the Requirements expression of a job, how would you do that with -af ? Is the output what you expected? (HINT: for ClassAd attributes like \"Requirements\" that are long expressions, instead of simple values, you can use -af:r to view the expressions, instead of what it's current evaluation.) References \u00b6 As suggested above, if you want to learn more about condor_q , you can do some reading: Read the condor_q man page or HTCondor Manual section (same text) to learn about more options Read about ClassAd attributes in Appendix A of the HTCondor Manual","title":"2.1. condor_q"},{"location":"materials/day1/part2-ex1-queue/#monday-exercise-21-explore-condor_q","text":"The goal of this exercise is try out some of the most common options to the condor_q command, so that you can view jobs effectively. The main part of this exercise should take just a few minutes, but if you have more time later, come back and work on the extension ideas at the end to become a condor_q expert!","title":"Monday Exercise 2.1: Explore condor_q"},{"location":"materials/day1/part2-ex1-queue/#selecting-jobs","text":"The condor_q program has many options for selecting which jobs are listed. You have already seen that the default mode (as of version 8.5) is to show only your jobs in \"batch\" mode: %UCL_PROMPT_SHORT% <strong>condor_q</strong> You've seen that you can view all jobs (all users) in the submit node's queue by using the -all argument: %UCL_PROMPT_SHORT% <strong>condor_q -all</strong> And you've seen that you can view more details about queued jobs, with each separate job on a single line using the -nobatch option: %UCL_PROMPT_SHORT% <strong>condor_q -nobatch</strong> %UCL_PROMPT_SHORT% <strong>condor_q -all -nobatch</strong> Did you know you can also name one or more user IDs on the command line, in which case jobs for all of the named users are listed at once? %UCL_PROMPT_SHORT% <strong>condor_q <em><i>username1 username2 username3</i></em></strong> There are two other, simple selection criteria that you can use. To list just the jobs associated with a single cluster number: %UCL_PROMPT_SHORT% <strong>condor_q <em><i>CLUSTER</i></em></strong> For example, if you want to see the jobs in cluster 5678 (i.e., 5678.0 , 5678.1 , etc.), you use condor_q 5678 . To list a specific job (i.e., cluster.process, as in 5678.0): %UCL_PROMPT_SHORT% <strong>condor_q <em><i>JOB_ID</i></em></strong> For example, to see job ID 5678.1, you use condor_q 5678.1 . Note: You can name more than one cluster, job ID, or combination thereof on the command line, in which case jobs for all of the named clusters and/or job IDs are listed. Let\u2019s get some practice using condor_q selections! Using a previous exercise, submit several sleep jobs List all jobs in the queue \u2014 are there others besides your own? Practice using all forms of condor_q that you have learned: List just your jobs, with and without batching List a specific cluster List a specific job ID Try listing several users at once Try listing several clusters and job IDs at once When there are a variety of jobs in the queue, try combining a user ID and a different user's cluster or job ID in the same command \u2014 what happens?","title":"Selecting Jobs"},{"location":"materials/day1/part2-ex1-queue/#viewing-a-job-classad","text":"You may have wondered why it is useful to be able to list a single job ID using condor_q . By itself, it may not be that useful. But, in combination with another option, it is very useful! If you add the -long option to condor_q (or its short form, -l ), it will show the complete ClassAd for each selected job, instead of the one-line summary that you have seen so far. Because job ClassAds may have 80\u201390 attributes (or more), it probably makes the most sense to show the ClassAd for a single job at a time. And you know how to show just one job! Here is what the command looks like: %UCL_PROMPT_SHORT% <strong>condor_q -long <em>job-id</em></strong> The output from this command is long and complex. Most of the attributes that HTCondor adds to a job are arcane and uninteresting for us now. But here are some examples of common, interesting attributes taken directly from condor_q output (except with some line breaks added to the Requirements attribute): MyType = \"Job\" Err = \"sleep.err\" UserLog = \"/home/cat/1-monday-2.1-queue/sleep.log\" JobUniverse = 5 Requirements = ( IsOSGSchoolSlot =?= true ) && ( TARGET.Arch == \"X86_64\" ) && ( TARGET.OpSys == \"LINUX\" ) && ( TARGET.Disk >= RequestDisk ) && ( TARGET.Memory >= RequestMemory ) && ( TARGET.HasFileTransfer ) ClusterId = 2420 WhenToTransferOutput = \"ON_EXIT\" Owner = \"cat\" CondorVersion = \"$CondorVersion: 8.5.5 May 03 2016 BuildID: 366162 $\" Out = \"sleep.out\" Cmd = \"/bin/sleep\" Arguments = \"120\" Note: Attributes are listed in no particular order and may change from time to time. Do not assume anything about the order of attributes in condor_q output. See what you can find in a job ClassAd from your own job. Using a previous exercise, submit a sleep job Before the job executes, capture its ClassAd and save to a file:\\ condor_q -l job-id > classad-1.txt After the job starts execution but before it finishes, capture its ClassAd again and save to a file:\\ condor_q -l job-id > classad-2.txt Now examine each saved ClassAd file. Here are a few things to look for: Can you find attributes that came from your submit file? (E.g., JobUniverse, Cmd, Arguments, Out, Err, UserLog, and so forth) Can you find attributes that could have come from your submit file, but that HTCondor added for you? (E.g., Requirements) How many of the following attributes can you guess the meaning of? DiskUsage ImageSize BytesSent JobStartDate \u2014 what format is the value in? (Hint: The HTCondor developers are primarily trained in the Unix way of doing things.) JobStatus","title":"Viewing a Job ClassAd"},{"location":"materials/day1/part2-ex1-queue/#why-is-my-job-not-running","text":"Sometimes, you submit a job and it just sits in the queue in Idle state, never running. It can be difficult to figure out why a job never matches and runs. Fortunately, HTCondor can give you some help. To ask HTCondor why you job is not running, add the -better-analyze option to condor_q for the specific job. For example, for job ID 2423.0, the command is: %UCL_PROMPT_SHORT% <strong>condor_q -better-analyze <em>2423.0</em></strong> Of course, replace the job ID with your own. Let\u2019s submit a job that will never run and see what happens. Here is the submit file to use: universe = vanilla executable = /bin/hostname output = norun.out error = norun.err log = norun.log should_transfer_files = YES when_to_transfer_output = ON_EXIT request_memory = 2TB queue (Do you see what I did?) Save and submit this file Run condor_q -analyze on the job ID There is a lot of output, but a few items are worth highlighting. Here is a sample from my own job (with many lines left out): -- Submitter: learn.chtc.wisc.edu : .... ... --- 2423.000: Run analysis summary. <em>Of 12388 machines,</em> <em> 12388 are rejected by your job's requirements </em> ... WARNING: Be advised: <em> No resources matched request's constraints</em> The Requirements expression for your job is: ... Suggestions: Condition Machines Matched Suggestion --------- ---------------- ---------- <em>1 ( TARGET.Memory >= 2097152 ) 0 MODIFY TO 1000064</em> 2 ( ... ) 12145 3 ( TARGET.Arch == \"X86_64\" ) 12388 4 ( TARGET.OpSys == \"LINUX\" ) 12386 5 ( TARGET.Disk >= 20 ) 12387 6 ( TARGET.HasFileTransfer ) 12388 Toward the top, condor_q said that it considered 12388 \u201cmachines\u201d (really, slots) and all 12388 of them were rejected by my job\u2019s requirements . In other words, I am asking for something that is not available. But what? The real clue comes from the breakdown of the Requirements expression, at the end of the output. Note the highlighted line: My job asked for 2 terabytes of memory (2,097,152 MB) and no machines matched that part of the expression. Well, of course! 2 TB is a lot of memory on today\u2019s machines. And finally, note the suggestion: If I reduce my memory request to 1,000,064 MB (about 1 TB), there will be at least one slot in the pool that will match that expression. The output from condor_q -analyze (and condor_q -better-analyze ) may be helpful or it may not be, depending on your exact case. The example above was constructed so that it would be obvious what the problem was. But in many cases, this is a good place to start looking if you are having problems matching.","title":"Why Is My Job Not Running?"},{"location":"materials/day1/part2-ex1-queue/#automatic-formatting-output-optional","text":"There is a way to format output from condor_q with the -autoformat or -af option. In this case, HTCondor decides for you how to format the data you ask for from job ClassAd(s). (To tell HTCondor how to format this information, yourself, you could use the -format option, which we're not covering.) To use autoformatting, use the -af option followed by the attribute name, for each attribute that you want to output: %UCL_PROMPT_SHORT% <strong>condor_q -af Owner -af ClusterId -af Cmd</strong> moate 2418 /share/test.sh cat 2421 /bin/sleep cat 2422 /bin/sleep Note that the -af option saves some typing but provides less control over the formatting of the output. Bonus Question : If you wanted to print out the Requirements expression of a job, how would you do that with -af ? Is the output what you expected? (HINT: for ClassAd attributes like \"Requirements\" that are long expressions, instead of simple values, you can use -af:r to view the expressions, instead of what it's current evaluation.)","title":"Automatic Formatting Output (Optional)"},{"location":"materials/day1/part2-ex1-queue/#references","text":"As suggested above, if you want to learn more about condor_q , you can do some reading: Read the condor_q man page or HTCondor Manual section (same text) to learn about more options Read about ClassAd attributes in Appendix A of the HTCondor Manual","title":"References"},{"location":"materials/day1/part2-ex2-status/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Monday Exercise 2.2: Explore condor_status \u00b6 The goal of this exercise is try out some of the most common options to the condor_status command, so that you can view slots effectively. The main part of this exercise should take just a few minutes, but if you have more time later, come back and work on the extension ideas at the end to become a condor_status expert! Selecting Slots \u00b6 The condor_status program has many options for selecting which slots are listed. You've already learned the basic condor_status and the condor_status -compact variation (which you may wish to retry now, before proceeding). Another convenient option is to list only those slots that are available now: %UCL_PROMPT_SHORT% <strong>condor_status -avail</strong> Of course, the individual execute machines only report their slots to the collector at certain time intervals, so this list will not reflect the up-to-the-second reality of all slots. But this limitation is true of all condor_status output, not just with the -avail option. Similar to condor_q , you can limit the slots that are listed in two easy ways. To list just the slots on a specific machine: %UCL_PROMPT_SHORT% <strong>condor_status <em><i>HOSTNAME</i></em></strong> For example, if you want to see the slots on e242.chtc.wisc.edu (in the CHTC pool): %UCL_PROMPT_SHORT% <strong>condor_status e242.chtc.wisc.edu</strong> To list a specific slot on a machine: %UCL_PROMPT_SHORT% <strong>condor_status <em><i>SLOT</i></em>@<em><i>HOSTNAME</i></em></strong> For example, to see the \u201cfirst\u201d slot on the machine above: %UCL_PROMPT_SHORT% <strong>condor_status slot1@e242.chtc.wisc.edu</strong> Note: You can name more than one hostname, slot, or combination thereof on the command line, in which case slots for all of the named hostnames and/or slots are listed. Let\u2019s get some practice using condor_status selections! List all slots in the pool \u2014 how many are there total? Practice using all forms of condor_status that you have learned: List the available slots List the slots on a specific machine (e.g., e242.chtc.wisc.edu ) List a specific slot from that machine Try listing the slots from a few (but not all) machines at once Try using a mix of hostnames and slot IDs at once Viewing a Slot ClassAd \u00b6 Just as with condor_q , you can use condor_status to view the complete ClassAd for a given slot (often confusingly called the \u201cmachine\u201d ad): %UCL_PROMPT_SHORT% <strong>condor_status -long <em><i>SLOT</i></em>@<em><i>HOSTNAME</i></em></strong> Because slot ClassAds may have 150\u2013200 attributes (or more), it probably makes the most sense to show the ClassAd for a single slot at a time, as shown above. Here are some examples of common, interesting attributes taken directly from condor_status output: OpSys = \"LINUX\" DetectedCpus = 24 OpSysAndVer = \"SL6\" MyType = \"Machine\" LoadAvg = 0.99 TotalDisk = 798098404 OSIssue = \"Scientific Linux release 6.6 (Carbon)\" TotalMemory = 24016 Machine = \"e242.chtc.wisc.edu\" CondorVersion = \"$CondorVersion: 8.5.5 May 03 2016 BuildID: 366162 $\" Memory = 1024 As you may be able to tell, there is a mix of attributes about the machine as a whole (hence the name \u201cmachine ad\u201d) and about the slot in particular. Go ahead and examine a machine ClassAd now. I suggest looking at one of the slots on, say, c010.chtc.wisc.edu because of its relatively simple configuration. Viewing Slots by ClassAd Expression \u00b6 Often, it is helpful to view slots that meet some particular criteria. For example, if you know that your job needs a lot of memory to run, you may want to see how many high-memory slots there are and whether they are busy. You can filter the list of slots like this using the -constraint option and a ClassAd expression. For example, suppose we want to list all slots that are running Scientific Linux 6 (operating system) and have at least 16 GB memory available. Note that memory is reported in units of Megabytes. The command is: %UCL_PROMPT_SHORT% <strong>condor_status -constraint 'OpSysAndVer == \"SL6\" && Memory >= 64000'</strong> Note: Be very careful with using quote characters appropriately in these commands. In the example above, the single quotes ( ' ) are for the shell, so that the entire expression is passed to condor_status untouched, and the double quotes ( \" ) surround a string value within the expression itself. Currently on CHTC, there are only a few slots that meet these criteria. If you are interested in learning more about writing ClassAd expressions, look at section 4.1 and especially 4.1.4 of the HTCondor Manual. This is definitely advanced material, so if you do not want to read it, that is fine. But if you do, take some time to practice writing expressions for the condor_status -constraint command. Note: The condor_q command accepts the -constraint option as well! As you might expect, the option allows you to limit the jobs that are listed based on a ClassAd expression. Formatting Output (Optional) \u00b6 The condor_status command accepts the same -format ( -f ) and -autoformat ( -af ) options that condor_q accepts, and the options have the same meanings in both commands. Of course, the attributes available in machine ads may differ from the ones that are available in job ads. Use the HTCondor Manual or look at individual slot ClassAds to get a better idea of what attributes are available. For example, I was curious about the Windows slot listed in the condor_status summary output. Here are two commands that show the full hostnames and major version information for the Windows slots: %UCL_PROMPT_SHORT% <strong>condor_status -format '%30s ' Machine -format '%s\\n' OpSysAndVer -constraint 'OpSys == \"WINDOWS\"'</strong> %UCL_PROMPT_SHORT% <strong>condor_status -af Machine -af OpSysAndVer -constraint 'OpSys == \"WINDOWS\"'</strong> If you like, spend a few minutes now or later experimenting with condor_status formatting. References \u00b6 As suggested above, if you want to learn more about condor_q , you can do some reading: Read the condor_status man page or HTCondor Manual section (same text) to learn about more options Read about ClassAd attributes in Appendix A of the HTCondor Manual Read about ClassAd expressions in section 4.1.4 of the HTCondor Manual","title":"2.2. condor_status"},{"location":"materials/day1/part2-ex2-status/#monday-exercise-22-explore-condor_status","text":"The goal of this exercise is try out some of the most common options to the condor_status command, so that you can view slots effectively. The main part of this exercise should take just a few minutes, but if you have more time later, come back and work on the extension ideas at the end to become a condor_status expert!","title":"Monday Exercise 2.2: Explore condor_status"},{"location":"materials/day1/part2-ex2-status/#selecting-slots","text":"The condor_status program has many options for selecting which slots are listed. You've already learned the basic condor_status and the condor_status -compact variation (which you may wish to retry now, before proceeding). Another convenient option is to list only those slots that are available now: %UCL_PROMPT_SHORT% <strong>condor_status -avail</strong> Of course, the individual execute machines only report their slots to the collector at certain time intervals, so this list will not reflect the up-to-the-second reality of all slots. But this limitation is true of all condor_status output, not just with the -avail option. Similar to condor_q , you can limit the slots that are listed in two easy ways. To list just the slots on a specific machine: %UCL_PROMPT_SHORT% <strong>condor_status <em><i>HOSTNAME</i></em></strong> For example, if you want to see the slots on e242.chtc.wisc.edu (in the CHTC pool): %UCL_PROMPT_SHORT% <strong>condor_status e242.chtc.wisc.edu</strong> To list a specific slot on a machine: %UCL_PROMPT_SHORT% <strong>condor_status <em><i>SLOT</i></em>@<em><i>HOSTNAME</i></em></strong> For example, to see the \u201cfirst\u201d slot on the machine above: %UCL_PROMPT_SHORT% <strong>condor_status slot1@e242.chtc.wisc.edu</strong> Note: You can name more than one hostname, slot, or combination thereof on the command line, in which case slots for all of the named hostnames and/or slots are listed. Let\u2019s get some practice using condor_status selections! List all slots in the pool \u2014 how many are there total? Practice using all forms of condor_status that you have learned: List the available slots List the slots on a specific machine (e.g., e242.chtc.wisc.edu ) List a specific slot from that machine Try listing the slots from a few (but not all) machines at once Try using a mix of hostnames and slot IDs at once","title":"Selecting Slots"},{"location":"materials/day1/part2-ex2-status/#viewing-a-slot-classad","text":"Just as with condor_q , you can use condor_status to view the complete ClassAd for a given slot (often confusingly called the \u201cmachine\u201d ad): %UCL_PROMPT_SHORT% <strong>condor_status -long <em><i>SLOT</i></em>@<em><i>HOSTNAME</i></em></strong> Because slot ClassAds may have 150\u2013200 attributes (or more), it probably makes the most sense to show the ClassAd for a single slot at a time, as shown above. Here are some examples of common, interesting attributes taken directly from condor_status output: OpSys = \"LINUX\" DetectedCpus = 24 OpSysAndVer = \"SL6\" MyType = \"Machine\" LoadAvg = 0.99 TotalDisk = 798098404 OSIssue = \"Scientific Linux release 6.6 (Carbon)\" TotalMemory = 24016 Machine = \"e242.chtc.wisc.edu\" CondorVersion = \"$CondorVersion: 8.5.5 May 03 2016 BuildID: 366162 $\" Memory = 1024 As you may be able to tell, there is a mix of attributes about the machine as a whole (hence the name \u201cmachine ad\u201d) and about the slot in particular. Go ahead and examine a machine ClassAd now. I suggest looking at one of the slots on, say, c010.chtc.wisc.edu because of its relatively simple configuration.","title":"Viewing a Slot ClassAd"},{"location":"materials/day1/part2-ex2-status/#viewing-slots-by-classad-expression","text":"Often, it is helpful to view slots that meet some particular criteria. For example, if you know that your job needs a lot of memory to run, you may want to see how many high-memory slots there are and whether they are busy. You can filter the list of slots like this using the -constraint option and a ClassAd expression. For example, suppose we want to list all slots that are running Scientific Linux 6 (operating system) and have at least 16 GB memory available. Note that memory is reported in units of Megabytes. The command is: %UCL_PROMPT_SHORT% <strong>condor_status -constraint 'OpSysAndVer == \"SL6\" && Memory >= 64000'</strong> Note: Be very careful with using quote characters appropriately in these commands. In the example above, the single quotes ( ' ) are for the shell, so that the entire expression is passed to condor_status untouched, and the double quotes ( \" ) surround a string value within the expression itself. Currently on CHTC, there are only a few slots that meet these criteria. If you are interested in learning more about writing ClassAd expressions, look at section 4.1 and especially 4.1.4 of the HTCondor Manual. This is definitely advanced material, so if you do not want to read it, that is fine. But if you do, take some time to practice writing expressions for the condor_status -constraint command. Note: The condor_q command accepts the -constraint option as well! As you might expect, the option allows you to limit the jobs that are listed based on a ClassAd expression.","title":"Viewing Slots by ClassAd Expression"},{"location":"materials/day1/part2-ex2-status/#formatting-output-optional","text":"The condor_status command accepts the same -format ( -f ) and -autoformat ( -af ) options that condor_q accepts, and the options have the same meanings in both commands. Of course, the attributes available in machine ads may differ from the ones that are available in job ads. Use the HTCondor Manual or look at individual slot ClassAds to get a better idea of what attributes are available. For example, I was curious about the Windows slot listed in the condor_status summary output. Here are two commands that show the full hostnames and major version information for the Windows slots: %UCL_PROMPT_SHORT% <strong>condor_status -format '%30s ' Machine -format '%s\\n' OpSysAndVer -constraint 'OpSys == \"WINDOWS\"'</strong> %UCL_PROMPT_SHORT% <strong>condor_status -af Machine -af OpSysAndVer -constraint 'OpSys == \"WINDOWS\"'</strong> If you like, spend a few minutes now or later experimenting with condor_status formatting.","title":"Formatting Output (Optional)"},{"location":"materials/day1/part2-ex2-status/#references","text":"As suggested above, if you want to learn more about condor_q , you can do some reading: Read the condor_status man page or HTCondor Manual section (same text) to learn about more options Read about ClassAd attributes in Appendix A of the HTCondor Manual Read about ClassAd expressions in section 4.1.4 of the HTCondor Manual","title":"References"},{"location":"materials/day1/part2-ex3-files/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Monday Exercise 2.3: Work With Input and Output Files \u00b6 The goal of this exercise is make input files available to your job on the execute machine, and return output files back. This small change significantly adds to the kinds of jobs that you can run. Viewing a Job Sandbox \u00b6 Before you learn to transfer files to and from your job, it is good to understand a bit more about the environment in which your job runs. When the HTCondor starter process prepares to run your job, it creates a new directory for your job and all of its files. We call this directory the job sandbox , because it is your job\u2019s private space to play. Let\u2019s see what is in the job sandbox for a very simple job with no special input or output files. Save the script below in a file named sandbox.sh :\\ #/bin/sh echo 'Date: ' `date` echo 'Host: ' `hostname` echo 'System: ' `uname -spo` echo 'OS info: ' `cat /etc/redhat-release` echo 'Sandbox: ' `pwd` echo ls -alF Create a submit file for this script and submit it When the job finishes, look at the contents of the output file In the output file, note the Sandbox: line: That is the full path to your job sandbox for the run. It was created just for your job, and it was removed as soon as your job finished. Next, look at the output that appears after the Sandbox: line; it is the output from the ls command in the script. It shows all of the files in your job sandbox, as they existed at the end of the execution of sandbox.sh . The files are: .chirp.config Configuration for an advanced feature .job.ad The job ClassAd .machine.ad The machine ClassAd _condor_stderr Saved standard error from the job _condor_stdout Saved standard output from the job condor_exec.exe The executable, renamed from sandbox.sh tmp/ A directory in which to put temporary files So, HTCondor wrote copies of the job and machine ads (for use by the job, if desired), transferred your executable ( sandbox.sh ), renamed it ( condor_exec.exe ), ran it, and saved its standard output and standard error into files. Notice that your submit file, which was in the same directory on the submit machine as your executable, was not transferred, nor were any other files that happened to be in directory with the submit file. Now that we know something about the sandbox, we can transfer more files to and from it. Running a Job With Input Files \u00b6 Next, you will run a job that requires an input file. As with all previous examples, you will tell HTCondor to transfer files to the sandbox (=should_transfer_files = YES=). Remember, the initial job sandbox contains only the renamed job executable and nothing else from your directory on the submit machine. You must tell HTCondor explicitly about every other file to transfer to the sandbox. Fortunately, this is easy. Here is a simple Python script that takes the name of an input file (containing one word per line) from the command line, counts the number of times each (lowercased) word occurs in the text, and prints out the final list of words and their counts. #!/usr/bin/env python import os import sys if len(sys.argv) != 2: print 'Usage: %s DATA' % (os.path.basename(sys.argv[0])) sys.exit(1) input_filename = sys.argv[1] words = {} my_file = open(input_filename, 'r') for line in my_file: word = line.strip().lower() if word in words: words[word] += 1 else: words[word] = 1 my_file.close() for word in sorted(words.keys()): print '%8d %s' % (words[word], word) Save the Python script in a file named freq.py Download the input file for the script (263K lines, ~1.4 MB) and save it in your submit directory:\\ wget http://proxy.chtc.wisc.edu/SQUID/osgschool16/mon-3.2-words.txt Create a basic submit file for the freq.py executable Add a line to tell HTCondor to transfer the input file:\\ transfer_input_files = mon-3.2-words.txt \\ As with all submit file commands, it does not matter where this line goes. I usually group it with the other file transfer commands. Do not forget to add a line to name the input file as the argument to the Python script Submit the job, wait for it to finish, and check the output! If things do not work the first time, keep trying! At this point in the exercises, we are telling you less and less explicitly how to do steps that you have done before. If you get stuck, ask a neighbor or one of the instructors. Note: If you want to transfer more than one input file, list all of them on a single transfer_input_files command, separated by commas. For example, if there are three input files: transfer_input_files = a.txt, b.txt, c.txt Extra Challenge \u00b6 Many standard command-line program operate on input files. For example, the cat command can takes one or more input files as arguments, printing to standard output each file in order. Other common commands that take input files as arguments are grep , diff , and sort . Using commands like these, or others that you know and that are readily available, create one or more submit files that take input files and produce output. If you are using a command that is not contained in your submit directory, be sure to put its complete path in your executable line; use the which command to find the paths of standard programs. Also be sure to set the arguments line correctly for each program. Transferring Output Files \u00b6 So far, we have relied on programs that send their output to the standard output and error streams, which HTCondor captures, saves, and returns back to the submit directory. But what if your program writes one or more files for its output? How do you tell HTCondor to bring them back? Let\u2019s start by exploring what happens to files that a jobs creates in the sandbox. We will use a very simple method for creating a new file: We will copy an input file to another name. Find or create a small input file (it is fine to use any small file from a previous exercise) Create a submit file that transfers the input file and copies it to another name (as if doing /bin/cp input.txt output.txt on the command line) Make the output filename different than any filenames that are in your submit directory What is the executable line? What is the arguments line? How do you tell HTCondor to transfer the input file? As always, use output , error , and log filenames that are different from previous exercises Submit the job and wait for it to finish What happened? Can you tell what HTCondor did with your output file, after it was created in the job sandbox? Look carefully at the list of files in your submit directory now\u2026 Transferring Specific Output Files \u00b6 As you saw in the last exercise, by default HTCondor transfers files that are created in the job sandbox back to the submit directory when the job finishes. In fact, HTCondor will also transfer back changed input files, too. But, this only works for files that are in the top-level sandbox directory, and not for ones contained in subdirectories. What if you want to bring back only some output files, or output files contained in subdirectories? Here is a simple shell script that creates several files, including a copy of an input file in a new subdirectory: #!/bin/sh if [ $# -ne 1 ]; then echo \"Usage: $0 INPUT\"; exit 1; fi date > output-timestamp.txt cal > output-calendar.txt mkdir subdirectory cp $1 subdirectory/backup-$1 First, let\u2019s confirm that HTCondor does not bring back the output file in the subdirectory: Save the shell script in a file named output.sh Write a submit file that transfers an input file and runs output.sh on it Submit the job, wait for it to finish, and examine the contents of your submit directory Suppose you decide that you want only the timestamp output file and all files in the subdirectory, but not the calendar output file. You can tell HTCondor to transfer these specific files: transfer_output_files = output-timestamp.txt, subdirectory/ Note: See the trailing slash ( / ) on the subdirectory? That tells HTCondor to transfer back the files contained in the subdirectory, but not the directory itself; the files will be written directly into the submit directory itself. If you want HTCondor to transfer back an entire directory, leave off the trailing slash. Remove all output files from the previous run, including output-timestamp.txt and output-calendar.txt Copy the previous submit file that ran output.sh and add the transfer_output_files line from above Submit the job, wait for it to finish, and examine the contents of your submit directory Did it work as you expected? Thinking About Progress So Far \u00b6 At this point, you can do just about everything that you need in order to run jobs on a local HTC pool. You can identify the executable, arguments, and input files, and you can get output back from the job. This is a big achievement! In some ways, everything after this exercise just makes it easier to run certain kinds of jobs and deal with certain kinds of situations. References \u00b6 There are many more details about HTCondor\u2019s file transfer mechanism not covered here. For more information, read section 2.5.9 of the HTCondor Manual.","title":"2.3. I/O files"},{"location":"materials/day1/part2-ex3-files/#monday-exercise-23-work-with-input-and-output-files","text":"The goal of this exercise is make input files available to your job on the execute machine, and return output files back. This small change significantly adds to the kinds of jobs that you can run.","title":"Monday Exercise 2.3: Work With Input and Output Files"},{"location":"materials/day1/part2-ex3-files/#viewing-a-job-sandbox","text":"Before you learn to transfer files to and from your job, it is good to understand a bit more about the environment in which your job runs. When the HTCondor starter process prepares to run your job, it creates a new directory for your job and all of its files. We call this directory the job sandbox , because it is your job\u2019s private space to play. Let\u2019s see what is in the job sandbox for a very simple job with no special input or output files. Save the script below in a file named sandbox.sh :\\ #/bin/sh echo 'Date: ' `date` echo 'Host: ' `hostname` echo 'System: ' `uname -spo` echo 'OS info: ' `cat /etc/redhat-release` echo 'Sandbox: ' `pwd` echo ls -alF Create a submit file for this script and submit it When the job finishes, look at the contents of the output file In the output file, note the Sandbox: line: That is the full path to your job sandbox for the run. It was created just for your job, and it was removed as soon as your job finished. Next, look at the output that appears after the Sandbox: line; it is the output from the ls command in the script. It shows all of the files in your job sandbox, as they existed at the end of the execution of sandbox.sh . The files are: .chirp.config Configuration for an advanced feature .job.ad The job ClassAd .machine.ad The machine ClassAd _condor_stderr Saved standard error from the job _condor_stdout Saved standard output from the job condor_exec.exe The executable, renamed from sandbox.sh tmp/ A directory in which to put temporary files So, HTCondor wrote copies of the job and machine ads (for use by the job, if desired), transferred your executable ( sandbox.sh ), renamed it ( condor_exec.exe ), ran it, and saved its standard output and standard error into files. Notice that your submit file, which was in the same directory on the submit machine as your executable, was not transferred, nor were any other files that happened to be in directory with the submit file. Now that we know something about the sandbox, we can transfer more files to and from it.","title":"Viewing a Job Sandbox"},{"location":"materials/day1/part2-ex3-files/#running-a-job-with-input-files","text":"Next, you will run a job that requires an input file. As with all previous examples, you will tell HTCondor to transfer files to the sandbox (=should_transfer_files = YES=). Remember, the initial job sandbox contains only the renamed job executable and nothing else from your directory on the submit machine. You must tell HTCondor explicitly about every other file to transfer to the sandbox. Fortunately, this is easy. Here is a simple Python script that takes the name of an input file (containing one word per line) from the command line, counts the number of times each (lowercased) word occurs in the text, and prints out the final list of words and their counts. #!/usr/bin/env python import os import sys if len(sys.argv) != 2: print 'Usage: %s DATA' % (os.path.basename(sys.argv[0])) sys.exit(1) input_filename = sys.argv[1] words = {} my_file = open(input_filename, 'r') for line in my_file: word = line.strip().lower() if word in words: words[word] += 1 else: words[word] = 1 my_file.close() for word in sorted(words.keys()): print '%8d %s' % (words[word], word) Save the Python script in a file named freq.py Download the input file for the script (263K lines, ~1.4 MB) and save it in your submit directory:\\ wget http://proxy.chtc.wisc.edu/SQUID/osgschool16/mon-3.2-words.txt Create a basic submit file for the freq.py executable Add a line to tell HTCondor to transfer the input file:\\ transfer_input_files = mon-3.2-words.txt \\ As with all submit file commands, it does not matter where this line goes. I usually group it with the other file transfer commands. Do not forget to add a line to name the input file as the argument to the Python script Submit the job, wait for it to finish, and check the output! If things do not work the first time, keep trying! At this point in the exercises, we are telling you less and less explicitly how to do steps that you have done before. If you get stuck, ask a neighbor or one of the instructors. Note: If you want to transfer more than one input file, list all of them on a single transfer_input_files command, separated by commas. For example, if there are three input files: transfer_input_files = a.txt, b.txt, c.txt","title":"Running a Job With Input Files"},{"location":"materials/day1/part2-ex3-files/#extra-challenge","text":"Many standard command-line program operate on input files. For example, the cat command can takes one or more input files as arguments, printing to standard output each file in order. Other common commands that take input files as arguments are grep , diff , and sort . Using commands like these, or others that you know and that are readily available, create one or more submit files that take input files and produce output. If you are using a command that is not contained in your submit directory, be sure to put its complete path in your executable line; use the which command to find the paths of standard programs. Also be sure to set the arguments line correctly for each program.","title":"Extra Challenge"},{"location":"materials/day1/part2-ex3-files/#transferring-output-files","text":"So far, we have relied on programs that send their output to the standard output and error streams, which HTCondor captures, saves, and returns back to the submit directory. But what if your program writes one or more files for its output? How do you tell HTCondor to bring them back? Let\u2019s start by exploring what happens to files that a jobs creates in the sandbox. We will use a very simple method for creating a new file: We will copy an input file to another name. Find or create a small input file (it is fine to use any small file from a previous exercise) Create a submit file that transfers the input file and copies it to another name (as if doing /bin/cp input.txt output.txt on the command line) Make the output filename different than any filenames that are in your submit directory What is the executable line? What is the arguments line? How do you tell HTCondor to transfer the input file? As always, use output , error , and log filenames that are different from previous exercises Submit the job and wait for it to finish What happened? Can you tell what HTCondor did with your output file, after it was created in the job sandbox? Look carefully at the list of files in your submit directory now\u2026","title":"Transferring Output Files"},{"location":"materials/day1/part2-ex3-files/#transferring-specific-output-files","text":"As you saw in the last exercise, by default HTCondor transfers files that are created in the job sandbox back to the submit directory when the job finishes. In fact, HTCondor will also transfer back changed input files, too. But, this only works for files that are in the top-level sandbox directory, and not for ones contained in subdirectories. What if you want to bring back only some output files, or output files contained in subdirectories? Here is a simple shell script that creates several files, including a copy of an input file in a new subdirectory: #!/bin/sh if [ $# -ne 1 ]; then echo \"Usage: $0 INPUT\"; exit 1; fi date > output-timestamp.txt cal > output-calendar.txt mkdir subdirectory cp $1 subdirectory/backup-$1 First, let\u2019s confirm that HTCondor does not bring back the output file in the subdirectory: Save the shell script in a file named output.sh Write a submit file that transfers an input file and runs output.sh on it Submit the job, wait for it to finish, and examine the contents of your submit directory Suppose you decide that you want only the timestamp output file and all files in the subdirectory, but not the calendar output file. You can tell HTCondor to transfer these specific files: transfer_output_files = output-timestamp.txt, subdirectory/ Note: See the trailing slash ( / ) on the subdirectory? That tells HTCondor to transfer back the files contained in the subdirectory, but not the directory itself; the files will be written directly into the submit directory itself. If you want HTCondor to transfer back an entire directory, leave off the trailing slash. Remove all output files from the previous run, including output-timestamp.txt and output-calendar.txt Copy the previous submit file that ran output.sh and add the transfer_output_files line from above Submit the job, wait for it to finish, and examine the contents of your submit directory Did it work as you expected?","title":"Transferring Specific Output Files"},{"location":"materials/day1/part2-ex3-files/#thinking-about-progress-so-far","text":"At this point, you can do just about everything that you need in order to run jobs on a local HTC pool. You can identify the executable, arguments, and input files, and you can get output back from the job. This is a big achievement! In some ways, everything after this exercise just makes it easier to run certain kinds of jobs and deal with certain kinds of situations.","title":"Thinking About Progress So Far"},{"location":"materials/day1/part2-ex3-files/#references","text":"There are many more details about HTCondor\u2019s file transfer mechanism not covered here. For more information, read section 2.5.9 of the HTCondor Manual.","title":"References"},{"location":"materials/day1/part2-ex4-queue-n/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Monday Exercise 2.4: Use queue N , $(Cluster), and $(Process) \u00b6 The goal of this exercise is to learn to submit many jobs from a single queue statement, and then to control filenames and arguments per job. Submitting Many Jobs With One Submit File \u00b6 Suppose you have a program that you want to run many times. The program takes an argument, and you want to change the argument for each run of the program. With what you know so far, you have a couple of choices (assuming that you cannot change the job itself to work this way): Write one submit file; submit one job, change the argument in the submit file, submit another job, change the submit file, \u2026 Write many submit files that are nearly identical except for the program argument Neither of these options seems very satisfying. Fortunately, we can do better with HTCondor. Running Many Jobs With One queue Statement \u00b6 Here is a C program that uses a simple stochastic (random) method to estimate the value of \u03c0 \u2014 feel free to try to figure out the method from the code, but it is not critical for this exercise. The single argument to the program is the number of samples to take. More samples should result in better estimates! #include <stdio.h> #include <stdlib.h> #include <sys/time.h> int main(int argc, char *argv[]) { struct timeval my_timeval; int iterations = 0; int inside_circle = 0; int i; double x, y, pi_estimate; gettimeofday(&my_timeval, NULL); srand48(my_timeval.tv_sec ^ my_timeval.tv_usec); if (argc == 2) { iterations = atoi(argv[1]); } else { printf(&quot;usage: circlepi ITERATIONS\\n&quot;); exit(1); } for (i = 0; i < iterations; i++) { x = (drand48() - 0.5) * 2.0; y = (drand48() - 0.5) * 2.0; if (((x * x) + (y * y)) <= 1.0) { inside_circle++; } } pi_estimate = 4.0 * ((double) inside_circle / (double) iterations); printf(&quot;%d iterations, %d inside; pi = %f\\n&quot;, iterations, inside_circle, pi_estimate); return 0; } In a new directory for this exercise, save the code to a file named circlepi.c Compile the code (we will cover this in more detail Wednesday):\\ gcc -static -o circlepi circlepi.c If there are errors, check the file contents and compile command carefully, otherwise see the instructors Test the program with just a few samples:\\ ./circlepi 10000 Now suppose that you want to run the program many times, to produce many estimates. This is exactly what a statement like queue 3 is useful for. Let\u2019s see how it works. Write a normal submit file for this program Pass 1 billion ( 1000000000 ) as the command line argument to circlepi Remember to use queue 3 instead of just queue Submit the file\\ Note the slightly different message from condor_submit : \\ 3 job(s) submitted to cluster NNNN . Before the jobs execute, look at the job queue to see the multiple jobs Here is some sample condor_q -nobatch output: ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 10228.0 cat 7/25 11:57 0+00:00:00 I 0 0.7 circlepi 1000000000 10228.1 cat 7/25 11:57 0+00:00:00 I 0 0.7 circlepi 1000000000 10228.2 cat 7/25 11:57 0+00:00:00 I 0 0.7 circlepi 1000000000 In this sample, all three jobs are part of cluster 10228 , but the first job was assigned process 0 , the second job was assigned process 1 , and the third one was assigned process 2 . (Historical note: Programmers like to start counting from 0, hence the odd numbering scheme.) At this time, it is worth reviewing the definition of a job ID . It is a job\u2019s cluster number, a dot ( . ), and the job\u2019s process number. So in the example above, the job ID of the second job is 10228.1 . Pop Quiz: Do you remember how to ask HTCondor to list all of the jobs from one cluster? How about one specific job ID? Using queue N With Output \u00b6 When all three jobs in your single cluster are finished, examine the resulting files. What is in the output file? What is in the error file (hopefully nothing)? What is in the log file? Look carefully at the job IDs in each event. Is this what you expected? Is it what you wanted? Using $(Process) to Distinguish Jobs \u00b6 As you saw with the experiment above, we need a way to separate output (and error) files per job that is queued , not just for the whole cluster of jobs. Fortunately, HTCondor has a way to separate the files easily. When processing a submit file, HTCondor defines and uses a special variable for the process number of each job. If you write $(Process) in a submit file, HTCondor will replace it with the process number of the job, independently for each job that is queued. For example, you can use the $(Process) variable to define a separate output file name for each job. Suppose the following two lines are in a submit file: output = my-output-file-$(Process).out queue 10 Even though the output filename is defined only once, HTCondor will create separate output filenames for each job: First job my-output-file-0.out Second job my-output-file-1.out Third job my-output-file-2.out ... Last (tenth) job my-output-file-9.out Let\u2019s see how this works for our program that estimates \u03c0. In your submit file, change the definitions of output and error to use $(Process) , in a way that is similar to the example above Remove any output, error, and log files from previous runs Submit the updated file When all three jobs are finished, examine the resulting files again. How many files are there of each type? What are their names? Is this what you expected? Is it what you wanted from the \u03c0 estimation process? Using $(Cluster) to Separate Files Across Runs \u00b6 With $(Process) , you can get separate output (and error) filenames for each job within a run. However, the next time you submit the same file, all of the output and error files are overwritten by new ones created by the new jobs. Maybe this is the behavior that you want. But sometimes, you may want to separate files by run, as well. In addition to $(Process) , there is also a $(Cluster) variable that you can use in your submit files. It works just like $(Process) , except it is replaced with the cluster number of the entire submission. Because the cluster number is the same for all jobs within a single submission, it does not separate files by job within a submission. But when used with $(Process) , it can be used to separate files by run. For example, consider this output statement: output = my-output-file-$(Cluster)-$(Process).out For one particular run, it might result in output filenames like this: First job my-output-file-2444-0.out Second job my-output-file-2444-1.out Third job my-output-file-2444-2.out ... If you like, change your submit file from the previous exercise to use both $(Cluster) and $(Process) . Submit your file twice to see the separate files for each run. Be careful how many jobs you run total, as the number of output files grows quickly! Using $(Process) and $(Cluster) in Other Statements \u00b6 The $(Cluster) and $(Process) variables can be used in any submit file statement, although they are useful in some kinds of statements more than others. For instance, it is hard to imagine a truly good reason to use the $(Process) variable in a rank statement (i.e., for preferring some execute slots over others), and in general the $(Cluster) variable often makes little sense to use. But in some situations, the $(Process) variable can be very helpful. Common uses are in the following kinds of statements \u2014 can you think of a scenario in which each use might be helpful? log transfer_input_files transfer_output_files arguments Unfortunately, HTCondor does not let you perform math on the $(Process) number when using it. So, for example, if you use $(Process) as a numeric argument to a command, it will always result in jobs getting the arguments 0, 1, 2, and so on. If you have control over your program and the way in which it uses command-line arguments, then you are fine. Otherwise, you might need to transform the $(Process) numbers into something more appropriate using a wrapper script , which will be discussed on Wednesday. (Optional) Defining JobBatchName for Tracking \u00b6 During the lecture, it was mentioned that you can define arbitrary attributes in your submit file, and that one purpose of such attributes is to track or report on different jobs separately. In this optional exercise, you will see how this technique can be used. Once again, we will use sleep jobs, so that your jobs remain in the queue long enough to experiment on. Create a basic submit file that runs sleep 120 (or some reasonable duration). Instead of a single queue statement, write this:\\ jobbatchname = 1 queue 5 \\ The highlighted statements give the extra attribute jobbatchname to your jobs; the first 5 jobs have one value, and the second 5 have another. Submit the file. Now, quickly edit the submit file to instead say: jobbatchname = 2 Submit the file again. Check on the submissions using a normal condor_q and condor_q -nobatch . Of course, your special attribute does not appear in the condor_q -nobatch output, but it is present in the condor_q output and in each job\u2019s ClassAd. You can see the effect of the attribute by limiting your condor_q output to one type of job or another. First, run this command: %UCL_PROMPT_SHORT% <strong>condor_q -constraint 'JobBatchName == \"1\"'</strong> Do you get the output that you expected? Using the example command above, how would you list your other five jobs?","title":"2.4. queue N"},{"location":"materials/day1/part2-ex4-queue-n/#monday-exercise-24-use-queue-n-cluster-and-process","text":"The goal of this exercise is to learn to submit many jobs from a single queue statement, and then to control filenames and arguments per job.","title":"Monday Exercise 2.4: Use queue N, $(Cluster), and $(Process)"},{"location":"materials/day1/part2-ex4-queue-n/#submitting-many-jobs-with-one-submit-file","text":"Suppose you have a program that you want to run many times. The program takes an argument, and you want to change the argument for each run of the program. With what you know so far, you have a couple of choices (assuming that you cannot change the job itself to work this way): Write one submit file; submit one job, change the argument in the submit file, submit another job, change the submit file, \u2026 Write many submit files that are nearly identical except for the program argument Neither of these options seems very satisfying. Fortunately, we can do better with HTCondor.","title":"Submitting Many Jobs With One Submit File"},{"location":"materials/day1/part2-ex4-queue-n/#running-many-jobs-with-one-queue-statement","text":"Here is a C program that uses a simple stochastic (random) method to estimate the value of \u03c0 \u2014 feel free to try to figure out the method from the code, but it is not critical for this exercise. The single argument to the program is the number of samples to take. More samples should result in better estimates! #include <stdio.h> #include <stdlib.h> #include <sys/time.h> int main(int argc, char *argv[]) { struct timeval my_timeval; int iterations = 0; int inside_circle = 0; int i; double x, y, pi_estimate; gettimeofday(&my_timeval, NULL); srand48(my_timeval.tv_sec ^ my_timeval.tv_usec); if (argc == 2) { iterations = atoi(argv[1]); } else { printf(&quot;usage: circlepi ITERATIONS\\n&quot;); exit(1); } for (i = 0; i < iterations; i++) { x = (drand48() - 0.5) * 2.0; y = (drand48() - 0.5) * 2.0; if (((x * x) + (y * y)) <= 1.0) { inside_circle++; } } pi_estimate = 4.0 * ((double) inside_circle / (double) iterations); printf(&quot;%d iterations, %d inside; pi = %f\\n&quot;, iterations, inside_circle, pi_estimate); return 0; } In a new directory for this exercise, save the code to a file named circlepi.c Compile the code (we will cover this in more detail Wednesday):\\ gcc -static -o circlepi circlepi.c If there are errors, check the file contents and compile command carefully, otherwise see the instructors Test the program with just a few samples:\\ ./circlepi 10000 Now suppose that you want to run the program many times, to produce many estimates. This is exactly what a statement like queue 3 is useful for. Let\u2019s see how it works. Write a normal submit file for this program Pass 1 billion ( 1000000000 ) as the command line argument to circlepi Remember to use queue 3 instead of just queue Submit the file\\ Note the slightly different message from condor_submit : \\ 3 job(s) submitted to cluster NNNN . Before the jobs execute, look at the job queue to see the multiple jobs Here is some sample condor_q -nobatch output: ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 10228.0 cat 7/25 11:57 0+00:00:00 I 0 0.7 circlepi 1000000000 10228.1 cat 7/25 11:57 0+00:00:00 I 0 0.7 circlepi 1000000000 10228.2 cat 7/25 11:57 0+00:00:00 I 0 0.7 circlepi 1000000000 In this sample, all three jobs are part of cluster 10228 , but the first job was assigned process 0 , the second job was assigned process 1 , and the third one was assigned process 2 . (Historical note: Programmers like to start counting from 0, hence the odd numbering scheme.) At this time, it is worth reviewing the definition of a job ID . It is a job\u2019s cluster number, a dot ( . ), and the job\u2019s process number. So in the example above, the job ID of the second job is 10228.1 . Pop Quiz: Do you remember how to ask HTCondor to list all of the jobs from one cluster? How about one specific job ID?","title":"Running Many Jobs With One queue Statement"},{"location":"materials/day1/part2-ex4-queue-n/#using-queue-n-with-output","text":"When all three jobs in your single cluster are finished, examine the resulting files. What is in the output file? What is in the error file (hopefully nothing)? What is in the log file? Look carefully at the job IDs in each event. Is this what you expected? Is it what you wanted?","title":"Using queue N With Output"},{"location":"materials/day1/part2-ex4-queue-n/#using-process-to-distinguish-jobs","text":"As you saw with the experiment above, we need a way to separate output (and error) files per job that is queued , not just for the whole cluster of jobs. Fortunately, HTCondor has a way to separate the files easily. When processing a submit file, HTCondor defines and uses a special variable for the process number of each job. If you write $(Process) in a submit file, HTCondor will replace it with the process number of the job, independently for each job that is queued. For example, you can use the $(Process) variable to define a separate output file name for each job. Suppose the following two lines are in a submit file: output = my-output-file-$(Process).out queue 10 Even though the output filename is defined only once, HTCondor will create separate output filenames for each job: First job my-output-file-0.out Second job my-output-file-1.out Third job my-output-file-2.out ... Last (tenth) job my-output-file-9.out Let\u2019s see how this works for our program that estimates \u03c0. In your submit file, change the definitions of output and error to use $(Process) , in a way that is similar to the example above Remove any output, error, and log files from previous runs Submit the updated file When all three jobs are finished, examine the resulting files again. How many files are there of each type? What are their names? Is this what you expected? Is it what you wanted from the \u03c0 estimation process?","title":"Using $(Process) to Distinguish Jobs"},{"location":"materials/day1/part2-ex4-queue-n/#using-cluster-to-separate-files-across-runs","text":"With $(Process) , you can get separate output (and error) filenames for each job within a run. However, the next time you submit the same file, all of the output and error files are overwritten by new ones created by the new jobs. Maybe this is the behavior that you want. But sometimes, you may want to separate files by run, as well. In addition to $(Process) , there is also a $(Cluster) variable that you can use in your submit files. It works just like $(Process) , except it is replaced with the cluster number of the entire submission. Because the cluster number is the same for all jobs within a single submission, it does not separate files by job within a submission. But when used with $(Process) , it can be used to separate files by run. For example, consider this output statement: output = my-output-file-$(Cluster)-$(Process).out For one particular run, it might result in output filenames like this: First job my-output-file-2444-0.out Second job my-output-file-2444-1.out Third job my-output-file-2444-2.out ... If you like, change your submit file from the previous exercise to use both $(Cluster) and $(Process) . Submit your file twice to see the separate files for each run. Be careful how many jobs you run total, as the number of output files grows quickly!","title":"Using $(Cluster) to Separate Files Across Runs"},{"location":"materials/day1/part2-ex4-queue-n/#using-process-and-cluster-in-other-statements","text":"The $(Cluster) and $(Process) variables can be used in any submit file statement, although they are useful in some kinds of statements more than others. For instance, it is hard to imagine a truly good reason to use the $(Process) variable in a rank statement (i.e., for preferring some execute slots over others), and in general the $(Cluster) variable often makes little sense to use. But in some situations, the $(Process) variable can be very helpful. Common uses are in the following kinds of statements \u2014 can you think of a scenario in which each use might be helpful? log transfer_input_files transfer_output_files arguments Unfortunately, HTCondor does not let you perform math on the $(Process) number when using it. So, for example, if you use $(Process) as a numeric argument to a command, it will always result in jobs getting the arguments 0, 1, 2, and so on. If you have control over your program and the way in which it uses command-line arguments, then you are fine. Otherwise, you might need to transform the $(Process) numbers into something more appropriate using a wrapper script , which will be discussed on Wednesday.","title":"Using $(Process) and $(Cluster) in Other Statements"},{"location":"materials/day1/part2-ex4-queue-n/#optional-defining-jobbatchname-for-tracking","text":"During the lecture, it was mentioned that you can define arbitrary attributes in your submit file, and that one purpose of such attributes is to track or report on different jobs separately. In this optional exercise, you will see how this technique can be used. Once again, we will use sleep jobs, so that your jobs remain in the queue long enough to experiment on. Create a basic submit file that runs sleep 120 (or some reasonable duration). Instead of a single queue statement, write this:\\ jobbatchname = 1 queue 5 \\ The highlighted statements give the extra attribute jobbatchname to your jobs; the first 5 jobs have one value, and the second 5 have another. Submit the file. Now, quickly edit the submit file to instead say: jobbatchname = 2 Submit the file again. Check on the submissions using a normal condor_q and condor_q -nobatch . Of course, your special attribute does not appear in the condor_q -nobatch output, but it is present in the condor_q output and in each job\u2019s ClassAd. You can see the effect of the attribute by limiting your condor_q output to one type of job or another. First, run this command: %UCL_PROMPT_SHORT% <strong>condor_q -constraint 'JobBatchName == \"1\"'</strong> Do you get the output that you expected? Using the example command above, how would you list your other five jobs?","title":"(Optional) Defining JobBatchName for Tracking"},{"location":"materials/day1/part2-ex5-queue-matching/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Monday Exercise 2.5: Submit With \u201cqueue matching\u201d \u00b6 In this exercise and the next one, you will explore more ways to use a single submit file to submit many jobs. The focus of this exercise is to submit one job per filename that matches a given pattern. In all cases of submitting many jobs from a single submit file, the key questions are: What makes each job unique? In other words, there is one job per ____ _ ? What tells HTCondor how to distinguish each job? For queue <i>N</i> , jobs are distinguished simply by number and HTCondor assigns the numbers itself. But with the remaining queue forms, you help HTCondor distinguish jobs by other, more meaningful means. Counting Words in Files \u00b6 Suppose you have a collection of books, and you want to analyze how words vary from book to book or author to author. As mentioned in the lecture, HTCondor provides many ways to do this task. You could create a separate submit file for each book, and submit all of the files manually. Or you could be a bit more clever and create one submit file for all of the books: (DON'T ACTUALLY CREATE THIS SUBMIT FILE) universe = vanilla executable = freq.py request_memory = 20MB request_disk = 20MB should_transfer_files = YES when_to_transfer_output = ON_EXIT transfer_input_files = AAiW.txt arguments = &quot;AAiW.txt&quot; output = AAiW.out error = AAiW.err log = AAiW.log queue transfer_input_files = PandP.txt arguments = &quot;PandP.txt&quot; output = PandP.out error = PandP.err log = PandP.log queue transfer_input_files = TAoSH.txt arguments = &quot;TAoSH.txt&quot; output = TAoSH.out error = TAoSH.err log = TAoSH.log queue If you use many queue statements in one submit file, HTCondor remembers and carries over values for attributes, such as universe , executable , request_memory , and so on in this example. But even then, this approach results in a long, repetitive submit file. And if you add more books, you must add five more lines to the submit file for each book. Therefore, this is not a recommended approach to submitting many jobs with one submit file. Fortunately, HTCondor has many queue features to make this kind of job submission process easy! Queue Jobs By Matching Filenames \u00b6 For our analysis, we will have a new version of the word-frequency counting script. It takes a single command-line argument, which is the name of the input file containing the text of a book, and it outputs the frequency of each word from least to most common. There will be several book files, and the filename for each book ends with .txt . This is an example of a common scenario: We want to run one job per file, where the filenames match a certain consistent pattern. The queue ... matching statement is made for this scenario. Let\u2019s see this in action. First, here is the new version of the script: #!/usr/bin/env python import os import sys import operator if len(sys.argv) != 2: print 'Usage: %s DATA' % (os.path.basename(sys.argv[0])) sys.exit(1) input_filename = sys.argv[1] words = {} my_file = open(input_filename, 'r') for line in my_file: line_words = line.split() for word in line_words: if word in words: words[word] += 1 else: words[word] = 1 my_file.close() sorted_words = sorted(words.items(), key=operator.itemgetter(1)) for word in sorted_words: print '%s %8d' % (word[0], word[1]) To use the script: Save it as wordcount.py Download and unpack some books from Project Gutenberg:\\ wget http://proxy.chtc.wisc.edu/SQUID/osgschool17/books.zip unzip books.zip Verify the script by running it on one book manually Create a submit file to submit one file (pick one), including memory and disk requests of 20 MB; submit it, if you like Modify the following submit file statements to work for all books:\\ transfer_input_files = $(BOOK) arguments = $(book) output = $(book).out error = $(book).err queue book matching *.txt \\ Note, as always, the order of statements in a submit file does not matter, except that the queue statement should be last. Also note that any submit file variable name (here, book , but true for process and all others) may be used in any mixture of upper- and lowercase letters. Submit the jobs HTCondor uses the queue ... matching statement to look for files in the submit directory that match the given pattern, then queues one job per match. For each job, the given variable (e.g., book here) is assigned the name of the matching file, so that it can be used in output , error , and other statements. The result is the same as if we had written out a much longer submit file: ... transfer_input_files = AAiW.txt arguments = &quot;AAiW.txt&quot; output = AAiW.txt.out error = AAiW.txt.err queue transfer_input_files = PandP.txt arguments = &quot;PandP.txt&quot; output = PandP.txt.out error = PandP.txt.err queue transfer_input_files = TAoSH.txt arguments = &quot;TAoSH.txt&quot; output = TAoSH.txt.out error = TAoSH.txt.err queue Here is some sample condor_q -nobatch output: ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 89.0 iaross 7/17 11:41 0+00:00:00 I 0 0.0 wordcount.py AAiW.txt 89.1 iaross 7/17 11:41 0+00:00:00 I 0 0.0 wordcount.py PandP.txt 89.2 iaross 7/17 11:41 0+00:00:00 I 0 0.0 wordcount.py TAoSH.txt All three jobs were part of cluster 89. The first filename that was matched in the queue statement resulted in a process ID of 0, the second match has a process ID of 1, and the third has a process ID of 2. When the three jobs finish, carefully look at the resulting files. Do they match your expectations? There should be a single log file, but three separate output files and three separate (and hopefully empty) error files, one for each job. Extra Challenge 1 \u00b6 In the example above, you used a single log file for all three jobs. HTCondor handles this situation with no problem; each job writes its events into the log file without getting in the way of other events and other jobs. But as you may have seen, it may be difficult for a person to understand the events for any particular job in the combined log file. Create a new submit file that works just like the one above, except that each job writes its own log file.","title":"2.5. queue matching"},{"location":"materials/day1/part2-ex5-queue-matching/#monday-exercise-25-submit-with-queue-matching","text":"In this exercise and the next one, you will explore more ways to use a single submit file to submit many jobs. The focus of this exercise is to submit one job per filename that matches a given pattern. In all cases of submitting many jobs from a single submit file, the key questions are: What makes each job unique? In other words, there is one job per ____ _ ? What tells HTCondor how to distinguish each job? For queue <i>N</i> , jobs are distinguished simply by number and HTCondor assigns the numbers itself. But with the remaining queue forms, you help HTCondor distinguish jobs by other, more meaningful means.","title":"Monday Exercise 2.5: Submit With \u201cqueue matching\u201d"},{"location":"materials/day1/part2-ex5-queue-matching/#counting-words-in-files","text":"Suppose you have a collection of books, and you want to analyze how words vary from book to book or author to author. As mentioned in the lecture, HTCondor provides many ways to do this task. You could create a separate submit file for each book, and submit all of the files manually. Or you could be a bit more clever and create one submit file for all of the books: (DON'T ACTUALLY CREATE THIS SUBMIT FILE) universe = vanilla executable = freq.py request_memory = 20MB request_disk = 20MB should_transfer_files = YES when_to_transfer_output = ON_EXIT transfer_input_files = AAiW.txt arguments = &quot;AAiW.txt&quot; output = AAiW.out error = AAiW.err log = AAiW.log queue transfer_input_files = PandP.txt arguments = &quot;PandP.txt&quot; output = PandP.out error = PandP.err log = PandP.log queue transfer_input_files = TAoSH.txt arguments = &quot;TAoSH.txt&quot; output = TAoSH.out error = TAoSH.err log = TAoSH.log queue If you use many queue statements in one submit file, HTCondor remembers and carries over values for attributes, such as universe , executable , request_memory , and so on in this example. But even then, this approach results in a long, repetitive submit file. And if you add more books, you must add five more lines to the submit file for each book. Therefore, this is not a recommended approach to submitting many jobs with one submit file. Fortunately, HTCondor has many queue features to make this kind of job submission process easy!","title":"Counting Words in Files"},{"location":"materials/day1/part2-ex5-queue-matching/#queue-jobs-by-matching-filenames","text":"For our analysis, we will have a new version of the word-frequency counting script. It takes a single command-line argument, which is the name of the input file containing the text of a book, and it outputs the frequency of each word from least to most common. There will be several book files, and the filename for each book ends with .txt . This is an example of a common scenario: We want to run one job per file, where the filenames match a certain consistent pattern. The queue ... matching statement is made for this scenario. Let\u2019s see this in action. First, here is the new version of the script: #!/usr/bin/env python import os import sys import operator if len(sys.argv) != 2: print 'Usage: %s DATA' % (os.path.basename(sys.argv[0])) sys.exit(1) input_filename = sys.argv[1] words = {} my_file = open(input_filename, 'r') for line in my_file: line_words = line.split() for word in line_words: if word in words: words[word] += 1 else: words[word] = 1 my_file.close() sorted_words = sorted(words.items(), key=operator.itemgetter(1)) for word in sorted_words: print '%s %8d' % (word[0], word[1]) To use the script: Save it as wordcount.py Download and unpack some books from Project Gutenberg:\\ wget http://proxy.chtc.wisc.edu/SQUID/osgschool17/books.zip unzip books.zip Verify the script by running it on one book manually Create a submit file to submit one file (pick one), including memory and disk requests of 20 MB; submit it, if you like Modify the following submit file statements to work for all books:\\ transfer_input_files = $(BOOK) arguments = $(book) output = $(book).out error = $(book).err queue book matching *.txt \\ Note, as always, the order of statements in a submit file does not matter, except that the queue statement should be last. Also note that any submit file variable name (here, book , but true for process and all others) may be used in any mixture of upper- and lowercase letters. Submit the jobs HTCondor uses the queue ... matching statement to look for files in the submit directory that match the given pattern, then queues one job per match. For each job, the given variable (e.g., book here) is assigned the name of the matching file, so that it can be used in output , error , and other statements. The result is the same as if we had written out a much longer submit file: ... transfer_input_files = AAiW.txt arguments = &quot;AAiW.txt&quot; output = AAiW.txt.out error = AAiW.txt.err queue transfer_input_files = PandP.txt arguments = &quot;PandP.txt&quot; output = PandP.txt.out error = PandP.txt.err queue transfer_input_files = TAoSH.txt arguments = &quot;TAoSH.txt&quot; output = TAoSH.txt.out error = TAoSH.txt.err queue Here is some sample condor_q -nobatch output: ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 89.0 iaross 7/17 11:41 0+00:00:00 I 0 0.0 wordcount.py AAiW.txt 89.1 iaross 7/17 11:41 0+00:00:00 I 0 0.0 wordcount.py PandP.txt 89.2 iaross 7/17 11:41 0+00:00:00 I 0 0.0 wordcount.py TAoSH.txt All three jobs were part of cluster 89. The first filename that was matched in the queue statement resulted in a process ID of 0, the second match has a process ID of 1, and the third has a process ID of 2. When the three jobs finish, carefully look at the resulting files. Do they match your expectations? There should be a single log file, but three separate output files and three separate (and hopefully empty) error files, one for each job.","title":"Queue Jobs By Matching Filenames"},{"location":"materials/day1/part2-ex5-queue-matching/#extra-challenge-1","text":"In the example above, you used a single log file for all three jobs. HTCondor handles this situation with no problem; each job writes its events into the log file without getting in the way of other events and other jobs. But as you may have seen, it may be difficult for a person to understand the events for any particular job in the combined log file. Create a new submit file that works just like the one above, except that each job writes its own log file.","title":"Extra Challenge 1"},{"location":"materials/day1/part2-ex6-queue-from/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Monday Exercise 2.6: Submit With \u201cqueue from\u201d \u00b6 The goal of this exercise is to submit many jobs from a single submit file by using the queue ... from syntax to read variable values from a file. More Job Submission Alternatives \u00b6 In the previous exercise, we use the queue ... matching syntax, which is primarily useful for collections of files with similar names. But that method has its weaknesses, too. It is less useful when a collection of files does not have a consistent and unique pattern for its names, or when the values for different job conditions are not filenames, or both. Queue Jobs From File Contents \u00b6 Suppose we want to modify our word-frequency analysis a little bit so that it outputs only the most common N words of a document. Further, we want to experiment with different values of N . And finally, even though we downloaded it, we no longer want to analyze the Sherlock Holmes stories in the TAoSH.txt file. Clearly, queue ... matching will not help us in this case. First, we need a new version of the word counting program so that in accepts an extra number as a command line argument and outputs only that many of the most common words. Here is the new code: #!/usr/bin/env python import os import sys import operator if len(sys.argv) != 3: print 'Usage: %s DATA NUM_WORDS' % (os.path.basename(sys.argv[0])) sys.exit(1) input_filename = sys.argv[1] num_words = int(sys.argv[2]) words = {} my_file = open(input_filename, 'r') for line in my_file: line_words = line.split() for word in line_words: if word in words: words[word] += 1 else: words[word] = 1 my_file.close() sorted_words = sorted(words.items(), key=operator.itemgetter(1)) for word in sorted_words[-num_words:]: print '%s %8d' % (word[0], word[1]) To submit this program with a collection of two variables values for each run: In the same directory as the last exercise, save the script as wordcount-top-n.py Copy your submit file from the last exercise to a new name (maybe wordcount-top.sub ) Update the executable and log statements as appropriate Update other statements to work with two variables, book and n :\\ output = $(book)_top_$(n).out error = $(book)_top_$(n).err transfer_input_files = $(book) arguments = \"$(book) $(n)\" queue book, n from books_n.txt \\ Note especially the changes to the queue statement; it now tells HTCondor to read a separate text file of pairs of values, which will be assigned to book and n respectively. Create the separate text file of job variable values and save it as books_n.txt : AAiW.txt, 10 AAiW.txt, 25 AAiW.txt, 50 PandP.txt, 10 PandP.txt, 25 PandP.txt, 50 \\ Note that we used 3 different values for N for each book, and that we dropped the Sherlock Holmes book, TAoSH.txt . Submit the file Do a quick sanity check: How many jobs were submitted? How many log, output, and error files were created? Extra Challenge 1 \u00b6 Between this exercise and the previous one, you have explored two of the three primary queue statements. How would you use the queue in ... list statement to accomplish the same thing(s) as one or both of the exercises? Extra Challenge 2 \u00b6 You may have noticed that the output of these jobs has a messy naming convention. Because our macros resolve to the filenames, including their extension (e.g., AAiW.txt ), the output filenames contain with multiple extensions (e.g., AAiW.txt.err ). Although the extra extension is acceptable, it makes the filenames harder to read and possibly organize. Change your submit file for this exercise so that the output filenames do not include the .txt extension. Extra Challenge 3 \u00b6 Adjust your queue...matching submit file so that those results also don't include the .txt extension. ( Hint : You can use function macros to format variables. Check the HTCondor Manual .)","title":"2.6. queue from"},{"location":"materials/day1/part2-ex6-queue-from/#monday-exercise-26-submit-with-queue-from","text":"The goal of this exercise is to submit many jobs from a single submit file by using the queue ... from syntax to read variable values from a file.","title":"Monday Exercise 2.6: Submit With \u201cqueue from\u201d"},{"location":"materials/day1/part2-ex6-queue-from/#more-job-submission-alternatives","text":"In the previous exercise, we use the queue ... matching syntax, which is primarily useful for collections of files with similar names. But that method has its weaknesses, too. It is less useful when a collection of files does not have a consistent and unique pattern for its names, or when the values for different job conditions are not filenames, or both.","title":"More Job Submission Alternatives"},{"location":"materials/day1/part2-ex6-queue-from/#queue-jobs-from-file-contents","text":"Suppose we want to modify our word-frequency analysis a little bit so that it outputs only the most common N words of a document. Further, we want to experiment with different values of N . And finally, even though we downloaded it, we no longer want to analyze the Sherlock Holmes stories in the TAoSH.txt file. Clearly, queue ... matching will not help us in this case. First, we need a new version of the word counting program so that in accepts an extra number as a command line argument and outputs only that many of the most common words. Here is the new code: #!/usr/bin/env python import os import sys import operator if len(sys.argv) != 3: print 'Usage: %s DATA NUM_WORDS' % (os.path.basename(sys.argv[0])) sys.exit(1) input_filename = sys.argv[1] num_words = int(sys.argv[2]) words = {} my_file = open(input_filename, 'r') for line in my_file: line_words = line.split() for word in line_words: if word in words: words[word] += 1 else: words[word] = 1 my_file.close() sorted_words = sorted(words.items(), key=operator.itemgetter(1)) for word in sorted_words[-num_words:]: print '%s %8d' % (word[0], word[1]) To submit this program with a collection of two variables values for each run: In the same directory as the last exercise, save the script as wordcount-top-n.py Copy your submit file from the last exercise to a new name (maybe wordcount-top.sub ) Update the executable and log statements as appropriate Update other statements to work with two variables, book and n :\\ output = $(book)_top_$(n).out error = $(book)_top_$(n).err transfer_input_files = $(book) arguments = \"$(book) $(n)\" queue book, n from books_n.txt \\ Note especially the changes to the queue statement; it now tells HTCondor to read a separate text file of pairs of values, which will be assigned to book and n respectively. Create the separate text file of job variable values and save it as books_n.txt : AAiW.txt, 10 AAiW.txt, 25 AAiW.txt, 50 PandP.txt, 10 PandP.txt, 25 PandP.txt, 50 \\ Note that we used 3 different values for N for each book, and that we dropped the Sherlock Holmes book, TAoSH.txt . Submit the file Do a quick sanity check: How many jobs were submitted? How many log, output, and error files were created?","title":"Queue Jobs From File Contents"},{"location":"materials/day1/part2-ex6-queue-from/#extra-challenge-1","text":"Between this exercise and the previous one, you have explored two of the three primary queue statements. How would you use the queue in ... list statement to accomplish the same thing(s) as one or both of the exercises?","title":"Extra Challenge 1"},{"location":"materials/day1/part2-ex6-queue-from/#extra-challenge-2","text":"You may have noticed that the output of these jobs has a messy naming convention. Because our macros resolve to the filenames, including their extension (e.g., AAiW.txt ), the output filenames contain with multiple extensions (e.g., AAiW.txt.err ). Although the extra extension is acceptable, it makes the filenames harder to read and possibly organize. Change your submit file for this exercise so that the output filenames do not include the .txt extension.","title":"Extra Challenge 2"},{"location":"materials/day1/part2-ex6-queue-from/#extra-challenge-3","text":"Adjust your queue...matching submit file so that those results also don't include the .txt extension. ( Hint : You can use function macros to format variables. Check the HTCondor Manual .)","title":"Extra Challenge 3"},{"location":"materials/day1/part3-ex1-job-retry/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Monday Exercise 3.1: Retries \u00b6 The goal of this exercise is to demonstrate running a job that intermittently fails and thus could benefit from having HTCondor automatically retry it. This first part of the exercise should take only a few minutes, and is designed to setup the next exercises. Bad Job \u00b6 Let\u2019s assume that a colleague has shared with you a program, and it fails once in a while. In the real world, we would probably just fix the program, but what if you cannot change the software? Unfortunately, this situation happens more often than we would like. Below is a simple Python script that fails once in a while. We will not fix it, but use it to simulate a program that can fail and that we cannot fix. #!/usr/bin/env python # murphy.py simulates a real program with real problems import random import sys import time # Create a random number seeded by system entropy r = random.SystemRandom() # One time in three, simulate a runtime error if (r.randint(0,2) == 0): # intentionally print no output sys.exit(15) else: time.sleep(3) print \"All work done correctly\" # By convention, zero exit code means success sys.exit(0) Even if you are not a Python expert, you may be able to figure out what this program does. Let\u2019s see what happens when a program like this one is run in HTCondor. In a new directory for this exercise, save the script above as murphy.py Write a submit file for the script; queue 20 instances of the job and be sure to ask for 20 MB of memory and disk Submit the file and wait for the jobs to finish What output do you expect? What output did you get? If you are curious about the exit code from the job, it is saved in completed jobs in condor_history in the ExitCode attribute. The following command will show the ExitCode for a given cluster of jobs: %UCL_PROMPT_SHORT% <strong>condor_history <em>CLUSTERID</em> -af ProcId ExitCode</strong> (Be sure to replace CLUSTERID with your actual cluster ID) How many of the jobs succeeded? How many failed? Retrying Failed Jobs \u00b6 Now let\u2019s see if we can solve the problem of jobs that fail once in a while. In this particular case, if HTCondor runs a failed job again, it has a good chance of succeeding. Not all failing jobs are like this, but in this case it is a reasonable assumption. From the lecture materials, implement the max_retries feature to retry any job with a non-zero exit code up to 5 times, then resubmit the jobs. Did your change work? After the jobs have finished, examine the log file(s) to see what happened in detail. Did any jobs need to be restarted? Another way to see how many restarts there were is to look at the NumJobStarts attribute of a completed job with the condor_history command, in the same way you looked at the ExitCode attribute earlier. Does the number of retries seem correct? For those jobs which did need to be retried, what is their ExitCode=; and what about the =ExitCode from earlier execution attempts? A (Too) Long Running Job \u00b6 Sometimes, an ill-behaved job will get stuck in a loop and run forever, instead of exiting with a failure code. We can modify our Python program to simulate this kind of bad job with the following file: #!/usr/bin/env python # murphy.py simulate a real program with real problems import random import sys import time # Create a random number seeded by system entropy r = random.SystemRandom() # One time in three, simulate an infinite loop if (r.randint(0,2) == 0): # intentionally print no output time.sleep(3600) sys.exit(15) else: time.sleep(3) print \"All work done correctly\" # By convention, zero exit code means success sys.exit(0) Again, you may be able to figure out what this new program does. Save the script to a new file named murphy2.py Copy your previous submit file to a new name and change the executable to murphy2.py If you like, submit the new file \u2014 but after a while be sure to remove the whole cluster to clear out the \u201chung\u201d jobs. Now try to change the submit file to automatically remove any jobs that run for more than one minute. You can make this change with just a single line in your submit file\\ periodic_remove = (JobStatus == 2) && (CurrentTime - EnteredCurrentStatus) > 60 Submit the new file. Do the long running jobs get removed? What does condor_history show for the cluster after all jobs are done? Bonus Exercise \u00b6 If you have time, edit your submit file so that instead of removing long running jobs, have HTCondor automatically put the long-running job on hold, and then automatically release it.","title":"3.1. Retries"},{"location":"materials/day1/part3-ex1-job-retry/#monday-exercise-31-retries","text":"The goal of this exercise is to demonstrate running a job that intermittently fails and thus could benefit from having HTCondor automatically retry it. This first part of the exercise should take only a few minutes, and is designed to setup the next exercises.","title":"Monday Exercise 3.1: Retries"},{"location":"materials/day1/part3-ex1-job-retry/#bad-job","text":"Let\u2019s assume that a colleague has shared with you a program, and it fails once in a while. In the real world, we would probably just fix the program, but what if you cannot change the software? Unfortunately, this situation happens more often than we would like. Below is a simple Python script that fails once in a while. We will not fix it, but use it to simulate a program that can fail and that we cannot fix. #!/usr/bin/env python # murphy.py simulates a real program with real problems import random import sys import time # Create a random number seeded by system entropy r = random.SystemRandom() # One time in three, simulate a runtime error if (r.randint(0,2) == 0): # intentionally print no output sys.exit(15) else: time.sleep(3) print \"All work done correctly\" # By convention, zero exit code means success sys.exit(0) Even if you are not a Python expert, you may be able to figure out what this program does. Let\u2019s see what happens when a program like this one is run in HTCondor. In a new directory for this exercise, save the script above as murphy.py Write a submit file for the script; queue 20 instances of the job and be sure to ask for 20 MB of memory and disk Submit the file and wait for the jobs to finish What output do you expect? What output did you get? If you are curious about the exit code from the job, it is saved in completed jobs in condor_history in the ExitCode attribute. The following command will show the ExitCode for a given cluster of jobs: %UCL_PROMPT_SHORT% <strong>condor_history <em>CLUSTERID</em> -af ProcId ExitCode</strong> (Be sure to replace CLUSTERID with your actual cluster ID) How many of the jobs succeeded? How many failed?","title":"Bad Job"},{"location":"materials/day1/part3-ex1-job-retry/#retrying-failed-jobs","text":"Now let\u2019s see if we can solve the problem of jobs that fail once in a while. In this particular case, if HTCondor runs a failed job again, it has a good chance of succeeding. Not all failing jobs are like this, but in this case it is a reasonable assumption. From the lecture materials, implement the max_retries feature to retry any job with a non-zero exit code up to 5 times, then resubmit the jobs. Did your change work? After the jobs have finished, examine the log file(s) to see what happened in detail. Did any jobs need to be restarted? Another way to see how many restarts there were is to look at the NumJobStarts attribute of a completed job with the condor_history command, in the same way you looked at the ExitCode attribute earlier. Does the number of retries seem correct? For those jobs which did need to be retried, what is their ExitCode=; and what about the =ExitCode from earlier execution attempts?","title":"Retrying Failed Jobs"},{"location":"materials/day1/part3-ex1-job-retry/#a-too-long-running-job","text":"Sometimes, an ill-behaved job will get stuck in a loop and run forever, instead of exiting with a failure code. We can modify our Python program to simulate this kind of bad job with the following file: #!/usr/bin/env python # murphy.py simulate a real program with real problems import random import sys import time # Create a random number seeded by system entropy r = random.SystemRandom() # One time in three, simulate an infinite loop if (r.randint(0,2) == 0): # intentionally print no output time.sleep(3600) sys.exit(15) else: time.sleep(3) print \"All work done correctly\" # By convention, zero exit code means success sys.exit(0) Again, you may be able to figure out what this new program does. Save the script to a new file named murphy2.py Copy your previous submit file to a new name and change the executable to murphy2.py If you like, submit the new file \u2014 but after a while be sure to remove the whole cluster to clear out the \u201chung\u201d jobs. Now try to change the submit file to automatically remove any jobs that run for more than one minute. You can make this change with just a single line in your submit file\\ periodic_remove = (JobStatus == 2) && (CurrentTime - EnteredCurrentStatus) > 60 Submit the new file. Do the long running jobs get removed? What does condor_history show for the cluster after all jobs are done?","title":"A (Too) Long Running Job"},{"location":"materials/day1/part3-ex1-job-retry/#bonus-exercise","text":"If you have time, edit your submit file so that instead of removing long running jobs, have HTCondor automatically put the long-running job on hold, and then automatically release it.","title":"Bonus Exercise"},{"location":"materials/day1/part3-ex2-mandelbrot/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Monday Exercise 3.2: A Brief Detour Through the Mandelbrot Set \u00b6 Before we explore using DAGs to implement workflows, let\u2019s get a more interesting job. Let\u2019s make pretty pictures! We have a small program that draws pictures of the Mandelbrot set. You can read about the Mandelbrot set on Wikipedia , or you can simply appreciate the pretty pictures. It\u2019s a fractal. We have a simple program that can draw the Mandelbrot set. It's called goatbrot , and you can find it in /usr/local/bin/goatbrot . Running goatbrot From the Command Line \u00b6 You can generate the Mandelbrot set with two simple commands. Generate a PPM image of the Mandelbrot set:\\ /usr/local/bin/goatbrot -i 1000 -o tile_000000_000000.ppm -c 0,0 -w 3 -s 1000,1000 Convert the image to the JPEG format:\\ convert tile_000000_000000.ppm mandel.jpg The goatbroat program takes several parameters. Let's break them down: -i 1000 The number of iterations. Bigger numbers generate more accurate images but are slower to run. -o tile_000000_000000.ppm The output file to generate. -c 0,0 The center point of the image. Here it is the point (0,0). -w 3 The width of the image. Here is 3. -s 1000,1000 The size of the final image. Here we generate a picture that is 1000 pixels wide and 1000 pixels tall. Dividing the Work into Smaller Pieces \u00b6 The Mandelbrot set can take a while to create, particularly if you make the iterations large or the image size large. What if we broke the creation of the image into multiple invocations then stitched them together? Once we do that, we can run the each goatbroat in parallel in our cluster. Here's an example you can run by hand. Run goatbroat 4 times:\\ /usr/local/bin/goatbrot -i 1000 -o tile_000000_000000.ppm -c -0.75,0.75 -w 1.5 -s 500,500 /usr/local/bin/goatbrot -i 1000 -o tile_000000_000001.ppm -c 0.75,0.75 -w 1.5 -s 500,500 /usr/local/bin/goatbrot -i 1000 -o tile_000001_000000.ppm -c -0.75,-0.75 -w 1.5 -s 500,500 /usr/local/bin/goatbrot -i 1000 -o tile_000001_000001.ppm -c 0.75,-0.75 -w 1.5 -s 500,500 Stitch them small images together into the complete image (in JPEG format):\\ montage tile_000000_000000.ppm tile_000000_000001.ppm tile_000001_000000.ppm tile_000001_000001.ppm -mode Concatenate -tile 2x2 mandel.jpg This will produce the same image as above. We divided the image space into a 2\u00d72 grid and ran goatbrot on each section of the grid. The montage program simply stitches the files together and writes out the final image in JPEG format. Try it! \u00b6 Run the commands above, make sure you can create the Mandelbrot image. When you create the image, you might wonder how you can view it. If you're comfortable with scp you can copy it back to your computer to view it. Otherwise you can view it in your web browser in three easy steps. Make your web directory (you only need to do this once):\\ cd ~ mkdir ~/public_html chmod 0711 . chmod 0755 public_html Copy the image into your web directory:\\ cp mandel.jpg ~/public_html/ Access the URL below in your web browser (change USER to your username on the submit machine):\\ http://learn.chtc.wisc.edu/ ~ USER /mandel.jpg","title":"3.2. Mandelbrot set"},{"location":"materials/day1/part3-ex2-mandelbrot/#monday-exercise-32-a-brief-detour-through-the-mandelbrot-set","text":"Before we explore using DAGs to implement workflows, let\u2019s get a more interesting job. Let\u2019s make pretty pictures! We have a small program that draws pictures of the Mandelbrot set. You can read about the Mandelbrot set on Wikipedia , or you can simply appreciate the pretty pictures. It\u2019s a fractal. We have a simple program that can draw the Mandelbrot set. It's called goatbrot , and you can find it in /usr/local/bin/goatbrot .","title":"Monday Exercise 3.2: A Brief Detour Through the Mandelbrot Set"},{"location":"materials/day1/part3-ex2-mandelbrot/#running-goatbrot-from-the-command-line","text":"You can generate the Mandelbrot set with two simple commands. Generate a PPM image of the Mandelbrot set:\\ /usr/local/bin/goatbrot -i 1000 -o tile_000000_000000.ppm -c 0,0 -w 3 -s 1000,1000 Convert the image to the JPEG format:\\ convert tile_000000_000000.ppm mandel.jpg The goatbroat program takes several parameters. Let's break them down: -i 1000 The number of iterations. Bigger numbers generate more accurate images but are slower to run. -o tile_000000_000000.ppm The output file to generate. -c 0,0 The center point of the image. Here it is the point (0,0). -w 3 The width of the image. Here is 3. -s 1000,1000 The size of the final image. Here we generate a picture that is 1000 pixels wide and 1000 pixels tall.","title":"Running goatbrot From the Command Line"},{"location":"materials/day1/part3-ex2-mandelbrot/#dividing-the-work-into-smaller-pieces","text":"The Mandelbrot set can take a while to create, particularly if you make the iterations large or the image size large. What if we broke the creation of the image into multiple invocations then stitched them together? Once we do that, we can run the each goatbroat in parallel in our cluster. Here's an example you can run by hand. Run goatbroat 4 times:\\ /usr/local/bin/goatbrot -i 1000 -o tile_000000_000000.ppm -c -0.75,0.75 -w 1.5 -s 500,500 /usr/local/bin/goatbrot -i 1000 -o tile_000000_000001.ppm -c 0.75,0.75 -w 1.5 -s 500,500 /usr/local/bin/goatbrot -i 1000 -o tile_000001_000000.ppm -c -0.75,-0.75 -w 1.5 -s 500,500 /usr/local/bin/goatbrot -i 1000 -o tile_000001_000001.ppm -c 0.75,-0.75 -w 1.5 -s 500,500 Stitch them small images together into the complete image (in JPEG format):\\ montage tile_000000_000000.ppm tile_000000_000001.ppm tile_000001_000000.ppm tile_000001_000001.ppm -mode Concatenate -tile 2x2 mandel.jpg This will produce the same image as above. We divided the image space into a 2\u00d72 grid and ran goatbrot on each section of the grid. The montage program simply stitches the files together and writes out the final image in JPEG format.","title":"Dividing the Work into Smaller Pieces"},{"location":"materials/day1/part3-ex2-mandelbrot/#try-it","text":"Run the commands above, make sure you can create the Mandelbrot image. When you create the image, you might wonder how you can view it. If you're comfortable with scp you can copy it back to your computer to view it. Otherwise you can view it in your web browser in three easy steps. Make your web directory (you only need to do this once):\\ cd ~ mkdir ~/public_html chmod 0711 . chmod 0755 public_html Copy the image into your web directory:\\ cp mandel.jpg ~/public_html/ Access the URL below in your web browser (change USER to your username on the submit machine):\\ http://learn.chtc.wisc.edu/ ~ USER /mandel.jpg","title":"Try it!"},{"location":"materials/day1/part3-ex3-simple-dag/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Monday Exercise 3.3: Coordinating a Set of Jobs With a Simple DAG \u00b6 The objective of this exercise is to learn the very basics of running a set of jobs, where our set is just one job. What is DAGMan? \u00b6 In short, DAGMan lets you submit complex sequences of jobs as long as they can be expressed as a directed acylic graph. For example, you may wish to run a large parameter sweep but before the sweep run you need to prepare your data. After the sweep runs, you need to collate the results. This might look like this, assuming you want to sweep over five parameters: DAGMan has many abilities, such as throttling jobs, recovery from failures, and more. More information about DAGMan can be found at in the Condor manual . Submitting a Simple DAG \u00b6 For our job, we will return briefly to the sleep program. executable = /bin/sleep arguments = 4 log = simple.log output = simple.out error = simple.error request_memory = 1GB request_disk = 1GB request_cpus = 1 queue We are going to get a bit more sophisticated in submitting our jobs now. Let's have three windows open. In one window, you'll submit the job. In another you will watch the queue, and in the third you will watch what DAGMan does. First we will create the most minimal DAG that can be created: a DAG with just one node. Put this into a file named simple.dag . Job Simple simple.sub In your first window, submit the DAG: %UCL_PROMPT_SHORT% <strong>condor_submit_dag simple.dag</strong> ----------------------------------------------------------------------- File for submitting this DAG to Condor : simple.dag.condor.sub Log of DAGMan debugging messages : simple.dag.dagman.out Log of Condor library output : simple.dag.lib.out Log of Condor library error messages : simple.dag.lib.err Log of the life of condor_dagman itself : simple.dag.dagman.log Submitting job(s). 1 job(s) submitted to cluster 61. ----------------------------------------------------------------------- In the second window, watch the queue: %UCL_PROMPT_SHORT% <strong>watch -n 10 condor_q -nobatch</strong> -- Submitter: learn.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : learn.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 61.0 roy 6/21 22:51 0+00:00:01 R 0 0.3 condor_dagman 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended -- Submitter: learn.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : learn.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 61.0 roy 6/21 22:51 0+00:01:25 R 0 0.3 condor_dagman 62.0 roy 6/21 22:51 0+00:00:00 I 0 0.7 simple 4 10 2 jobs; 0 completed, 0 removed, 1 idle, 1 running, 0 held, 0 suspended -- Submitter: learn.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : learn.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 61.0 roy 6/21 22:51 0+00:03:47 R 0 0.3 condor_dagman 62.0 roy 6/21 22:51 0+00:00:03 R 0 0.7 simple 4 10 2 jobs; 0 completed, 0 removed, 0 idle, 2 running, 0 held, 0 suspended -- Submitter: learn.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : learn.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 0 jobs; 0 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended <i>Ctrl-C</i> In the third window, watch what DAGMan does: %UCL_PROMPT_SHORT% <strong>tail -f --lines=500 simple.dag.dagman.out</strong> 6/21/12 22:51:13 Setting maximum accepts per cycle 8. 06/21/12 22:51:13 ****************************************************** 06/21/12 22:51:13 ** condor_scheduniv_exec.61.0 (CONDOR_DAGMAN) STARTING UP 06/21/12 22:51:13 ** /usr/bin/condor_dagman 06/21/12 22:51:13 ** SubsystemInfo: name=DAGMAN type=DAGMAN(10) class=DAEMON(1) 06/21/12 22:51:13 ** Configuration: subsystem:DAGMAN local:<NONE> class:DAEMON 06/21/12 22:51:13 ** $CondorVersion: 7.7.6 Apr 16 2012 BuildID: 34175 PRE-RELEASE-UWCS $ 06/21/12 22:51:13 ** $CondorPlatform: x86_64_rhap_5.7 $ 06/21/12 22:51:13 ** PID = 5812 06/21/12 22:51:13 ** Log last touched 6/21 22:51:00 06/21/12 22:51:13 ****************************************************** 06/21/12 22:51:13 Using config source: /etc/condor/condor_config 06/21/12 22:51:13 Using local config sources: 06/21/12 22:51:13 /etc/condor/config.d/00-chtc-global.conf 06/21/12 22:51:13 /etc/condor/config.d/01-chtc-submit.conf 06/21/12 22:51:13 /etc/condor/config.d/02-chtc-flocking.conf 06/21/12 22:51:13 /etc/condor/config.d/03-chtc-jobrouter.conf 06/21/12 22:51:13 /etc/condor/config.d/04-chtc-blacklist.conf 06/21/12 22:51:13 /etc/condor/config.d/99-osg-ss-group.conf 06/21/12 22:51:13 /etc/condor/config.d/99-roy-extras.conf 06/21/12 22:51:13 /etc/condor/condor_config.local 06/21/12 22:51:13 DaemonCore: command socket at <128.104.100.55:60417> 06/21/12 22:51:13 DaemonCore: private command socket at <128.104.100.55:60417> 06/21/12 22:51:13 Setting maximum accepts per cycle 8. 06/21/12 22:51:13 DAGMAN_USE_STRICT setting: 0 06/21/12 22:51:13 DAGMAN_VERBOSITY setting: 3 06/21/12 22:51:13 DAGMAN_DEBUG_CACHE_SIZE setting: 5242880 06/21/12 22:51:13 DAGMAN_DEBUG_CACHE_ENABLE setting: False 06/21/12 22:51:13 DAGMAN_SUBMIT_DELAY setting: 0 06/21/12 22:51:13 DAGMAN_MAX_SUBMIT_ATTEMPTS setting: 6 06/21/12 22:51:13 DAGMAN_STARTUP_CYCLE_DETECT setting: False 06/21/12 22:51:13 DAGMAN_MAX_SUBMITS_PER_INTERVAL setting: 5 06/21/12 22:51:13 DAGMAN_USER_LOG_SCAN_INTERVAL setting: 5 06/21/12 22:51:13 allow_events (DAGMAN_IGNORE_DUPLICATE_JOB_EXECUTION, DAGMAN_ALLOW_EVENTS) setting: 114 06/21/12 22:51:13 DAGMAN_RETRY_SUBMIT_FIRST setting: True 06/21/12 22:51:13 DAGMAN_RETRY_NODE_FIRST setting: False 06/21/12 22:51:13 DAGMAN_MAX_JOBS_IDLE setting: 0 06/21/12 22:51:13 DAGMAN_MAX_JOBS_SUBMITTED setting: 0 06/21/12 22:51:15 DAGMAN_MAX_PRE_SCRIPTS setting: 0 06/21/12 22:51:15 DAGMAN_MAX_POST_SCRIPTS setting: 0 06/21/12 22:51:15 DAGMAN_ALLOW_LOG_ERROR setting: False 06/21/12 22:51:15 DAGMAN_MUNGE_NODE_NAMES setting: True 06/21/12 22:51:15 DAGMAN_PROHIBIT_MULTI_JOBS setting: False 06/21/12 22:51:15 DAGMAN_SUBMIT_DEPTH_FIRST setting: False 06/21/12 22:51:15 DAGMAN_ALWAYS_RUN_POST setting: True 06/21/12 22:51:15 DAGMAN_ABORT_DUPLICATES setting: True 06/21/12 22:51:15 DAGMAN_ABORT_ON_SCARY_SUBMIT setting: True 06/21/12 22:51:15 DAGMAN_PENDING_REPORT_INTERVAL setting: 600 06/21/12 22:51:15 DAGMAN_AUTO_RESCUE setting: True 06/21/12 22:51:15 DAGMAN_MAX_RESCUE_NUM setting: 100 06/21/12 22:51:15 DAGMAN_WRITE_PARTIAL_RESCUE setting: True 06/21/12 22:51:15 DAGMAN_DEFAULT_NODE_LOG setting: null 06/21/12 22:51:15 DAGMAN_GENERATE_SUBDAG_SUBMITS setting: True 06/21/12 22:51:15 ALL_DEBUG setting: 06/21/12 22:51:15 DAGMAN_DEBUG setting: 06/21/12 22:51:15 argv[0] == \"condor_scheduniv_exec.61.0\" 06/21/12 22:51:15 argv[1] == \"-Lockfile\" 06/21/12 22:51:15 argv[2] == \"simple.dag.lock\" 06/21/12 22:51:15 argv[3] == \"-AutoRescue\" 06/21/12 22:51:15 argv[4] == \"1\" 06/21/12 22:51:15 argv[5] == \"-DoRescueFrom\" 06/21/12 22:51:15 argv[6] == \"0\" 06/21/12 22:51:15 argv[7] == \"-Dag\" 06/21/12 22:51:15 argv[8] == \"simple.dag\" 06/21/12 22:51:15 argv[9] == \"-CsdVersion\" 06/21/12 22:51:15 argv[10] == \"$CondorVersion: 7.7.6 Apr 16 2012 BuildID: 34175 PRE-RELEASE-UWCS $\" 06/21/12 22:51:15 argv[11] == \"-Force\" 06/21/12 22:51:15 argv[12] == \"-Dagman\" 06/21/12 22:51:15 argv[13] == \"/usr/bin/condor_dagman\" 06/21/12 22:51:15 Default node log file is: </home/roy/condor/simple.dag.nodes.log> 06/21/12 22:51:15 DAG Lockfile will be written to simple.dag.lock 06/21/12 22:51:15 DAG Input file is simple.dag 06/21/12 22:51:15 Parsing 1 dagfiles 06/21/12 22:51:15 Parsing simple.dag ... 06/21/12 22:51:15 Dag contains 1 total jobs 06/21/12 22:51:15 Sleeping for 12 seconds to ensure ProcessId uniqueness 06/21/12 22:51:27 Bootstrapping... 06/21/12 22:51:27 Number of pre-completed nodes: 0 06/21/12 22:51:27 Registering condor_event_timer... 06/21/12 22:51:28 Sleeping for one second for log file consistency 06/21/12 22:51:29 MultiLogFiles: truncating log file /home/roy/condor/simple.log 06/21/12 22:51:29 Submitting Condor Node Simple job(s)... %RED%Here's where the job is submitted%ENDCOLOR% 06/21/12 22:51:29 submitting: condor_submit -a dag_node_name' '=' 'Simple -a +DAGManJobId' '=' '61 -a DAGManJobId' '=' '61 -a submit_event_notes' '=' 'DAG' 'Node:' 'Simple -a DAG_STATUS' '=' '0 -a FAILED_COUNT' '=' '0 -a +DAGParentNodeNames' '=' '\"\" submit 06/21/12 22:51:30 From submit: Submitting job(s). 06/21/12 22:51:30 From submit: 1 job(s) submitted to cluster 62. 06/21/12 22:51:30 assigned Condor ID (62.0.0) 06/21/12 22:51:30 Just submitted 1 job this cycle... 06/21/12 22:51:30 Currently monitoring 1 Condor log file(s) 06/21/12 22:51:30 Event: ULOG_SUBMIT for Condor Node Simple (62.0.0) 06/21/12 22:51:30 Number of idle job procs: 1 06/21/12 22:51:30 Of 1 nodes total: 06/21/12 22:51:30 Done Pre Queued Post Ready Un-Ready Failed 06/21/12 22:51:30 === === === === === === === 06/21/12 22:51:30 0 0 1 0 0 0 0 06/21/12 22:51:30 0 job proc(s) currently held 06/21/12 22:55:05 Currently monitoring 1 Condor log file(s) %RED%Here's where DAGMan noticed that the job is running%ENDCOLOR% 06/21/12 22:55:05 Event: ULOG_EXECUTE for Condor Node Simple (62.0.0) 06/21/12 22:55:05 Number of idle job procs: 0 06/21/12 22:55:10 Currently monitoring 1 Condor log file(s) 06/21/12 22:55:10 Event: ULOG_IMAGE_SIZE for Condor Node Simple (62.0.0) 06/21/12 22:56:05 Currently monitoring 1 Condor log file(s) 06/21/12 22:56:05 Event: ULOG_IMAGE_SIZE for Condor Node Simple (62.0.0) %RED%Here's where DAGMan noticed that the job finished.%ENDCOLOR% 06/21/12 22:56:05 Event: ULOG_JOB_TERMINATED for Condor Node Simple (62.0.0) 06/21/12 22:56:05 Node Simple job proc (62.0.0) completed successfully. 06/21/12 22:56:05 Node Simple job completed 06/21/12 22:56:05 Number of idle job procs: 0 06/21/12 22:56:05 Of 1 nodes total: 06/21/12 22:56:05 Done Pre Queued Post Ready Un-Ready Failed 06/21/12 22:56:05 === === === === === === === 06/21/12 22:56:05 1 0 0 0 0 0 0 06/21/12 22:56:05 0 job proc(s) currently held %RED%Here's where DAGMan noticed that all the work is done.%ENDCOLOR% 06/21/12 22:56:05 All jobs Completed! 06/21/12 22:56:05 Note: 0 total job deferrals because of -MaxJobs limit (0) 06/21/12 22:56:05 Note: 0 total job deferrals because of -MaxIdle limit (0) 06/21/12 22:56:05 Note: 0 total job deferrals because of node category throttles 06/21/12 22:56:05 Note: 0 total PRE script deferrals because of -MaxPre limit (0) 06/21/12 22:56:05 Note: 0 total POST script deferrals because of -MaxPost limit (0) 06/21/12 22:56:05 **** condor_scheduniv_exec.61.0 (condor_DAGMAN) pid 5812 EXITING WITH STATUS 0 Now verify your results: %UCL_PROMPT_SHORT% <strong>cat simple.log</strong> 000 (062.000.000) 06/21 22:51:30 Job submitted from host: <128.104.100.55:9618?sock=28867_10e4_2> DAG Node: Simple ... 001 (062.000.000) 06/21 22:55:00 Job executing on host: <128.104.58.36:46761> ... 006 (062.000.000) 06/21 22:55:09 Image size of job updated: 750 3 - MemoryUsage of job (MB) 2324 - ResidentSetSize of job (KB) ... 006 (062.000.000) 06/21 22:56:00 Image size of job updated: 780 3 - MemoryUsage of job (MB) 2324 - ResidentSetSize of job (KB) ... 005 (062.000.000) 06/21 22:56:00 Job terminated. (1) Normal termination (return value 0) Usr 0 00:00:00, Sys 0 00:00:00 - Run Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Run Local Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Local Usage 57 - Run Bytes Sent By Job 608490 - Run Bytes Received By Job 57 - Total Bytes Sent By Job 608490 - Total Bytes Received By Job Partitionable Resources : Usage Request Cpus : 1 Disk (KB) : 750 750 Memory (MB) : 3 3 ... Looking at DAGMan's various files, we see that DAGMan itself ran as a Condor job (specifically, a scheduler universe job). %UCL_PROMPT_SHORT% <strong>ls simple.dag.*</strong> simple.dag.condor.sub simple.dag.dagman.log simple.dag.dagman.out simple.dag.lib.err simple.dag.lib.out %UCL_PROMPT_SHORT% <strong>cat simple.dag.condor.sub</strong> # Filename: simple.dag.condor.sub # Generated by condor_submit_dag simple.dag universe = scheduler executable = /usr/bin/condor_dagman getenv = True output = simple.dag.lib.out error = simple.dag.lib.err log = simple.dag.dagman.log remove_kill_sig = SIGUSR1 +OtherJobRemoveRequirements = \"DAGManJobId == $(cluster)\" # Note: default on_exit_remove expression: # ( ExitSignal =?= 11 || (ExitCode =!= UNDEFINED && ExitCode >=0 && ExitCode <= 2)) # attempts to ensure that DAGMan is automatically # requeued by the schedd if it exits abnormally or # is killed (e.g., during a reboot). on_exit_remove = ( ExitSignal =?= 11 || (ExitCode =!= UNDEFINED && ExitCode >=0 && ExitCode <= 2)) copy_to_spool = False arguments = \"-f -l . -Lockfile simple.dag.lock -AutoRescue 1 -DoRescueFrom 0 -Dag simple.dag -CsdVersion $CondorVersion:' '7.7.6' 'Apr' '16' '2012' 'BuildID:' '34175' 'PRE-RELEASE-UWCS' '$ -Force -Dagman /usr/bin/condor_dagman\" environment = _CONDOR_DAGMAN_LOG=simple.dag.dagman.out;_CONDOR_MAX_DAGMAN_LOG=0 queue Clean up some of these files: %UCL_PROMPT_SHORT% <strong>rm simple.dag.*</strong> On Your Own \u00b6 Why does DAGman run as a Condor job? Look at the submit file for DAGMan: What does on_exit_remove do? Why is this here? Challenge \u00b6 What is the scheduler universe? Why does DAGMan use it?","title":"3.3. Simple DAG"},{"location":"materials/day1/part3-ex3-simple-dag/#monday-exercise-33-coordinating-a-set-of-jobs-with-a-simple-dag","text":"The objective of this exercise is to learn the very basics of running a set of jobs, where our set is just one job.","title":"Monday Exercise 3.3: Coordinating a Set of Jobs With a Simple DAG"},{"location":"materials/day1/part3-ex3-simple-dag/#what-is-dagman","text":"In short, DAGMan lets you submit complex sequences of jobs as long as they can be expressed as a directed acylic graph. For example, you may wish to run a large parameter sweep but before the sweep run you need to prepare your data. After the sweep runs, you need to collate the results. This might look like this, assuming you want to sweep over five parameters: DAGMan has many abilities, such as throttling jobs, recovery from failures, and more. More information about DAGMan can be found at in the Condor manual .","title":"What is DAGMan?"},{"location":"materials/day1/part3-ex3-simple-dag/#submitting-a-simple-dag","text":"For our job, we will return briefly to the sleep program. executable = /bin/sleep arguments = 4 log = simple.log output = simple.out error = simple.error request_memory = 1GB request_disk = 1GB request_cpus = 1 queue We are going to get a bit more sophisticated in submitting our jobs now. Let's have three windows open. In one window, you'll submit the job. In another you will watch the queue, and in the third you will watch what DAGMan does. First we will create the most minimal DAG that can be created: a DAG with just one node. Put this into a file named simple.dag . Job Simple simple.sub In your first window, submit the DAG: %UCL_PROMPT_SHORT% <strong>condor_submit_dag simple.dag</strong> ----------------------------------------------------------------------- File for submitting this DAG to Condor : simple.dag.condor.sub Log of DAGMan debugging messages : simple.dag.dagman.out Log of Condor library output : simple.dag.lib.out Log of Condor library error messages : simple.dag.lib.err Log of the life of condor_dagman itself : simple.dag.dagman.log Submitting job(s). 1 job(s) submitted to cluster 61. ----------------------------------------------------------------------- In the second window, watch the queue: %UCL_PROMPT_SHORT% <strong>watch -n 10 condor_q -nobatch</strong> -- Submitter: learn.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : learn.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 61.0 roy 6/21 22:51 0+00:00:01 R 0 0.3 condor_dagman 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended -- Submitter: learn.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : learn.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 61.0 roy 6/21 22:51 0+00:01:25 R 0 0.3 condor_dagman 62.0 roy 6/21 22:51 0+00:00:00 I 0 0.7 simple 4 10 2 jobs; 0 completed, 0 removed, 1 idle, 1 running, 0 held, 0 suspended -- Submitter: learn.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : learn.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 61.0 roy 6/21 22:51 0+00:03:47 R 0 0.3 condor_dagman 62.0 roy 6/21 22:51 0+00:00:03 R 0 0.7 simple 4 10 2 jobs; 0 completed, 0 removed, 0 idle, 2 running, 0 held, 0 suspended -- Submitter: learn.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : learn.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 0 jobs; 0 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended <i>Ctrl-C</i> In the third window, watch what DAGMan does: %UCL_PROMPT_SHORT% <strong>tail -f --lines=500 simple.dag.dagman.out</strong> 6/21/12 22:51:13 Setting maximum accepts per cycle 8. 06/21/12 22:51:13 ****************************************************** 06/21/12 22:51:13 ** condor_scheduniv_exec.61.0 (CONDOR_DAGMAN) STARTING UP 06/21/12 22:51:13 ** /usr/bin/condor_dagman 06/21/12 22:51:13 ** SubsystemInfo: name=DAGMAN type=DAGMAN(10) class=DAEMON(1) 06/21/12 22:51:13 ** Configuration: subsystem:DAGMAN local:<NONE> class:DAEMON 06/21/12 22:51:13 ** $CondorVersion: 7.7.6 Apr 16 2012 BuildID: 34175 PRE-RELEASE-UWCS $ 06/21/12 22:51:13 ** $CondorPlatform: x86_64_rhap_5.7 $ 06/21/12 22:51:13 ** PID = 5812 06/21/12 22:51:13 ** Log last touched 6/21 22:51:00 06/21/12 22:51:13 ****************************************************** 06/21/12 22:51:13 Using config source: /etc/condor/condor_config 06/21/12 22:51:13 Using local config sources: 06/21/12 22:51:13 /etc/condor/config.d/00-chtc-global.conf 06/21/12 22:51:13 /etc/condor/config.d/01-chtc-submit.conf 06/21/12 22:51:13 /etc/condor/config.d/02-chtc-flocking.conf 06/21/12 22:51:13 /etc/condor/config.d/03-chtc-jobrouter.conf 06/21/12 22:51:13 /etc/condor/config.d/04-chtc-blacklist.conf 06/21/12 22:51:13 /etc/condor/config.d/99-osg-ss-group.conf 06/21/12 22:51:13 /etc/condor/config.d/99-roy-extras.conf 06/21/12 22:51:13 /etc/condor/condor_config.local 06/21/12 22:51:13 DaemonCore: command socket at <128.104.100.55:60417> 06/21/12 22:51:13 DaemonCore: private command socket at <128.104.100.55:60417> 06/21/12 22:51:13 Setting maximum accepts per cycle 8. 06/21/12 22:51:13 DAGMAN_USE_STRICT setting: 0 06/21/12 22:51:13 DAGMAN_VERBOSITY setting: 3 06/21/12 22:51:13 DAGMAN_DEBUG_CACHE_SIZE setting: 5242880 06/21/12 22:51:13 DAGMAN_DEBUG_CACHE_ENABLE setting: False 06/21/12 22:51:13 DAGMAN_SUBMIT_DELAY setting: 0 06/21/12 22:51:13 DAGMAN_MAX_SUBMIT_ATTEMPTS setting: 6 06/21/12 22:51:13 DAGMAN_STARTUP_CYCLE_DETECT setting: False 06/21/12 22:51:13 DAGMAN_MAX_SUBMITS_PER_INTERVAL setting: 5 06/21/12 22:51:13 DAGMAN_USER_LOG_SCAN_INTERVAL setting: 5 06/21/12 22:51:13 allow_events (DAGMAN_IGNORE_DUPLICATE_JOB_EXECUTION, DAGMAN_ALLOW_EVENTS) setting: 114 06/21/12 22:51:13 DAGMAN_RETRY_SUBMIT_FIRST setting: True 06/21/12 22:51:13 DAGMAN_RETRY_NODE_FIRST setting: False 06/21/12 22:51:13 DAGMAN_MAX_JOBS_IDLE setting: 0 06/21/12 22:51:13 DAGMAN_MAX_JOBS_SUBMITTED setting: 0 06/21/12 22:51:15 DAGMAN_MAX_PRE_SCRIPTS setting: 0 06/21/12 22:51:15 DAGMAN_MAX_POST_SCRIPTS setting: 0 06/21/12 22:51:15 DAGMAN_ALLOW_LOG_ERROR setting: False 06/21/12 22:51:15 DAGMAN_MUNGE_NODE_NAMES setting: True 06/21/12 22:51:15 DAGMAN_PROHIBIT_MULTI_JOBS setting: False 06/21/12 22:51:15 DAGMAN_SUBMIT_DEPTH_FIRST setting: False 06/21/12 22:51:15 DAGMAN_ALWAYS_RUN_POST setting: True 06/21/12 22:51:15 DAGMAN_ABORT_DUPLICATES setting: True 06/21/12 22:51:15 DAGMAN_ABORT_ON_SCARY_SUBMIT setting: True 06/21/12 22:51:15 DAGMAN_PENDING_REPORT_INTERVAL setting: 600 06/21/12 22:51:15 DAGMAN_AUTO_RESCUE setting: True 06/21/12 22:51:15 DAGMAN_MAX_RESCUE_NUM setting: 100 06/21/12 22:51:15 DAGMAN_WRITE_PARTIAL_RESCUE setting: True 06/21/12 22:51:15 DAGMAN_DEFAULT_NODE_LOG setting: null 06/21/12 22:51:15 DAGMAN_GENERATE_SUBDAG_SUBMITS setting: True 06/21/12 22:51:15 ALL_DEBUG setting: 06/21/12 22:51:15 DAGMAN_DEBUG setting: 06/21/12 22:51:15 argv[0] == \"condor_scheduniv_exec.61.0\" 06/21/12 22:51:15 argv[1] == \"-Lockfile\" 06/21/12 22:51:15 argv[2] == \"simple.dag.lock\" 06/21/12 22:51:15 argv[3] == \"-AutoRescue\" 06/21/12 22:51:15 argv[4] == \"1\" 06/21/12 22:51:15 argv[5] == \"-DoRescueFrom\" 06/21/12 22:51:15 argv[6] == \"0\" 06/21/12 22:51:15 argv[7] == \"-Dag\" 06/21/12 22:51:15 argv[8] == \"simple.dag\" 06/21/12 22:51:15 argv[9] == \"-CsdVersion\" 06/21/12 22:51:15 argv[10] == \"$CondorVersion: 7.7.6 Apr 16 2012 BuildID: 34175 PRE-RELEASE-UWCS $\" 06/21/12 22:51:15 argv[11] == \"-Force\" 06/21/12 22:51:15 argv[12] == \"-Dagman\" 06/21/12 22:51:15 argv[13] == \"/usr/bin/condor_dagman\" 06/21/12 22:51:15 Default node log file is: </home/roy/condor/simple.dag.nodes.log> 06/21/12 22:51:15 DAG Lockfile will be written to simple.dag.lock 06/21/12 22:51:15 DAG Input file is simple.dag 06/21/12 22:51:15 Parsing 1 dagfiles 06/21/12 22:51:15 Parsing simple.dag ... 06/21/12 22:51:15 Dag contains 1 total jobs 06/21/12 22:51:15 Sleeping for 12 seconds to ensure ProcessId uniqueness 06/21/12 22:51:27 Bootstrapping... 06/21/12 22:51:27 Number of pre-completed nodes: 0 06/21/12 22:51:27 Registering condor_event_timer... 06/21/12 22:51:28 Sleeping for one second for log file consistency 06/21/12 22:51:29 MultiLogFiles: truncating log file /home/roy/condor/simple.log 06/21/12 22:51:29 Submitting Condor Node Simple job(s)... %RED%Here's where the job is submitted%ENDCOLOR% 06/21/12 22:51:29 submitting: condor_submit -a dag_node_name' '=' 'Simple -a +DAGManJobId' '=' '61 -a DAGManJobId' '=' '61 -a submit_event_notes' '=' 'DAG' 'Node:' 'Simple -a DAG_STATUS' '=' '0 -a FAILED_COUNT' '=' '0 -a +DAGParentNodeNames' '=' '\"\" submit 06/21/12 22:51:30 From submit: Submitting job(s). 06/21/12 22:51:30 From submit: 1 job(s) submitted to cluster 62. 06/21/12 22:51:30 assigned Condor ID (62.0.0) 06/21/12 22:51:30 Just submitted 1 job this cycle... 06/21/12 22:51:30 Currently monitoring 1 Condor log file(s) 06/21/12 22:51:30 Event: ULOG_SUBMIT for Condor Node Simple (62.0.0) 06/21/12 22:51:30 Number of idle job procs: 1 06/21/12 22:51:30 Of 1 nodes total: 06/21/12 22:51:30 Done Pre Queued Post Ready Un-Ready Failed 06/21/12 22:51:30 === === === === === === === 06/21/12 22:51:30 0 0 1 0 0 0 0 06/21/12 22:51:30 0 job proc(s) currently held 06/21/12 22:55:05 Currently monitoring 1 Condor log file(s) %RED%Here's where DAGMan noticed that the job is running%ENDCOLOR% 06/21/12 22:55:05 Event: ULOG_EXECUTE for Condor Node Simple (62.0.0) 06/21/12 22:55:05 Number of idle job procs: 0 06/21/12 22:55:10 Currently monitoring 1 Condor log file(s) 06/21/12 22:55:10 Event: ULOG_IMAGE_SIZE for Condor Node Simple (62.0.0) 06/21/12 22:56:05 Currently monitoring 1 Condor log file(s) 06/21/12 22:56:05 Event: ULOG_IMAGE_SIZE for Condor Node Simple (62.0.0) %RED%Here's where DAGMan noticed that the job finished.%ENDCOLOR% 06/21/12 22:56:05 Event: ULOG_JOB_TERMINATED for Condor Node Simple (62.0.0) 06/21/12 22:56:05 Node Simple job proc (62.0.0) completed successfully. 06/21/12 22:56:05 Node Simple job completed 06/21/12 22:56:05 Number of idle job procs: 0 06/21/12 22:56:05 Of 1 nodes total: 06/21/12 22:56:05 Done Pre Queued Post Ready Un-Ready Failed 06/21/12 22:56:05 === === === === === === === 06/21/12 22:56:05 1 0 0 0 0 0 0 06/21/12 22:56:05 0 job proc(s) currently held %RED%Here's where DAGMan noticed that all the work is done.%ENDCOLOR% 06/21/12 22:56:05 All jobs Completed! 06/21/12 22:56:05 Note: 0 total job deferrals because of -MaxJobs limit (0) 06/21/12 22:56:05 Note: 0 total job deferrals because of -MaxIdle limit (0) 06/21/12 22:56:05 Note: 0 total job deferrals because of node category throttles 06/21/12 22:56:05 Note: 0 total PRE script deferrals because of -MaxPre limit (0) 06/21/12 22:56:05 Note: 0 total POST script deferrals because of -MaxPost limit (0) 06/21/12 22:56:05 **** condor_scheduniv_exec.61.0 (condor_DAGMAN) pid 5812 EXITING WITH STATUS 0 Now verify your results: %UCL_PROMPT_SHORT% <strong>cat simple.log</strong> 000 (062.000.000) 06/21 22:51:30 Job submitted from host: <128.104.100.55:9618?sock=28867_10e4_2> DAG Node: Simple ... 001 (062.000.000) 06/21 22:55:00 Job executing on host: <128.104.58.36:46761> ... 006 (062.000.000) 06/21 22:55:09 Image size of job updated: 750 3 - MemoryUsage of job (MB) 2324 - ResidentSetSize of job (KB) ... 006 (062.000.000) 06/21 22:56:00 Image size of job updated: 780 3 - MemoryUsage of job (MB) 2324 - ResidentSetSize of job (KB) ... 005 (062.000.000) 06/21 22:56:00 Job terminated. (1) Normal termination (return value 0) Usr 0 00:00:00, Sys 0 00:00:00 - Run Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Run Local Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Remote Usage Usr 0 00:00:00, Sys 0 00:00:00 - Total Local Usage 57 - Run Bytes Sent By Job 608490 - Run Bytes Received By Job 57 - Total Bytes Sent By Job 608490 - Total Bytes Received By Job Partitionable Resources : Usage Request Cpus : 1 Disk (KB) : 750 750 Memory (MB) : 3 3 ... Looking at DAGMan's various files, we see that DAGMan itself ran as a Condor job (specifically, a scheduler universe job). %UCL_PROMPT_SHORT% <strong>ls simple.dag.*</strong> simple.dag.condor.sub simple.dag.dagman.log simple.dag.dagman.out simple.dag.lib.err simple.dag.lib.out %UCL_PROMPT_SHORT% <strong>cat simple.dag.condor.sub</strong> # Filename: simple.dag.condor.sub # Generated by condor_submit_dag simple.dag universe = scheduler executable = /usr/bin/condor_dagman getenv = True output = simple.dag.lib.out error = simple.dag.lib.err log = simple.dag.dagman.log remove_kill_sig = SIGUSR1 +OtherJobRemoveRequirements = \"DAGManJobId == $(cluster)\" # Note: default on_exit_remove expression: # ( ExitSignal =?= 11 || (ExitCode =!= UNDEFINED && ExitCode >=0 && ExitCode <= 2)) # attempts to ensure that DAGMan is automatically # requeued by the schedd if it exits abnormally or # is killed (e.g., during a reboot). on_exit_remove = ( ExitSignal =?= 11 || (ExitCode =!= UNDEFINED && ExitCode >=0 && ExitCode <= 2)) copy_to_spool = False arguments = \"-f -l . -Lockfile simple.dag.lock -AutoRescue 1 -DoRescueFrom 0 -Dag simple.dag -CsdVersion $CondorVersion:' '7.7.6' 'Apr' '16' '2012' 'BuildID:' '34175' 'PRE-RELEASE-UWCS' '$ -Force -Dagman /usr/bin/condor_dagman\" environment = _CONDOR_DAGMAN_LOG=simple.dag.dagman.out;_CONDOR_MAX_DAGMAN_LOG=0 queue Clean up some of these files: %UCL_PROMPT_SHORT% <strong>rm simple.dag.*</strong>","title":"Submitting a Simple DAG"},{"location":"materials/day1/part3-ex3-simple-dag/#on-your-own","text":"Why does DAGman run as a Condor job? Look at the submit file for DAGMan: What does on_exit_remove do? Why is this here?","title":"On Your Own"},{"location":"materials/day1/part3-ex3-simple-dag/#challenge","text":"What is the scheduler universe? Why does DAGMan use it?","title":"Challenge"},{"location":"materials/day1/part3-ex4-complex-dag/","text":"a pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: #008; } Monday Exercise 3.4: A More Complex DAG \u00b6 The objective of this exercise is to run a real set of jobs with DAGMan. Make Your Job Submission Files \u00b6 We'll run our goatbrot example. If you didn't read about it yet, please do so now . We are going to make a DAG with four simultaneous jobs ( goatbrot ) and one final node to stitch them together ( montage ). This means we have five jobs. We're going to run goatbrot with more iterations (100,000) so each job will take longer to run. You can create your five jobs. The goatbrot jobs are very similar to each other, but they have slightly different parameters and output files. goatbrot1.sub \u00b6 executable = /usr/local/bin/goatbrot arguments = -i 100000 -c -0.75,0.75 -w 1.5 -s 500,500 -o tile_0_0.ppm log = goatbrot.log output = goatbrot.out.0.0 error = goatbrot.err.0.0 request_memory = 1GB request_disk = 1GB request_cpus = 1 queue goatbrot2.sub \u00b6 executable = /usr/local/bin/goatbrot arguments = -i 100000 -c 0.75,0.75 -w 1.5 -s 500,500 -o tile_0_1.ppm log = goatbrot.log output = goatbrot.out.0.1 error = goatbrot.err.0.1 request_memory = 1GB request_disk = 1GB request_cpus = 1 queue goatbrot3.sub \u00b6 executable = /usr/local/bin/goatbrot arguments = -i 100000 -c -0.75,-0.75 -w 1.5 -s 500,500 -o tile_1_0.ppm log = goatbrot.log output = goatbrot.out.1.0 error = goatbrot.err.1.0 request_memory = 1GB request_disk = 1GB request_cpus = 1 queue goatbrot4.sub \u00b6 executable = /usr/local/bin/goatbrot arguments = -i 100000 -c 0.75,-0.75 -w 1.5 -s 500,500 -o tile_1_1.ppm log = goatbrot.log output = goatbrot.out.1.1 error = goatbrot.err.1.1 request_memory = 1GB request_disk = 1GB request_cpus = 1 queue montage.sub \u00b6 You should notice that the transfer_input_files statement refers to the files created by the other jobs. executable = /usr/bin/montage arguments = tile_0_0.ppm tile_0_1.ppm tile_1_0.ppm tile_1_1.ppm -mode Concatenate -tile 2x2 mandel-from-dag.jpg transfer_input_files = tile_0_0.ppm,tile_0_1.ppm,tile_1_0.ppm,tile_1_1.ppm output = montage.out error = montage.err log = montage.log request_memory = 1GB request_disk = 1GB request_cpus = 1 queue Make your DAG \u00b6 In a file called goatbrot.dag , you have your DAG specification: JOB g1 goatbrot1.sub JOB g2 goatbrot2.sub JOB g3 goatbrot3.sub JOB g4 goatbrot4.sub JOB montage montage.sub PARENT g1 g2 g3 g4 CHILD montage Ask yourself: do you know how we ensure that all the goatbrot commands can run simultaneously and all of them will complete before we run the montage job? Running the DAG \u00b6 Submit your DAG: %UCL_PROMPT_SHORT% <strong>condor_submit_dag goatbrot.dag</strong> ----------------------------------------------------------------------- File for submitting this DAG to Condor : goatbrot.dag.condor.sub Log of DAGMan debugging messages : goatbrot.dag.dagman.out Log of Condor library output : goatbrot.dag.lib.out Log of Condor library error messages : goatbrot.dag.lib.err Log of the life of condor_dagman itself : goatbrot.dag.dagman.log Submitting job(s). 1 job(s) submitted to cluster 71. ----------------------------------------------------------------------- Watch Your DAG \u00b6 Let\u2019s follow the progress of the whole DAG: Use the watch command to run condor_q -nobatch every 10 seconds: watch -n 10 condor_q -nobatch %RED%Here we see DAGMan running: -- Submitter: learn.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : learn.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 71.0 roy 6/22 17:39 0+00:00:03 R 0 0.3 condor_dagman 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended %RED%DAGMan has submitted the goatbrot jobs, but they haven't started running yet: -- Submitter: learn.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : learn.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 71.0 roy 6/22 17:39 0+00:00:17 R 0 0.3 condor_dagman 72.0 roy 6/22 17:39 0+00:00:00 I 0 0.0 goatbrot -i 100000 73.0 roy 6/22 17:39 0+00:00:00 I 0 0.0 goatbrot -i 100000 74.0 roy 6/22 17:39 0+00:00:00 I 0 0.0 goatbrot -i 100000 75.0 roy 6/22 17:39 0+00:00:00 I 0 0.0 goatbrot -i 100000 5 jobs; 0 completed, 0 removed, 4 idle, 1 running, 0 held, 0 suspended %RED%They're running%ENDCOLOR% -- Submitter: learn.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : learn.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 71.0 roy 6/22 17:39 0+00:07:15 R 0 0.3 condor_dagman 72.0 roy 6/22 17:39 0+00:00:03 R 0 0.0 goatbrot -i 100000 73.0 roy 6/22 17:39 0+00:00:03 R 0 0.0 goatbrot -i 100000 74.0 roy 6/22 17:39 0+00:00:03 R 0 0.0 goatbrot -i 100000 75.0 roy 6/22 17:39 0+00:00:03 R 0 0.0 goatbrot -i 100000 5 jobs; 0 completed, 0 removed, 0 idle, 5 running, 0 held, 0 suspended %RED%Two of the jobs have finished, while the others are still running: -- Submitter: learn.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : learn.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 71.0 roy 6/22 17:39 0+00:07:51 R 0 0.3 condor_dagman 72.0 roy 6/22 17:39 0+00:00:39 R 0 0.0 goatbrot -i 100000 74.0 roy 6/22 17:39 0+00:00:39 R 0 0.0 goatbrot -i 100000 3 jobs; 0 completed, 0 removed, 0 idle, 3 running, 0 held, 0 suspended %RED%They finished, but DAGMan hasn't noticed yet. It only checks periodically: -- Submitter: learn.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : learn.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 71.0 roy 6/22 17:39 0+00:08:46 R 0 0.3 condor_dagman 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended %RED%DAGMan submitted and ran the montage job. It ran so fast I didn't capture it running. DAGMan will finish up soon -- Submitter: learn.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : learn.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 71.0 roy 6/22 17:39 0+00:08:55 R 0 0.3 condor_dagman 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended %RED%Now it's all done: -- Submitter: learn.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : learn.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 0 jobs; 0 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended Examine your results. For some reason, goatbrot prints everything to stderr, not stdout. cat goatbrot.err.0.0 Complex image: Center: -0.75 + 0.75i Width: 1.5 Height: 1.5 Upper Left: -1.5 + 1.5i Lower Right: 0 + 0i Output image: Filename: tile_0_0.ppm Width, Height: 500, 500 Theme: beej Antialiased: no Mandelbrot: Max Iterations: 100000 Continuous: no Goatbrot: Multithreading: not supported in this build Completed: 100.0% Examine your log files ( goatbrot.log and montage.log ) and DAGMan output file ( goatbrot.dag.dagman.out ). Do they look as you expect? Can you see the progress of the DAG in the DAGMan output file? As you did earlier, copy the resulting mandel-from-dag.jpg to your public_html directory, then access it from your web browser. Does the image look correct? Clean up your results. Be careful about deleting the goatbrot.dag.* files, you do not want to delete the goatbrot.dag file, just goatbrot.dag.* . rm goatbrot.dag.* rm goatbrot.out.* rm goatbrot.err.* On Your Own \u00b6 Re-run your DAG. When jobs are running, try condor_q -nobatch -dag . What does it do differently? Challenge, if you have time: Make a bigger DAG by making more tiles in the same area.","title":"3.4. More complex DAG"},{"location":"materials/day1/part3-ex4-complex-dag/#monday-exercise-34-a-more-complex-dag","text":"The objective of this exercise is to run a real set of jobs with DAGMan.","title":"Monday Exercise 3.4: A More Complex DAG"},{"location":"materials/day1/part3-ex4-complex-dag/#make-your-job-submission-files","text":"We'll run our goatbrot example. If you didn't read about it yet, please do so now . We are going to make a DAG with four simultaneous jobs ( goatbrot ) and one final node to stitch them together ( montage ). This means we have five jobs. We're going to run goatbrot with more iterations (100,000) so each job will take longer to run. You can create your five jobs. The goatbrot jobs are very similar to each other, but they have slightly different parameters and output files.","title":"Make Your Job Submission Files"},{"location":"materials/day1/part3-ex4-complex-dag/#goatbrot1sub","text":"executable = /usr/local/bin/goatbrot arguments = -i 100000 -c -0.75,0.75 -w 1.5 -s 500,500 -o tile_0_0.ppm log = goatbrot.log output = goatbrot.out.0.0 error = goatbrot.err.0.0 request_memory = 1GB request_disk = 1GB request_cpus = 1 queue","title":"goatbrot1.sub"},{"location":"materials/day1/part3-ex4-complex-dag/#goatbrot2sub","text":"executable = /usr/local/bin/goatbrot arguments = -i 100000 -c 0.75,0.75 -w 1.5 -s 500,500 -o tile_0_1.ppm log = goatbrot.log output = goatbrot.out.0.1 error = goatbrot.err.0.1 request_memory = 1GB request_disk = 1GB request_cpus = 1 queue","title":"goatbrot2.sub"},{"location":"materials/day1/part3-ex4-complex-dag/#goatbrot3sub","text":"executable = /usr/local/bin/goatbrot arguments = -i 100000 -c -0.75,-0.75 -w 1.5 -s 500,500 -o tile_1_0.ppm log = goatbrot.log output = goatbrot.out.1.0 error = goatbrot.err.1.0 request_memory = 1GB request_disk = 1GB request_cpus = 1 queue","title":"goatbrot3.sub"},{"location":"materials/day1/part3-ex4-complex-dag/#goatbrot4sub","text":"executable = /usr/local/bin/goatbrot arguments = -i 100000 -c 0.75,-0.75 -w 1.5 -s 500,500 -o tile_1_1.ppm log = goatbrot.log output = goatbrot.out.1.1 error = goatbrot.err.1.1 request_memory = 1GB request_disk = 1GB request_cpus = 1 queue","title":"goatbrot4.sub"},{"location":"materials/day1/part3-ex4-complex-dag/#montagesub","text":"You should notice that the transfer_input_files statement refers to the files created by the other jobs. executable = /usr/bin/montage arguments = tile_0_0.ppm tile_0_1.ppm tile_1_0.ppm tile_1_1.ppm -mode Concatenate -tile 2x2 mandel-from-dag.jpg transfer_input_files = tile_0_0.ppm,tile_0_1.ppm,tile_1_0.ppm,tile_1_1.ppm output = montage.out error = montage.err log = montage.log request_memory = 1GB request_disk = 1GB request_cpus = 1 queue","title":"montage.sub"},{"location":"materials/day1/part3-ex4-complex-dag/#make-your-dag","text":"In a file called goatbrot.dag , you have your DAG specification: JOB g1 goatbrot1.sub JOB g2 goatbrot2.sub JOB g3 goatbrot3.sub JOB g4 goatbrot4.sub JOB montage montage.sub PARENT g1 g2 g3 g4 CHILD montage Ask yourself: do you know how we ensure that all the goatbrot commands can run simultaneously and all of them will complete before we run the montage job?","title":"Make your DAG"},{"location":"materials/day1/part3-ex4-complex-dag/#running-the-dag","text":"Submit your DAG: %UCL_PROMPT_SHORT% <strong>condor_submit_dag goatbrot.dag</strong> ----------------------------------------------------------------------- File for submitting this DAG to Condor : goatbrot.dag.condor.sub Log of DAGMan debugging messages : goatbrot.dag.dagman.out Log of Condor library output : goatbrot.dag.lib.out Log of Condor library error messages : goatbrot.dag.lib.err Log of the life of condor_dagman itself : goatbrot.dag.dagman.log Submitting job(s). 1 job(s) submitted to cluster 71. -----------------------------------------------------------------------","title":"Running the DAG"},{"location":"materials/day1/part3-ex4-complex-dag/#watch-your-dag","text":"Let\u2019s follow the progress of the whole DAG: Use the watch command to run condor_q -nobatch every 10 seconds: watch -n 10 condor_q -nobatch %RED%Here we see DAGMan running: -- Submitter: learn.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : learn.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 71.0 roy 6/22 17:39 0+00:00:03 R 0 0.3 condor_dagman 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended %RED%DAGMan has submitted the goatbrot jobs, but they haven't started running yet: -- Submitter: learn.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : learn.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 71.0 roy 6/22 17:39 0+00:00:17 R 0 0.3 condor_dagman 72.0 roy 6/22 17:39 0+00:00:00 I 0 0.0 goatbrot -i 100000 73.0 roy 6/22 17:39 0+00:00:00 I 0 0.0 goatbrot -i 100000 74.0 roy 6/22 17:39 0+00:00:00 I 0 0.0 goatbrot -i 100000 75.0 roy 6/22 17:39 0+00:00:00 I 0 0.0 goatbrot -i 100000 5 jobs; 0 completed, 0 removed, 4 idle, 1 running, 0 held, 0 suspended %RED%They're running%ENDCOLOR% -- Submitter: learn.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : learn.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 71.0 roy 6/22 17:39 0+00:07:15 R 0 0.3 condor_dagman 72.0 roy 6/22 17:39 0+00:00:03 R 0 0.0 goatbrot -i 100000 73.0 roy 6/22 17:39 0+00:00:03 R 0 0.0 goatbrot -i 100000 74.0 roy 6/22 17:39 0+00:00:03 R 0 0.0 goatbrot -i 100000 75.0 roy 6/22 17:39 0+00:00:03 R 0 0.0 goatbrot -i 100000 5 jobs; 0 completed, 0 removed, 0 idle, 5 running, 0 held, 0 suspended %RED%Two of the jobs have finished, while the others are still running: -- Submitter: learn.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : learn.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 71.0 roy 6/22 17:39 0+00:07:51 R 0 0.3 condor_dagman 72.0 roy 6/22 17:39 0+00:00:39 R 0 0.0 goatbrot -i 100000 74.0 roy 6/22 17:39 0+00:00:39 R 0 0.0 goatbrot -i 100000 3 jobs; 0 completed, 0 removed, 0 idle, 3 running, 0 held, 0 suspended %RED%They finished, but DAGMan hasn't noticed yet. It only checks periodically: -- Submitter: learn.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : learn.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 71.0 roy 6/22 17:39 0+00:08:46 R 0 0.3 condor_dagman 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended %RED%DAGMan submitted and ran the montage job. It ran so fast I didn't capture it running. DAGMan will finish up soon -- Submitter: learn.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : learn.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 71.0 roy 6/22 17:39 0+00:08:55 R 0 0.3 condor_dagman 1 jobs; 0 completed, 0 removed, 0 idle, 1 running, 0 held, 0 suspended %RED%Now it's all done: -- Submitter: learn.chtc.wisc.edu : <128.104.100.55:9618?sock=28867_10e4_2> : learn.chtc.wisc.edu ID OWNER SUBMITTED RUN_TIME ST PRI SIZE CMD 0 jobs; 0 completed, 0 removed, 0 idle, 0 running, 0 held, 0 suspended Examine your results. For some reason, goatbrot prints everything to stderr, not stdout. cat goatbrot.err.0.0 Complex image: Center: -0.75 + 0.75i Width: 1.5 Height: 1.5 Upper Left: -1.5 + 1.5i Lower Right: 0 + 0i Output image: Filename: tile_0_0.ppm Width, Height: 500, 500 Theme: beej Antialiased: no Mandelbrot: Max Iterations: 100000 Continuous: no Goatbrot: Multithreading: not supported in this build Completed: 100.0% Examine your log files ( goatbrot.log and montage.log ) and DAGMan output file ( goatbrot.dag.dagman.out ). Do they look as you expect? Can you see the progress of the DAG in the DAGMan output file? As you did earlier, copy the resulting mandel-from-dag.jpg to your public_html directory, then access it from your web browser. Does the image look correct? Clean up your results. Be careful about deleting the goatbrot.dag.* files, you do not want to delete the goatbrot.dag file, just goatbrot.dag.* . rm goatbrot.dag.* rm goatbrot.out.* rm goatbrot.err.*","title":"Watch Your DAG"},{"location":"materials/day1/part3-ex4-complex-dag/#on-your-own","text":"Re-run your DAG. When jobs are running, try condor_q -nobatch -dag . What does it do differently? Challenge, if you have time: Make a bigger DAG by making more tiles in the same area.","title":"On Your Own"},{"location":"materials/day1/part4-ex1-failed-dag/","text":"Handling a DAG That Fails \u00b6 The objective of this exercise is to help you learn how DAGMan deals with job failures. DAGMan is built to help you recover from such failures. Background \u00b6 DAGMan can handle a situation where some of the nodes in a DAG fail. DAGMan will run as many nodes as possible, then create a rescue DAG making it easy to continue when the problem is fixed. Breaking Things \u00b6 Recall that DAGMan decides that a jobs fails if its exit code is non-zero. Let's modify our montage job so that it fails. Work in the same directory where you did the last DAG. Edit montage.sub to add a -h to the arguments. It will look like this (the change is highlighted in red): universe = vanilla executable = /usr/bin/montage arguments = %RED%-h%ENDCOLOR% tile_0_0.ppm tile_0_1.ppm tile_1_0.ppm tile_1_1.ppm -mode Concatenate -tile 2x2 mandle.jpg transfer_input_files = tile_0_0.ppm,tile_0_1.ppm,tile_1_0.ppm,tile_1_1.ppm transfer_executable = false output = montage.out error = montage.err log = goat.log request_memory = 1G request_disk = 1G request_cpus = 1 queue Submit the DAG again: % condor_submit_dag goatbrot.dag ----------------------------------------------------------------------- File for submitting this DAG to Condor : goatbrot.dag.condor.sub Log of DAGMan debugging messages : goatbrot.dag.dagman.out Log of Condor library output : goatbrot.dag.lib.out Log of Condor library error messages : goatbrot.dag.lib.err Log of the life of condor_dagman itself : goatbrot.dag.dagman.log Submitting job(s). 1 job(s) submitted to cluster 77. ----------------------------------------------------------------------- Use watch to watch the jobs until they finish. In a separate window, use tail --lines=500 -f goatbrot.dag.dagman.out to watch what DAGMan does. 06/22/12 17:57:41 Setting maximum accepts per cycle 8. 06/22/12 17:57:41 ****************************************************** 06/22/12 17:57:41 ** condor_scheduniv_exec.77.0 (CONDOR_DAGMAN) STARTING UP 06/22/12 17:57:41 ** /usr/bin/condor_dagman 06/22/12 17:57:41 ** SubsystemInfo: name=DAGMAN type=DAGMAN(10) class=DAEMON(1) 06/22/12 17:57:41 ** Configuration: subsystem:DAGMAN local:<NONE> class:DAEMON 06/22/12 17:57:41 ** $CondorVersion: 7.7.6 Apr 16 2012 BuildID: 34175 PRE-RELEASE-UWCS $ 06/22/12 17:57:41 ** $CondorPlatform: x86_64_rhap_5.7 $ 06/22/12 17:57:41 ** PID = 26867 06/22/12 17:57:41 ** Log last touched time unavailable (No such file or directory) 06/22/12 17:57:41 ****************************************************** 06/22/12 17:57:41 Using config source: /etc/condor/condor_config 06/22/12 17:57:41 Using local config sources: 06/22/12 17:57:41 /etc/condor/config.d/00-chtc-global.conf 06/22/12 17:57:41 /etc/condor/config.d/01-chtc-submit.conf 06/22/12 17:57:41 /etc/condor/config.d/02-chtc-flocking.conf 06/22/12 17:57:41 /etc/condor/config.d/03-chtc-jobrouter.conf 06/22/12 17:57:41 /etc/condor/config.d/04-chtc-blacklist.conf 06/22/12 17:57:41 /etc/condor/config.d/99-osg-ss-group.conf 06/22/12 17:57:41 /etc/condor/config.d/99-roy-extras.conf 06/22/12 17:57:41 /etc/condor/condor_config.local ... output trimmed ... %RED%06/22/12 18:08:42 Event: ULOG_EXECUTE for Condor Node montage (82.0.0)%ENDCOLOR% 06/22/12 18:08:42 Number of idle job procs: 0 06/22/12 18:08:42 Event: ULOG_IMAGE_SIZE for Condor Node montage (82.0.0) 06/22/12 18:08:42 Event: ULOG_JOB_TERMINATED for Condor Node montage (82.0.0) %RED%06/22/12 18:08:42 Node montage job proc (82.0.0) failed with status 1.%ENDCOLOR% 06/22/12 18:08:42 Number of idle job procs: 0 06/22/12 18:08:42 Of 5 nodes total: 06/22/12 18:08:42 Done Pre Queued Post Ready Un-Ready Failed 06/22/12 18:08:42 === === === === === === === 06/22/12 18:08:42 4 0 0 0 0 0 1 06/22/12 18:08:42 0 job proc(s) currently held 06/22/12 18:08:42 Aborting DAG... 06/22/12 18:08:42 Writing Rescue DAG to goatbrot.dag.rescue001... 06/22/12 18:08:42 Note: 0 total job deferrals because of -MaxJobs limit (0) 06/22/12 18:08:42 Note: 0 total job deferrals because of -MaxIdle limit (0) 06/22/12 18:08:42 Note: 0 total job deferrals because of node category throttles 06/22/12 18:08:42 Note: 0 total PRE script deferrals because of -MaxPre limit (0) 06/22/12 18:08:42 Note: 0 total POST script deferrals because of -MaxPost limit (0) %RED%06/22/12 18:08:42 **** condor_scheduniv_exec.77.0 (condor_DAGMAN) pid 26867 EXITING WITH STATUS 1%ENDCOLOR% DAGMan notices that one of the jobs failed because it's exit code was non-zero. DAGMan ran as much of the DAG as possible and logged enough information to continue the run when the situation is resolved. Do you see the part where it wrote the resuce DAG? Look at the rescue DAG. It's called a partial DAG: it indicates what part of the DAG has already been completed. When you re-submit the original DAG, DAGMan will notice the rescue DAG and use it in combination with the original DAG. (The rescue DAG used to be the full DAG with nodes marked as done and you would ask DAGMan to run the new rescue DAG. For your simplicity DAGMan lets you resubmit the original DAG and it reads both files.) % cat goatbrot.dag.rescue001 # Rescue DAG file, created after running # the goatbrot.dag DAG file # Created 6/22/2012 23:08:42 UTC %RED%# Rescue DAG version: 2.0.1 (partial)%ENDCOLOR% # # Total number of Nodes: 5 # Nodes premarked DONE: 4 %RED%# Nodes that failed: 1 # montage,<ENDLIST>%ENDCOLOR% DONE g1 DONE g2 DONE g3 DONE g4 From the comment near the top, we know that the montage node failed. Let's fix it by getting rid of the offending -h argument. Change montage.sub to look like: universe = vanilla executable = /usr/bin/montage arguments = tile_0_0.ppm tile_0_1.ppm tile_1_0.ppm tile_1_1.ppm -mode Concatenate -tile 2x2 mandle.jpg transfer_input_files = tile_0_0.ppm,tile_0_1.ppm,tile_1_0.ppm,tile_1_1.ppm transfer_executable = false output = montage.out error = montage.err log = goat.log request_memory = 1G request_disk = 1G request_cpus = 1 queue Now we can re-submit our original DAG and DAGMan will pick up where it left off. It will automatically notice the rescue DAG If you didn't fix the problem, DAGMan would generate another rescue DAG. % condor_submit_dag goatbrot.dag %RED%Running rescue DAG 1%ENDCOLOR% ----------------------------------------------------------------------- File for submitting this DAG to Condor : goatbrot.dag.condor.sub Log of DAGMan debugging messages : goatbrot.dag.dagman.out Log of Condor library output : goatbrot.dag.lib.out Log of Condor library error messages : goatbrot.dag.lib.err Log of the life of condor_dagman itself : goatbrot.dag.dagman.log Submitting job(s). 1 job(s) submitted to cluster 83. ----------------------------------------------------------------------- % tail -f goatbrot.dag.dagman.out 06/23/12 11:30:53 ****************************************************** 06/23/12 11:30:53 ** condor_scheduniv_exec.83.0 (CONDOR_DAGMAN) STARTING UP 06/23/12 11:30:53 ** /usr/bin/condor_dagman 06/23/12 11:30:53 ** SubsystemInfo: name=DAGMAN type=DAGMAN(10) class=DAEMON(1) 06/23/12 11:30:53 ** Configuration: subsystem:DAGMAN local:<NONE> class:DAEMON 06/23/12 11:30:53 ** $CondorVersion: 7.7.6 Apr 16 2012 BuildID: 34175 PRE-RELEASE-UWCS $ 06/23/12 11:30:53 ** $CondorPlatform: x86_64_rhap_5.7 $ 06/23/12 11:30:53 ** PID = 28576 06/23/12 11:30:53 ** Log last touched 6/22 18:08:42 06/23/12 11:30:53 ****************************************************** 06/23/12 11:30:53 Using config source: /etc/condor/condor_config ... %RED%Here is where DAGMAN notices that there is a rescue DAG%ENDCOLOR% 06/23/12 11:30:53 Parsing 1 dagfiles 06/23/12 11:30:53 Parsing goatbrot.dag ... %RED%06/23/12 11:30:53 Found rescue DAG number 1; running goatbrot.dag.rescue001 in combination with normal DAG file%ENDCOLOR% 06/23/12 11:30:53 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 06/23/12 11:30:53 USING RESCUE DAG goatbrot.dag.rescue001 06/23/12 11:30:53 Dag contains 5 total jobs %RED%Shortly thereafter it sees that four jobs have already finished.%ENDCOLOR% 06/23/12 11:31:05 Bootstrapping... 06/23/12 11:31:05 Number of pre-completed nodes: 4 06/23/12 11:31:05 Registering condor_event_timer... 06/23/12 11:31:06 Sleeping for one second for log file consistency 06/23/12 11:31:07 MultiLogFiles: truncating log file /home/roy/condor/goatbrot/montage.log %RED%Here is where DAGMan resubmits the montage job and waits for it to complete.%ENDCOLOR% 06/23/12 11:31:07 Submitting Condor Node montage job(s)... 06/23/12 11:31:07 submitting: condor_submit -a dag_node_name' '=' 'montage -a +DAGManJobId' '=' '83 -a DAGManJobId' '=' '83 -a submit_event_notes' '=' 'DAG' 'Node:' 'montage -a DAG_STATUS' '=' '0 -a FAILED_COUNT' '=' '0 -a +DAGParentNodeNames' '=' '\"g1,g2,g3,g4\" montage.sub 06/23/12 11:31:07 From submit: Submitting job(s). 06/23/12 11:31:07 From submit: 1 job(s) submitted to cluster 84. 06/23/12 11:31:07 assigned Condor ID (84.0.0) 06/23/12 11:31:07 Just submitted 1 job this cycle... 06/23/12 11:31:07 Currently monitoring 1 Condor log file(s) 06/23/12 11:31:07 Event: ULOG_SUBMIT for Condor Node montage (84.0.0) 06/23/12 11:31:07 Number of idle job procs: 1 06/23/12 11:31:07 Of 5 nodes total: 06/23/12 11:31:07 Done Pre Queued Post Ready Un-Ready Failed 06/23/12 11:31:07 === === === === === === === 06/23/12 11:31:07 4 0 1 0 0 0 0 06/23/12 11:31:07 0 job proc(s) currently held 06/23/12 11:40:22 Currently monitoring 1 Condor log file(s) 06/23/12 11:40:22 Event: ULOG_EXECUTE for Condor Node montage (84.0.0) 06/23/12 11:40:22 Number of idle job procs: 0 06/23/12 11:40:22 Event: ULOG_IMAGE_SIZE for Condor Node montage (84.0.0) 06/23/12 11:40:22 Event: ULOG_JOB_TERMINATED for Condor Node montage (84.0.0) %RED%This is where the montage finished.%ENDCOLOR% 06/23/12 11:40:22 Node montage job proc (84.0.0) completed successfully. 06/23/12 11:40:22 Node montage job completed 06/23/12 11:40:22 Number of idle job procs: 0 06/23/12 11:40:22 Of 5 nodes total: 06/23/12 11:40:22 Done Pre Queued Post Ready Un-Ready Failed 06/23/12 11:40:22 === === === === === === === 06/23/12 11:40:22 5 0 0 0 0 0 0 06/23/12 11:40:22 0 job proc(s) currently held %RED%And here DAGMan decides that the work is all done.%ENDCOLOR% 06/23/12 11:40:22 All jobs Completed! 06/23/12 11:40:22 Note: 0 total job deferrals because of -MaxJobs limit (0) 06/23/12 11:40:22 Note: 0 total job deferrals because of -MaxIdle limit (0) 06/23/12 11:40:22 Note: 0 total job deferrals because of node category throttles 06/23/12 11:40:22 Note: 0 total PRE script deferrals because of -MaxPre limit (0) 06/23/12 11:40:22 Note: 0 total POST script deferrals because of -MaxPost limit (0) 06/23/12 11:40:22 **** condor_scheduniv_exec.83.0 (condor_DAGMAN) pid 28576 EXITING WITH STATUS 0 Success! Now go ahead and clean up. Challenge \u00b6 If you have time, add an extra node to the DAG. Copy our original \"simple\" program, but make it exit with a 1 instead of a 0. DAGMan would consider this a failure, but you'll tell DAGMan that it's really a success. This is reasonable--many real world programs use a variety of return codes, and you might need to help DAGMan distinguish success from failure. Write a POST script that checks the return value. Check the Condor manual to see how to describe your post script.","title":"4.1. Failed DAG jobs"},{"location":"materials/day1/part4-ex1-failed-dag/#handling-a-dag-that-fails","text":"The objective of this exercise is to help you learn how DAGMan deals with job failures. DAGMan is built to help you recover from such failures.","title":"Handling a DAG That Fails"},{"location":"materials/day1/part4-ex1-failed-dag/#background","text":"DAGMan can handle a situation where some of the nodes in a DAG fail. DAGMan will run as many nodes as possible, then create a rescue DAG making it easy to continue when the problem is fixed.","title":"Background"},{"location":"materials/day1/part4-ex1-failed-dag/#breaking-things","text":"Recall that DAGMan decides that a jobs fails if its exit code is non-zero. Let's modify our montage job so that it fails. Work in the same directory where you did the last DAG. Edit montage.sub to add a -h to the arguments. It will look like this (the change is highlighted in red): universe = vanilla executable = /usr/bin/montage arguments = %RED%-h%ENDCOLOR% tile_0_0.ppm tile_0_1.ppm tile_1_0.ppm tile_1_1.ppm -mode Concatenate -tile 2x2 mandle.jpg transfer_input_files = tile_0_0.ppm,tile_0_1.ppm,tile_1_0.ppm,tile_1_1.ppm transfer_executable = false output = montage.out error = montage.err log = goat.log request_memory = 1G request_disk = 1G request_cpus = 1 queue Submit the DAG again: % condor_submit_dag goatbrot.dag ----------------------------------------------------------------------- File for submitting this DAG to Condor : goatbrot.dag.condor.sub Log of DAGMan debugging messages : goatbrot.dag.dagman.out Log of Condor library output : goatbrot.dag.lib.out Log of Condor library error messages : goatbrot.dag.lib.err Log of the life of condor_dagman itself : goatbrot.dag.dagman.log Submitting job(s). 1 job(s) submitted to cluster 77. ----------------------------------------------------------------------- Use watch to watch the jobs until they finish. In a separate window, use tail --lines=500 -f goatbrot.dag.dagman.out to watch what DAGMan does. 06/22/12 17:57:41 Setting maximum accepts per cycle 8. 06/22/12 17:57:41 ****************************************************** 06/22/12 17:57:41 ** condor_scheduniv_exec.77.0 (CONDOR_DAGMAN) STARTING UP 06/22/12 17:57:41 ** /usr/bin/condor_dagman 06/22/12 17:57:41 ** SubsystemInfo: name=DAGMAN type=DAGMAN(10) class=DAEMON(1) 06/22/12 17:57:41 ** Configuration: subsystem:DAGMAN local:<NONE> class:DAEMON 06/22/12 17:57:41 ** $CondorVersion: 7.7.6 Apr 16 2012 BuildID: 34175 PRE-RELEASE-UWCS $ 06/22/12 17:57:41 ** $CondorPlatform: x86_64_rhap_5.7 $ 06/22/12 17:57:41 ** PID = 26867 06/22/12 17:57:41 ** Log last touched time unavailable (No such file or directory) 06/22/12 17:57:41 ****************************************************** 06/22/12 17:57:41 Using config source: /etc/condor/condor_config 06/22/12 17:57:41 Using local config sources: 06/22/12 17:57:41 /etc/condor/config.d/00-chtc-global.conf 06/22/12 17:57:41 /etc/condor/config.d/01-chtc-submit.conf 06/22/12 17:57:41 /etc/condor/config.d/02-chtc-flocking.conf 06/22/12 17:57:41 /etc/condor/config.d/03-chtc-jobrouter.conf 06/22/12 17:57:41 /etc/condor/config.d/04-chtc-blacklist.conf 06/22/12 17:57:41 /etc/condor/config.d/99-osg-ss-group.conf 06/22/12 17:57:41 /etc/condor/config.d/99-roy-extras.conf 06/22/12 17:57:41 /etc/condor/condor_config.local ... output trimmed ... %RED%06/22/12 18:08:42 Event: ULOG_EXECUTE for Condor Node montage (82.0.0)%ENDCOLOR% 06/22/12 18:08:42 Number of idle job procs: 0 06/22/12 18:08:42 Event: ULOG_IMAGE_SIZE for Condor Node montage (82.0.0) 06/22/12 18:08:42 Event: ULOG_JOB_TERMINATED for Condor Node montage (82.0.0) %RED%06/22/12 18:08:42 Node montage job proc (82.0.0) failed with status 1.%ENDCOLOR% 06/22/12 18:08:42 Number of idle job procs: 0 06/22/12 18:08:42 Of 5 nodes total: 06/22/12 18:08:42 Done Pre Queued Post Ready Un-Ready Failed 06/22/12 18:08:42 === === === === === === === 06/22/12 18:08:42 4 0 0 0 0 0 1 06/22/12 18:08:42 0 job proc(s) currently held 06/22/12 18:08:42 Aborting DAG... 06/22/12 18:08:42 Writing Rescue DAG to goatbrot.dag.rescue001... 06/22/12 18:08:42 Note: 0 total job deferrals because of -MaxJobs limit (0) 06/22/12 18:08:42 Note: 0 total job deferrals because of -MaxIdle limit (0) 06/22/12 18:08:42 Note: 0 total job deferrals because of node category throttles 06/22/12 18:08:42 Note: 0 total PRE script deferrals because of -MaxPre limit (0) 06/22/12 18:08:42 Note: 0 total POST script deferrals because of -MaxPost limit (0) %RED%06/22/12 18:08:42 **** condor_scheduniv_exec.77.0 (condor_DAGMAN) pid 26867 EXITING WITH STATUS 1%ENDCOLOR% DAGMan notices that one of the jobs failed because it's exit code was non-zero. DAGMan ran as much of the DAG as possible and logged enough information to continue the run when the situation is resolved. Do you see the part where it wrote the resuce DAG? Look at the rescue DAG. It's called a partial DAG: it indicates what part of the DAG has already been completed. When you re-submit the original DAG, DAGMan will notice the rescue DAG and use it in combination with the original DAG. (The rescue DAG used to be the full DAG with nodes marked as done and you would ask DAGMan to run the new rescue DAG. For your simplicity DAGMan lets you resubmit the original DAG and it reads both files.) % cat goatbrot.dag.rescue001 # Rescue DAG file, created after running # the goatbrot.dag DAG file # Created 6/22/2012 23:08:42 UTC %RED%# Rescue DAG version: 2.0.1 (partial)%ENDCOLOR% # # Total number of Nodes: 5 # Nodes premarked DONE: 4 %RED%# Nodes that failed: 1 # montage,<ENDLIST>%ENDCOLOR% DONE g1 DONE g2 DONE g3 DONE g4 From the comment near the top, we know that the montage node failed. Let's fix it by getting rid of the offending -h argument. Change montage.sub to look like: universe = vanilla executable = /usr/bin/montage arguments = tile_0_0.ppm tile_0_1.ppm tile_1_0.ppm tile_1_1.ppm -mode Concatenate -tile 2x2 mandle.jpg transfer_input_files = tile_0_0.ppm,tile_0_1.ppm,tile_1_0.ppm,tile_1_1.ppm transfer_executable = false output = montage.out error = montage.err log = goat.log request_memory = 1G request_disk = 1G request_cpus = 1 queue Now we can re-submit our original DAG and DAGMan will pick up where it left off. It will automatically notice the rescue DAG If you didn't fix the problem, DAGMan would generate another rescue DAG. % condor_submit_dag goatbrot.dag %RED%Running rescue DAG 1%ENDCOLOR% ----------------------------------------------------------------------- File for submitting this DAG to Condor : goatbrot.dag.condor.sub Log of DAGMan debugging messages : goatbrot.dag.dagman.out Log of Condor library output : goatbrot.dag.lib.out Log of Condor library error messages : goatbrot.dag.lib.err Log of the life of condor_dagman itself : goatbrot.dag.dagman.log Submitting job(s). 1 job(s) submitted to cluster 83. ----------------------------------------------------------------------- % tail -f goatbrot.dag.dagman.out 06/23/12 11:30:53 ****************************************************** 06/23/12 11:30:53 ** condor_scheduniv_exec.83.0 (CONDOR_DAGMAN) STARTING UP 06/23/12 11:30:53 ** /usr/bin/condor_dagman 06/23/12 11:30:53 ** SubsystemInfo: name=DAGMAN type=DAGMAN(10) class=DAEMON(1) 06/23/12 11:30:53 ** Configuration: subsystem:DAGMAN local:<NONE> class:DAEMON 06/23/12 11:30:53 ** $CondorVersion: 7.7.6 Apr 16 2012 BuildID: 34175 PRE-RELEASE-UWCS $ 06/23/12 11:30:53 ** $CondorPlatform: x86_64_rhap_5.7 $ 06/23/12 11:30:53 ** PID = 28576 06/23/12 11:30:53 ** Log last touched 6/22 18:08:42 06/23/12 11:30:53 ****************************************************** 06/23/12 11:30:53 Using config source: /etc/condor/condor_config ... %RED%Here is where DAGMAN notices that there is a rescue DAG%ENDCOLOR% 06/23/12 11:30:53 Parsing 1 dagfiles 06/23/12 11:30:53 Parsing goatbrot.dag ... %RED%06/23/12 11:30:53 Found rescue DAG number 1; running goatbrot.dag.rescue001 in combination with normal DAG file%ENDCOLOR% 06/23/12 11:30:53 ~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~~ 06/23/12 11:30:53 USING RESCUE DAG goatbrot.dag.rescue001 06/23/12 11:30:53 Dag contains 5 total jobs %RED%Shortly thereafter it sees that four jobs have already finished.%ENDCOLOR% 06/23/12 11:31:05 Bootstrapping... 06/23/12 11:31:05 Number of pre-completed nodes: 4 06/23/12 11:31:05 Registering condor_event_timer... 06/23/12 11:31:06 Sleeping for one second for log file consistency 06/23/12 11:31:07 MultiLogFiles: truncating log file /home/roy/condor/goatbrot/montage.log %RED%Here is where DAGMan resubmits the montage job and waits for it to complete.%ENDCOLOR% 06/23/12 11:31:07 Submitting Condor Node montage job(s)... 06/23/12 11:31:07 submitting: condor_submit -a dag_node_name' '=' 'montage -a +DAGManJobId' '=' '83 -a DAGManJobId' '=' '83 -a submit_event_notes' '=' 'DAG' 'Node:' 'montage -a DAG_STATUS' '=' '0 -a FAILED_COUNT' '=' '0 -a +DAGParentNodeNames' '=' '\"g1,g2,g3,g4\" montage.sub 06/23/12 11:31:07 From submit: Submitting job(s). 06/23/12 11:31:07 From submit: 1 job(s) submitted to cluster 84. 06/23/12 11:31:07 assigned Condor ID (84.0.0) 06/23/12 11:31:07 Just submitted 1 job this cycle... 06/23/12 11:31:07 Currently monitoring 1 Condor log file(s) 06/23/12 11:31:07 Event: ULOG_SUBMIT for Condor Node montage (84.0.0) 06/23/12 11:31:07 Number of idle job procs: 1 06/23/12 11:31:07 Of 5 nodes total: 06/23/12 11:31:07 Done Pre Queued Post Ready Un-Ready Failed 06/23/12 11:31:07 === === === === === === === 06/23/12 11:31:07 4 0 1 0 0 0 0 06/23/12 11:31:07 0 job proc(s) currently held 06/23/12 11:40:22 Currently monitoring 1 Condor log file(s) 06/23/12 11:40:22 Event: ULOG_EXECUTE for Condor Node montage (84.0.0) 06/23/12 11:40:22 Number of idle job procs: 0 06/23/12 11:40:22 Event: ULOG_IMAGE_SIZE for Condor Node montage (84.0.0) 06/23/12 11:40:22 Event: ULOG_JOB_TERMINATED for Condor Node montage (84.0.0) %RED%This is where the montage finished.%ENDCOLOR% 06/23/12 11:40:22 Node montage job proc (84.0.0) completed successfully. 06/23/12 11:40:22 Node montage job completed 06/23/12 11:40:22 Number of idle job procs: 0 06/23/12 11:40:22 Of 5 nodes total: 06/23/12 11:40:22 Done Pre Queued Post Ready Un-Ready Failed 06/23/12 11:40:22 === === === === === === === 06/23/12 11:40:22 5 0 0 0 0 0 0 06/23/12 11:40:22 0 job proc(s) currently held %RED%And here DAGMan decides that the work is all done.%ENDCOLOR% 06/23/12 11:40:22 All jobs Completed! 06/23/12 11:40:22 Note: 0 total job deferrals because of -MaxJobs limit (0) 06/23/12 11:40:22 Note: 0 total job deferrals because of -MaxIdle limit (0) 06/23/12 11:40:22 Note: 0 total job deferrals because of node category throttles 06/23/12 11:40:22 Note: 0 total PRE script deferrals because of -MaxPre limit (0) 06/23/12 11:40:22 Note: 0 total POST script deferrals because of -MaxPost limit (0) 06/23/12 11:40:22 **** condor_scheduniv_exec.83.0 (condor_DAGMAN) pid 28576 EXITING WITH STATUS 0 Success! Now go ahead and clean up.","title":"Breaking Things"},{"location":"materials/day1/part4-ex1-failed-dag/#challenge","text":"If you have time, add an extra node to the DAG. Copy our original \"simple\" program, but make it exit with a 1 instead of a 0. DAGMan would consider this a failure, but you'll tell DAGMan that it's really a success. This is reasonable--many real world programs use a variety of return codes, and you might need to help DAGMan distinguish success from failure. Write a POST script that checks the return value. Check the Condor manual to see how to describe your post script.","title":"Challenge"},{"location":"materials/day1/part4-ex2-dag-vars/","text":"Simpler DAGs with variable substitutions \u00b6 The objective of this exercise is to help you write simpler DAGs by using variable substitutions in your submit files. If you look at the DAG we made, you might find it a bit tedious because each goatbrot job has a separate HTCondor submit file. They are nearly identical except for a couple of parameters. Can we make it simpler? Yes, we can! Declare Your Variables \u00b6 First you need to declare your variables in your submit file. Make one submit file for all of your goatbrot jobs. Here's what it looks like. Call it goatbrot.sub: executable = /usr/local/bin/goatbrot arguments = -i 100000 -c $(CENTERX),$(CENTERY) -w 1.5 -s 500,500 -o tile_$(TILEY)_$(TILEX).ppm log = goatbrot.log output = goatbrot.out.$(TILEY).$(TILEX) error = goatbrot.err.$(TILEY).$(TILEX) request_memory = 1G request_disk = 1G request_cpus = 1 queue Then you need to change your DAG to use VARS for variable substitution. Here is what one of the jobs would look like: JOB g1 goatbrot.sub VARS g1 CENTERX=\"-0.75\" VARS g1 CENTERY=\"0.75\" VARS g1 TILEX=\"0\" VARS g1 TILEY=\"0\" Edit your DAG similarly for all of your goatbrot jobs. If you need help, check the Condor manual for for a description of how to use VARS . What happens?","title":"4.2. DAG variables"},{"location":"materials/day1/part4-ex2-dag-vars/#simpler-dags-with-variable-substitutions","text":"The objective of this exercise is to help you write simpler DAGs by using variable substitutions in your submit files. If you look at the DAG we made, you might find it a bit tedious because each goatbrot job has a separate HTCondor submit file. They are nearly identical except for a couple of parameters. Can we make it simpler? Yes, we can!","title":"Simpler DAGs with variable substitutions"},{"location":"materials/day1/part4-ex2-dag-vars/#declare-your-variables","text":"First you need to declare your variables in your submit file. Make one submit file for all of your goatbrot jobs. Here's what it looks like. Call it goatbrot.sub: executable = /usr/local/bin/goatbrot arguments = -i 100000 -c $(CENTERX),$(CENTERY) -w 1.5 -s 500,500 -o tile_$(TILEY)_$(TILEX).ppm log = goatbrot.log output = goatbrot.out.$(TILEY).$(TILEX) error = goatbrot.err.$(TILEY).$(TILEX) request_memory = 1G request_disk = 1G request_cpus = 1 queue Then you need to change your DAG to use VARS for variable substitution. Here is what one of the jobs would look like: JOB g1 goatbrot.sub VARS g1 CENTERX=\"-0.75\" VARS g1 CENTERY=\"0.75\" VARS g1 TILEX=\"0\" VARS g1 TILEY=\"0\" Edit your DAG similarly for all of your goatbrot jobs. If you need help, check the Condor manual for for a description of how to use VARS . What happens?","title":"Declare Your Variables"},{"location":"materials/day1/part4-ex3-dag-splice/","text":"Composing Larger DAGs From Simpler DAGs With SPLICE \u00b6 The objective of this exercise is to show how to build up sophisticated DAGs built from smaller DAG files without changing the original files. The basis of a lot of computer programming is to work up a small example, get it correct, and reuse it a building block for more sophisticated programs. The same is true for DAG files, and SPLICE is one way to do so. We are going to use the Mandelbrot DAG from our previous example, and use that as a \u201cblack box\u201d, so that we can add on to it, without changing that DAG at all. Name your SPLICE subroutine \u00b6 Our previously created DAG file is now going to be a SPLICE, or subroutine. We first need to create a master DAG file, which will call this as a splice, and name it. Let's call the splice MONTAGE. We will simple compute the size of the montage file we created in the previous DAG, with a new node which we will tack on to the last node in the previous file. We'll run the wc (word count) command with the -c argument to do so. The new submit file should look something like this; executable = /usr/bin/wc arguments = -c montage.out log = wc.log output = wc.out error = wc.err transfer_input_files = montage.out request_memory = 1G request_disk = 1G request_cpus = 1 queue By now, you should be able to create a simple dag file whose parent is the previous DAG (as a splice), and whose child is just this one node.. If you need help, check the HTCondor manual for for a description of how to use SPLICE .","title":"4.3. DAG SPLICE"},{"location":"materials/day1/part4-ex3-dag-splice/#composing-larger-dags-from-simpler-dags-with-splice","text":"The objective of this exercise is to show how to build up sophisticated DAGs built from smaller DAG files without changing the original files. The basis of a lot of computer programming is to work up a small example, get it correct, and reuse it a building block for more sophisticated programs. The same is true for DAG files, and SPLICE is one way to do so. We are going to use the Mandelbrot DAG from our previous example, and use that as a \u201cblack box\u201d, so that we can add on to it, without changing that DAG at all.","title":"Composing Larger DAGs From Simpler DAGs With SPLICE"},{"location":"materials/day1/part4-ex3-dag-splice/#name-your-splice-subroutine","text":"Our previously created DAG file is now going to be a SPLICE, or subroutine. We first need to create a master DAG file, which will call this as a splice, and name it. Let's call the splice MONTAGE. We will simple compute the size of the montage file we created in the previous DAG, with a new node which we will tack on to the last node in the previous file. We'll run the wc (word count) command with the -c argument to do so. The new submit file should look something like this; executable = /usr/bin/wc arguments = -c montage.out log = wc.log output = wc.out error = wc.err transfer_input_files = montage.out request_memory = 1G request_disk = 1G request_cpus = 1 queue By now, you should be able to create a simple dag file whose parent is the previous DAG (as a splice), and whose child is just this one node.. If you need help, check the HTCondor manual for for a description of how to use SPLICE .","title":"Name your SPLICE subroutine"},{"location":"materials/day1/part4-ex4-challenges/","text":"Challenge 1 \u00b6 Do you have any extra computation that needs to be done? Real work, from your life outside this summer school? If so, try it out on our HTCondor pool. Can't think of something? How about one of the existing distributed computing programs like distributed.net , SETI@home , Einstien@Home or others that you know. We prefer that you do your own work rather than one of these projects, but they are options. Challenge 2 \u00b6 Try to generate other Mandelbrot images. Some possible locations to look at with goatbroat: goatbrot -i 1000 -o ex1.ppm -c 0.0016437219722,-0.8224676332988 -w 2e-11 -s 1000,1000 goatbrot -i 1000 -o ex2.ppm -c 0.3958608398437499,-0.13431445312500012 -w 0.0002197265625 -s 1000,1000 goatbrot -i 1000 -o ex3.ppm -c 0.3965859374999999,-0.13378125000000013 -w 0.003515625 -s 1000,1000 You can convert ppm files with convert , like so: convert ex1.ppm ex1.jpg Now make a movie! Make a series of images where you zoom into a point in the Mandelbrot set gradually. (Those points above may work well.) Assemble these images with the \"convert\" tool which will let you convert a set of JPEG files into an MPEG movie. Challenge 3 \u00b6 Try out Pegasus . Pegasus is a workflow manager that uses DAGMan and can work in a grid environment and/or run across different types of clusters (with other queueing software). It will create the DAGs from abstract DAG descriptions and ensure they are appropriate for the location of the data and computation. Links to more information: Pegasus Website Pegasus Documentation Useful Tutorial Pegasus on OSG Connect (covered Thursday) If you have any questions or problems, please feel free to contact the Pegasus team by emailing pegasus-support@isi.edu Challenge 4 \u00b6 Try out Makeflow . You can think of it acting like DAGMan, but there several intriguing differences. Now that you understand DAGMan, you should think about how it compares to Makeflow. Why would you use DAGMan or Makeflow? These extra exercises for Makeflow were kindly provided by Doug Thain. Note: Be Nice \u00b6 Please be polite. Computers in our Condor pool will run multiple jobs at a time, and these computers are shared with your fellow students. If you have jobs that will use significant computational power or memory, limit your jobs to be kind to your neighbors, unless you run your jobs during off-hours.","title":"4.4. Bonus challenges"},{"location":"materials/day1/part4-ex4-challenges/#challenge-1","text":"Do you have any extra computation that needs to be done? Real work, from your life outside this summer school? If so, try it out on our HTCondor pool. Can't think of something? How about one of the existing distributed computing programs like distributed.net , SETI@home , Einstien@Home or others that you know. We prefer that you do your own work rather than one of these projects, but they are options.","title":"Challenge 1"},{"location":"materials/day1/part4-ex4-challenges/#challenge-2","text":"Try to generate other Mandelbrot images. Some possible locations to look at with goatbroat: goatbrot -i 1000 -o ex1.ppm -c 0.0016437219722,-0.8224676332988 -w 2e-11 -s 1000,1000 goatbrot -i 1000 -o ex2.ppm -c 0.3958608398437499,-0.13431445312500012 -w 0.0002197265625 -s 1000,1000 goatbrot -i 1000 -o ex3.ppm -c 0.3965859374999999,-0.13378125000000013 -w 0.003515625 -s 1000,1000 You can convert ppm files with convert , like so: convert ex1.ppm ex1.jpg Now make a movie! Make a series of images where you zoom into a point in the Mandelbrot set gradually. (Those points above may work well.) Assemble these images with the \"convert\" tool which will let you convert a set of JPEG files into an MPEG movie.","title":"Challenge 2"},{"location":"materials/day1/part4-ex4-challenges/#challenge-3","text":"Try out Pegasus . Pegasus is a workflow manager that uses DAGMan and can work in a grid environment and/or run across different types of clusters (with other queueing software). It will create the DAGs from abstract DAG descriptions and ensure they are appropriate for the location of the data and computation. Links to more information: Pegasus Website Pegasus Documentation Useful Tutorial Pegasus on OSG Connect (covered Thursday) If you have any questions or problems, please feel free to contact the Pegasus team by emailing pegasus-support@isi.edu","title":"Challenge 3"},{"location":"materials/day1/part4-ex4-challenges/#challenge-4","text":"Try out Makeflow . You can think of it acting like DAGMan, but there several intriguing differences. Now that you understand DAGMan, you should think about how it compares to Makeflow. Why would you use DAGMan or Makeflow? These extra exercises for Makeflow were kindly provided by Doug Thain.","title":"Challenge 4"},{"location":"materials/day1/part4-ex4-challenges/#note-be-nice","text":"Please be polite. Computers in our Condor pool will run multiple jobs at a time, and these computers are shared with your fellow students. If you have jobs that will use significant computational power or memory, limit your jobs to be kind to your neighbors, unless you run your jobs during off-hours.","title":"Note: Be Nice"},{"location":"materials/day2/part1-ex1-submit-refresher/","text":"Tuesday Exercise 1.1: Refresher \u2013 Submitting Multiple Jobs \u00b6 The goal of this exercise is to map the physical locations of some worker nodes in our local cluster. To do this, you will write a simple submit file that will queue multiple jobs and then manually collate the results. Where in the world are my jobs? \u00b6 To find the physical location of the computers your jobs our running on, you will use a method called geolocation . Geolocation uses a registry to match a computer\u2019s network address to an approximate latitude and longitude. Geolocation code \u00b6 Below is a Python script that geolocates the machine that it is running on, returning latitude and longitude coordinates . It is not important to understand how the internals of the script works. #!/bin/env python import sys import socket import urllib2 import json import time hostnames = set() if len(sys.argv) == 1: hostnames.add(socket.getfqdn()) else: for filename in sys.argv[1:]: try: with open(filename, 'r') as f: hostnames.update(set([x.strip() for x in f.readlines()])) except IOError: pass hostnames.discard('') for host in hostnames: try: ipaddr = socket.gethostbyname(host) for i in range(1,4): try: response = urllib2.urlopen('http://www.freegeoip.net/json/' + ipaddr).read() json_response = json.loads(response) lat = json_response['latitude'] lon = json_response['longitude'] if int(lat) is 0 and int(lon) is 0: exit(\"www.freegeoip.net could not find associated lat/lon coordinates.\") print \"%s,%s\" % (lat, lon) break except urllib2.HTTPError: time.sleep(3**i) pass except socket.gaierror: pass When called without any arguments, the above script will return the latitude and longitude coordinates of the machine that it is run on. Geolocating several machines \u00b6 Now, let\u2019s try to use this Python script and remember some basic HTCondor ideas from yesterday! Log on to learn.chtc.wisc.edu Create and change into a new folder for this exercise, for example tuesday-1.1 Save the Python script above as a file named location.py As always, ensure that your script has the proper permissions (hint: try running it from the command line) Create a submit file that generates ten jobs that run location.py and uses the $(Process) macro to write different output and error files. Try to do this step without looking at materials from yesterday. But if you are stuck, see yesterday\u2019s exercise 2.4 . Submit your jobs and wait for the results Collating your results \u00b6 Now that you have your results, it's time to summarize them. Rather than inspecting each output file individually, you can use the cat command to print the results from all of your output files at once. If all of your output files have the format location-#.out (e.g., location-10.out ), your command will look something like this: %UCL_PROMPT_SHORT% <strong>cat location-*.out</strong> The * is a wildcard so the above cat command runs on all files that start with location- and end in .out . Additionally, you can use cat in tandem with the sort and uniq commands to print only the unique results: %UCL_PROMPT_SHORT% <strong>cat location-*.out | sort | uniq</strong> Mapping your results \u00b6 To visualize the locations of the machines that your jobs ran on, you will be using http://www.mapcustomizer.com/ . Copy and paste the collated results into the text box that pops up when clicking on the 'Bulk Entry' button on the right-hand side. Are the results what you expected? Next exercise \u00b6 Once completed, move onto the next exercise: Logging in to the OSG Submit Machine","title":"1.1. Refresher"},{"location":"materials/day2/part1-ex1-submit-refresher/#tuesday-exercise-11-refresher-submitting-multiple-jobs","text":"The goal of this exercise is to map the physical locations of some worker nodes in our local cluster. To do this, you will write a simple submit file that will queue multiple jobs and then manually collate the results.","title":"Tuesday Exercise 1.1: Refresher \u2013 Submitting Multiple Jobs"},{"location":"materials/day2/part1-ex1-submit-refresher/#where-in-the-world-are-my-jobs","text":"To find the physical location of the computers your jobs our running on, you will use a method called geolocation . Geolocation uses a registry to match a computer\u2019s network address to an approximate latitude and longitude.","title":"Where in the world are my jobs?"},{"location":"materials/day2/part1-ex1-submit-refresher/#geolocation-code","text":"Below is a Python script that geolocates the machine that it is running on, returning latitude and longitude coordinates . It is not important to understand how the internals of the script works. #!/bin/env python import sys import socket import urllib2 import json import time hostnames = set() if len(sys.argv) == 1: hostnames.add(socket.getfqdn()) else: for filename in sys.argv[1:]: try: with open(filename, 'r') as f: hostnames.update(set([x.strip() for x in f.readlines()])) except IOError: pass hostnames.discard('') for host in hostnames: try: ipaddr = socket.gethostbyname(host) for i in range(1,4): try: response = urllib2.urlopen('http://www.freegeoip.net/json/' + ipaddr).read() json_response = json.loads(response) lat = json_response['latitude'] lon = json_response['longitude'] if int(lat) is 0 and int(lon) is 0: exit(\"www.freegeoip.net could not find associated lat/lon coordinates.\") print \"%s,%s\" % (lat, lon) break except urllib2.HTTPError: time.sleep(3**i) pass except socket.gaierror: pass When called without any arguments, the above script will return the latitude and longitude coordinates of the machine that it is run on.","title":"Geolocation code"},{"location":"materials/day2/part1-ex1-submit-refresher/#geolocating-several-machines","text":"Now, let\u2019s try to use this Python script and remember some basic HTCondor ideas from yesterday! Log on to learn.chtc.wisc.edu Create and change into a new folder for this exercise, for example tuesday-1.1 Save the Python script above as a file named location.py As always, ensure that your script has the proper permissions (hint: try running it from the command line) Create a submit file that generates ten jobs that run location.py and uses the $(Process) macro to write different output and error files. Try to do this step without looking at materials from yesterday. But if you are stuck, see yesterday\u2019s exercise 2.4 . Submit your jobs and wait for the results","title":"Geolocating several machines"},{"location":"materials/day2/part1-ex1-submit-refresher/#collating-your-results","text":"Now that you have your results, it's time to summarize them. Rather than inspecting each output file individually, you can use the cat command to print the results from all of your output files at once. If all of your output files have the format location-#.out (e.g., location-10.out ), your command will look something like this: %UCL_PROMPT_SHORT% <strong>cat location-*.out</strong> The * is a wildcard so the above cat command runs on all files that start with location- and end in .out . Additionally, you can use cat in tandem with the sort and uniq commands to print only the unique results: %UCL_PROMPT_SHORT% <strong>cat location-*.out | sort | uniq</strong>","title":"Collating your results"},{"location":"materials/day2/part1-ex1-submit-refresher/#mapping-your-results","text":"To visualize the locations of the machines that your jobs ran on, you will be using http://www.mapcustomizer.com/ . Copy and paste the collated results into the text box that pops up when clicking on the 'Bulk Entry' button on the right-hand side. Are the results what you expected?","title":"Mapping your results"},{"location":"materials/day2/part1-ex1-submit-refresher/#next-exercise","text":"Once completed, move onto the next exercise: Logging in to the OSG Submit Machine","title":"Next exercise"},{"location":"materials/day2/part1-ex2-login-scp/","text":"Tuesday Exercise 1.2: Log in to the OSG Submit Machine \u00b6 The goal of this exercise is to log on to a different submit host so that you can start submitting jobs into the OSG instead of the local cluster here at UW-Madison. Additionally, you will learn about the tar and scp commands, which will allow you to efficiently copy files between the two submit nodes. If you have trouble getting ssh access to the submit machine, ask the instructors right away! Gaining access is critical for all remaining exercises. Log in to the OSG submit machine \u00b6 For some of the remaining exercises today, you will be using a machine named osg-learn.chtc.wisc.edu . The username and password are listed on your 'Accounts' paper that you received yesterday. If you no longer have it, please ask the instructors for help. Once you have your account details, ssh in to the machine and take a look around. Preparing files for transfer \u00b6 When transferring files between computers, it's best to limit the number of files as well as their size. Smaller files transfer more quickly and if your network connection drops, restarting the transfer is less painful than it would be if you were transferring large files. Archiving tools (WinZip, 7zip, Archive Utility, etc.) can compress the size of your files and place them into a single, smaller archive file. The tar command is a one-stop shop for creating, extracting, and viewing the contents of tar archives (called tarballs) whose usage is as follows: To create a tarball named <archive filename> containing <archive contents> , use the following command: \\ %UCL_PROMPT_SHORT% <strong>tar -czvf <archive filename> <archive contents></strong> \\ Where <archive filename> should end in .tar.gz and <archive contents> can be a list of any number of files and/or folders, separated by spaces. To extract the files from a tarball into the current directory: \\ %UCL_PROMPT_SHORT% <strong>tar -xzvf <archive filename></strong> To list the files within a tarball: \\ %UCL_PROMPT_SHORT% <strong>tar -tzvf <archive filename></strong> Using the above knowledge, log into learn.chtc.wisc.edu , create a tarball that contains Monday's exercise 2.4 directory, and verify that it contains all the proper files. Comparing compressed sizes \u00b6 You can adjust the level of compression of tar by prepending your command with GZIP=--<span style=\"background-color: #FFCCFF;\"><COMPRESSION></span> , where <span style=\"background-color: #FFCCFF;\"><COMPRESSION></span> can be either fast for the least compression, or best for the most compression (the default compression is between best and fast ). Use wget to download the following files from our web server: Text file: http://proxy.chtc.wisc.edu/SQUID/osgschool17/random_text Archive: http://proxy.chtc.wisc.edu/SQUID/osgschool17/pdbaa.tar.gz Image: http://proxy.chtc.wisc.edu/SQUID/osgschool17/obligatory_cat.jpg Use tar on each file and compare the sizes of the original file and the compressed version. Which files were compressed the least? Why? Transferring files \u00b6 Using secure copy \u00b6 Secure copy ( scp ) is a command based on SSH that lets you securely copy files between two different hosts. It takes similar arguments to the cp command that you are familiar with but also takes additional host information: %UCL_PROMPT_SHORT% <strong>scp <source 1> <source 2>...<source N> <remote host>:<remote path> </strong> For example, if I were logged in to learn.chtc.wisc.edu and wanted to copy the file foo from my current directory to my home directory on osg-learn.chtc.wisc.edu , the command would look like this: %UCL_PROMPT_SHORT% <strong>scp foo osg-learn.chtc.wisc.edu:~ </strong> Additionally, I could also pull files from osg-learn.chtc.wisc.edu to learn.chtc.wisc.edu . The following command copies bar from my home directory on osg-learn.chtc.wisc.edu to my current directory on learn.chtc.wisc.edu : %UCL_PROMPT_SHORT% <strong>scp osg-learn.chtc.wisc.edu:~/bar . </strong> You can also copy folders between hosts using the -r option. If I kept all my files from Monday's exercise 1.3 in a folder named monday-1.3 on learn.chtc.wisc.edu , I could use the following command to copy them to my home directory on osg-learn.chtc.wisc.edu : %UCL_PROMPT_SHORT% <strong>scp -r monday-1.3 osg-learn.chtc.wisc.edu:~ </strong> Try copying the tarball you created earlier in this exercise on learn.chtc.wisc.edu to osg-learn.chtc.wisc.edu . Secure copy from your laptop \u00b6 During your research, you may need to retrieve output files from your submit host to inspect them on your personal machine, which can also be done with scp ! To use scp on your laptop, follow the instructions relevant to your machine's operating system: Mac and Linux users \u00b6 scp should be included by default and available via the terminal on both Mac and Linux operating systems. Open a terminal window on your laptop and try copying the tarball containing Monday's 2.4 exercise from osg-learn.chtc.wisc.edu to your laptop. Windows users \u00b6 WinSCP is an scp client for Windows operating systems. Install WinSCP from https://winscp.net/eng/index.php Start WinSCP and enter your SSH credentials for osg-learn.chtc.wisc.edu Copy the tarball containing Monday's 2.4 exercise exercise to your laptop Extra challenge: Using rsync \u00b6 scp is a great, ubiquitous tool for one-time transfers but if you find yourself transferring the same set of files to the same location repeatedly, there are better tools to use. Another common tool available on many linux machines is rsync , which is like a beefed-up version of scp . The invocation is similar to scp : you can transfer files and/or folders, but the options are different and when transferring folders, make sure they don't have a trailing slash ( / , this means to copy all the files within the folder instead of the folder itself): %UCL_PROMPT_SHORT% <strong>rsync -Pavz <source 1> <source 2>...<source N> <remote host>:<remote path> </strong> rsync has many benefits over scp but two of its biggest features are built-in compression (so you don't have to create a tarball) and the ability to only transfer files that have changed. Both of these feature are helpful when you're having connectivity issues so that you don't have to restart the transfer from scratch every time your connection fails. Use rsync to transfer the folder containing today's exercise 1.1 to osg-learn.chtc.wisc.edu Create a new file in your exercise 1.1 folder on learn.chtc.wisc.edu with the touch command: \\ touch Use the same rsync command to transfer the folder with the new file you just created. How many files were transferred the first time? How many files were transferred if you run the same rsync command again? Next exercise \u00b6 Once completed, move onto the next exercise: Running jobs in the OSG","title":"1.2. Submit host"},{"location":"materials/day2/part1-ex2-login-scp/#tuesday-exercise-12-log-in-to-the-osg-submit-machine","text":"The goal of this exercise is to log on to a different submit host so that you can start submitting jobs into the OSG instead of the local cluster here at UW-Madison. Additionally, you will learn about the tar and scp commands, which will allow you to efficiently copy files between the two submit nodes. If you have trouble getting ssh access to the submit machine, ask the instructors right away! Gaining access is critical for all remaining exercises.","title":"Tuesday Exercise 1.2: Log in to the OSG Submit Machine"},{"location":"materials/day2/part1-ex2-login-scp/#log-in-to-the-osg-submit-machine","text":"For some of the remaining exercises today, you will be using a machine named osg-learn.chtc.wisc.edu . The username and password are listed on your 'Accounts' paper that you received yesterday. If you no longer have it, please ask the instructors for help. Once you have your account details, ssh in to the machine and take a look around.","title":"Log in to the OSG submit machine"},{"location":"materials/day2/part1-ex2-login-scp/#preparing-files-for-transfer","text":"When transferring files between computers, it's best to limit the number of files as well as their size. Smaller files transfer more quickly and if your network connection drops, restarting the transfer is less painful than it would be if you were transferring large files. Archiving tools (WinZip, 7zip, Archive Utility, etc.) can compress the size of your files and place them into a single, smaller archive file. The tar command is a one-stop shop for creating, extracting, and viewing the contents of tar archives (called tarballs) whose usage is as follows: To create a tarball named <archive filename> containing <archive contents> , use the following command: \\ %UCL_PROMPT_SHORT% <strong>tar -czvf <archive filename> <archive contents></strong> \\ Where <archive filename> should end in .tar.gz and <archive contents> can be a list of any number of files and/or folders, separated by spaces. To extract the files from a tarball into the current directory: \\ %UCL_PROMPT_SHORT% <strong>tar -xzvf <archive filename></strong> To list the files within a tarball: \\ %UCL_PROMPT_SHORT% <strong>tar -tzvf <archive filename></strong> Using the above knowledge, log into learn.chtc.wisc.edu , create a tarball that contains Monday's exercise 2.4 directory, and verify that it contains all the proper files.","title":"Preparing files for transfer"},{"location":"materials/day2/part1-ex2-login-scp/#comparing-compressed-sizes","text":"You can adjust the level of compression of tar by prepending your command with GZIP=--<span style=\"background-color: #FFCCFF;\"><COMPRESSION></span> , where <span style=\"background-color: #FFCCFF;\"><COMPRESSION></span> can be either fast for the least compression, or best for the most compression (the default compression is between best and fast ). Use wget to download the following files from our web server: Text file: http://proxy.chtc.wisc.edu/SQUID/osgschool17/random_text Archive: http://proxy.chtc.wisc.edu/SQUID/osgschool17/pdbaa.tar.gz Image: http://proxy.chtc.wisc.edu/SQUID/osgschool17/obligatory_cat.jpg Use tar on each file and compare the sizes of the original file and the compressed version. Which files were compressed the least? Why?","title":"Comparing compressed sizes"},{"location":"materials/day2/part1-ex2-login-scp/#transferring-files","text":"","title":"Transferring files"},{"location":"materials/day2/part1-ex2-login-scp/#using-secure-copy","text":"Secure copy ( scp ) is a command based on SSH that lets you securely copy files between two different hosts. It takes similar arguments to the cp command that you are familiar with but also takes additional host information: %UCL_PROMPT_SHORT% <strong>scp <source 1> <source 2>...<source N> <remote host>:<remote path> </strong> For example, if I were logged in to learn.chtc.wisc.edu and wanted to copy the file foo from my current directory to my home directory on osg-learn.chtc.wisc.edu , the command would look like this: %UCL_PROMPT_SHORT% <strong>scp foo osg-learn.chtc.wisc.edu:~ </strong> Additionally, I could also pull files from osg-learn.chtc.wisc.edu to learn.chtc.wisc.edu . The following command copies bar from my home directory on osg-learn.chtc.wisc.edu to my current directory on learn.chtc.wisc.edu : %UCL_PROMPT_SHORT% <strong>scp osg-learn.chtc.wisc.edu:~/bar . </strong> You can also copy folders between hosts using the -r option. If I kept all my files from Monday's exercise 1.3 in a folder named monday-1.3 on learn.chtc.wisc.edu , I could use the following command to copy them to my home directory on osg-learn.chtc.wisc.edu : %UCL_PROMPT_SHORT% <strong>scp -r monday-1.3 osg-learn.chtc.wisc.edu:~ </strong> Try copying the tarball you created earlier in this exercise on learn.chtc.wisc.edu to osg-learn.chtc.wisc.edu .","title":"Using secure copy"},{"location":"materials/day2/part1-ex2-login-scp/#secure-copy-from-your-laptop","text":"During your research, you may need to retrieve output files from your submit host to inspect them on your personal machine, which can also be done with scp ! To use scp on your laptop, follow the instructions relevant to your machine's operating system:","title":"Secure copy from your laptop"},{"location":"materials/day2/part1-ex2-login-scp/#mac-and-linux-users","text":"scp should be included by default and available via the terminal on both Mac and Linux operating systems. Open a terminal window on your laptop and try copying the tarball containing Monday's 2.4 exercise from osg-learn.chtc.wisc.edu to your laptop.","title":"Mac and Linux users"},{"location":"materials/day2/part1-ex2-login-scp/#windows-users","text":"WinSCP is an scp client for Windows operating systems. Install WinSCP from https://winscp.net/eng/index.php Start WinSCP and enter your SSH credentials for osg-learn.chtc.wisc.edu Copy the tarball containing Monday's 2.4 exercise exercise to your laptop","title":"Windows users"},{"location":"materials/day2/part1-ex2-login-scp/#extra-challenge-using-rsync","text":"scp is a great, ubiquitous tool for one-time transfers but if you find yourself transferring the same set of files to the same location repeatedly, there are better tools to use. Another common tool available on many linux machines is rsync , which is like a beefed-up version of scp . The invocation is similar to scp : you can transfer files and/or folders, but the options are different and when transferring folders, make sure they don't have a trailing slash ( / , this means to copy all the files within the folder instead of the folder itself): %UCL_PROMPT_SHORT% <strong>rsync -Pavz <source 1> <source 2>...<source N> <remote host>:<remote path> </strong> rsync has many benefits over scp but two of its biggest features are built-in compression (so you don't have to create a tarball) and the ability to only transfer files that have changed. Both of these feature are helpful when you're having connectivity issues so that you don't have to restart the transfer from scratch every time your connection fails. Use rsync to transfer the folder containing today's exercise 1.1 to osg-learn.chtc.wisc.edu Create a new file in your exercise 1.1 folder on learn.chtc.wisc.edu with the touch command: \\ touch Use the same rsync command to transfer the folder with the new file you just created. How many files were transferred the first time? How many files were transferred if you run the same rsync command again?","title":"Extra challenge: Using rsync"},{"location":"materials/day2/part1-ex2-login-scp/#next-exercise","text":"Once completed, move onto the next exercise: Running jobs in the OSG","title":"Next exercise"},{"location":"materials/day2/part1-ex3-submit-osg/","text":"Tuesday Exercise 1.3: Running jobs in the OSG \u00b6 The goal of this exercise is to have your jobs running on the OSG and map their geographical locations. Where in the world are my jobs? (Part 2) \u00b6 In this version of the geolocating exercise, you will be submitting jobs to the OSG from osg-learn.chtc.wisc.edu and hopefully getting back much more interesting results! Due to some differences between the machines in the OSG and our local cluster here at UW-Madison, you will be using a slightly different payload and then performing the geolocation on the results from the submit host. Hostname fetching code \u00b6 The following Python script finds the ClassAd of the machine it's running on and finds a network identity that can be used to perform lookups: #!/bin/env python import re import os import socket machine_ad_file_name = os.getenv('_CONDOR_MACHINE_AD') try: machine_ad_file = open(machine_ad_file_name, 'r') machine_ad = machine_ad_file.read() machine_ad_file.close() except TypeError: print socket.getfqdn() exit(1) try: print re.search(r'GLIDEIN_Gatekeeper = \"(.*):\\d*/jobmanager-\\w*\"', machine_ad, re.MULTILINE).group(1) except AttributeError: try: print re.search(r'GLIDEIN_Gatekeeper = \"(\\S+) \\S+:9619\"', machine_ad, re.MULTILINE).group(1) except AttributeError: exit(1) Gathering network information from the OSG \u00b6 Now to create submit files and that will run in the OSG! If not already logged in, ssh into osg-learn.chtc.wisc.edu Make a new directory for this exercise, tuesday-1.3 and change into it Save the above Python script to a file and call it ce_hostname.py Create a submit file that runs ce_hostname.py 100 times and uses the $(Process) macro to write different output and error files Submit your file and wait for the results Geolocating machines in the OSG \u00b6 NOTE: In this section, we are bending a rule about running jobs locally on the submit host because this host is a closed environment only utilized by the class and this exercise is designed for low load. Normally you should NOT run jobs on your submit host. You will be re-using the Python script from the the last exercise to perform the geolocation except instead of submitting it as a job, you will run it manually. Copy it over from learn.chtc.wisc.edu using scp with the following command: Log on to learn.chtc.wisc.edu Copy the file over to osg-learn.chtc.wisc.edu : \\ %UCL_PROMPT_SHORT% <strong>scp tuesday-1.1/location.py osg-learn.chtc.wisc.edu:tuesday-1.3/</strong> location.py can take a text file that contains a list of locations as an argument, which can be done by collating your output files into a single file. The easiest way to do this is to use the cat command from today's exercise 1.2, the * wildcard from last exercise and a new operator > , which can write command output to a file. If your output files are named ce_hostname-0.out...ce_hostname-99.out , your commands would look like this: %UCL_PROMPT_SHORT% <strong>cat ce_hostname-*.out > hostnames.txt</strong> %UCL_PROMPT_SHORT% <strong>./location.py hostnames.txt</strong> Mapping your jobs \u00b6 As before, you will be using http://www.mapcustomizer.com/ from osg-learn.chtc.wisc.edu to visualize where your jobs have landed in the OSG. Copy and paste the results from the Python script into the bulk creation text box at the bottom of the screen. Where did your jobs end up? Extra Challenge: Running it all as a DAG \u00b6 Yesterday, you learned about DAGs and you can take advantage of them to avoid manually running location.py by hand. You could make this exercise into DAG with the hostname fetching submit file followed by a submit file that calls the geolocation code. You'll need to leverage PRE and/or POST scripts to collate the hostname results, which can be done with a little bit of shell scripting! To create a shell script: Write a file with a .sh extension Add a bash \"shebang\" line to the top of the script and lines for each shell command that you'd like to run (imagine it as a terminal in file form). For example, the following shell script would create a directory foo then list its contents:\\ #/bin/bash mkdir foo ls foo Mark the script as executable Run it by hand to verify that it works What did your results look this time around via DAG? Can you make your two-node DAG even smaller with slight modifications to the shell-script and DAG (Hint: NOOP)?","title":"1.3. Jobs in OSG"},{"location":"materials/day2/part1-ex3-submit-osg/#tuesday-exercise-13-running-jobs-in-the-osg","text":"The goal of this exercise is to have your jobs running on the OSG and map their geographical locations.","title":"Tuesday Exercise 1.3: Running jobs in the OSG"},{"location":"materials/day2/part1-ex3-submit-osg/#where-in-the-world-are-my-jobs-part-2","text":"In this version of the geolocating exercise, you will be submitting jobs to the OSG from osg-learn.chtc.wisc.edu and hopefully getting back much more interesting results! Due to some differences between the machines in the OSG and our local cluster here at UW-Madison, you will be using a slightly different payload and then performing the geolocation on the results from the submit host.","title":"Where in the world are my jobs? (Part 2)"},{"location":"materials/day2/part1-ex3-submit-osg/#hostname-fetching-code","text":"The following Python script finds the ClassAd of the machine it's running on and finds a network identity that can be used to perform lookups: #!/bin/env python import re import os import socket machine_ad_file_name = os.getenv('_CONDOR_MACHINE_AD') try: machine_ad_file = open(machine_ad_file_name, 'r') machine_ad = machine_ad_file.read() machine_ad_file.close() except TypeError: print socket.getfqdn() exit(1) try: print re.search(r'GLIDEIN_Gatekeeper = \"(.*):\\d*/jobmanager-\\w*\"', machine_ad, re.MULTILINE).group(1) except AttributeError: try: print re.search(r'GLIDEIN_Gatekeeper = \"(\\S+) \\S+:9619\"', machine_ad, re.MULTILINE).group(1) except AttributeError: exit(1)","title":"Hostname fetching code"},{"location":"materials/day2/part1-ex3-submit-osg/#gathering-network-information-from-the-osg","text":"Now to create submit files and that will run in the OSG! If not already logged in, ssh into osg-learn.chtc.wisc.edu Make a new directory for this exercise, tuesday-1.3 and change into it Save the above Python script to a file and call it ce_hostname.py Create a submit file that runs ce_hostname.py 100 times and uses the $(Process) macro to write different output and error files Submit your file and wait for the results","title":"Gathering network information from the OSG"},{"location":"materials/day2/part1-ex3-submit-osg/#geolocating-machines-in-the-osg","text":"NOTE: In this section, we are bending a rule about running jobs locally on the submit host because this host is a closed environment only utilized by the class and this exercise is designed for low load. Normally you should NOT run jobs on your submit host. You will be re-using the Python script from the the last exercise to perform the geolocation except instead of submitting it as a job, you will run it manually. Copy it over from learn.chtc.wisc.edu using scp with the following command: Log on to learn.chtc.wisc.edu Copy the file over to osg-learn.chtc.wisc.edu : \\ %UCL_PROMPT_SHORT% <strong>scp tuesday-1.1/location.py osg-learn.chtc.wisc.edu:tuesday-1.3/</strong> location.py can take a text file that contains a list of locations as an argument, which can be done by collating your output files into a single file. The easiest way to do this is to use the cat command from today's exercise 1.2, the * wildcard from last exercise and a new operator > , which can write command output to a file. If your output files are named ce_hostname-0.out...ce_hostname-99.out , your commands would look like this: %UCL_PROMPT_SHORT% <strong>cat ce_hostname-*.out > hostnames.txt</strong> %UCL_PROMPT_SHORT% <strong>./location.py hostnames.txt</strong>","title":"Geolocating machines in the OSG"},{"location":"materials/day2/part1-ex3-submit-osg/#mapping-your-jobs","text":"As before, you will be using http://www.mapcustomizer.com/ from osg-learn.chtc.wisc.edu to visualize where your jobs have landed in the OSG. Copy and paste the results from the Python script into the bulk creation text box at the bottom of the screen. Where did your jobs end up?","title":"Mapping your jobs"},{"location":"materials/day2/part1-ex3-submit-osg/#extra-challenge-running-it-all-as-a-dag","text":"Yesterday, you learned about DAGs and you can take advantage of them to avoid manually running location.py by hand. You could make this exercise into DAG with the hostname fetching submit file followed by a submit file that calls the geolocation code. You'll need to leverage PRE and/or POST scripts to collate the hostname results, which can be done with a little bit of shell scripting! To create a shell script: Write a file with a .sh extension Add a bash \"shebang\" line to the top of the script and lines for each shell command that you'd like to run (imagine it as a terminal in file form). For example, the following shell script would create a directory foo then list its contents:\\ #/bin/bash mkdir foo ls foo Mark the script as executable Run it by hand to verify that it works What did your results look this time around via DAG? Can you make your two-node DAG even smaller with slight modifications to the shell-script and DAG (Hint: NOOP)?","title":"Extra Challenge: Running it all as a DAG"},{"location":"materials/day2/part2-ex1-hardware-diffs/","text":"Tuesday Exercise 2.1: Hardware Differences in the OSG \u00b6 The goal of this exercise is to compare hardware differences between our local cluster (CHTC here at UW\u2013Madison) and an OSG glidein pool. Specifically, we will look at how easy it is to get access to resources in terms of the amount of memory that is requested. This will not be a very careful study, but should give you some idea of one way in which the pools are different. In the first two parts of the exercise, you will submit a bunch of jobs that differ only in how much memory each one requests; we call this a parameter sweep , in that we are testing many possible values of a parameter. We will request memory from 1\u201316 GB, doubling the memory each time. One set of jobs will be submitted locally, and the other, identical set of jobs will be submitted to OSG. You will check the queue periodically to see how many jobs have completed and how many are still waiting to run. Part 1: Checking on the availability of memory (locally) \u00b6 In this first part, you will create the submit file for both the local and OSG jobs, then submit the local set. Yet another queue syntax \u00b6 Yesterday, you learned about the queue statement and some of the different ways it can be invoked to submit multiple jobs. Similar to the queue from statement to submit jobs based on lines from a specific file, you can use queue in to submit jobs based on a list directly from your submit file: queue <# of jobs> <variable> in ( <item 1> <item 2> <item 3> ... ) For example, to submit 6 jobs that sleep for 5 , 5 , 10 , 10 , 15 , and 15 seconds, you could write a submit file like the following: executable = /bin/sleep queue 2 arguments in ( 5 10 15 ) Try submitting this file yourself and checking the jobs that end up in the queue with condor_q -nobatch . Create the submit files \u00b6 To create our parameter sweep, we will create one submit file with multiple queue statements and change the value of our parameter ( request_memory ) for each batch of jobs. If not already, log on to learn.chtc.wisc.edu Create and change into a new subdirectory called tuesday-2.1 \u2013 doing things this way will make Part 2 much easier Create a submit file that is named sleep.sub and that executes the command =/bin/sleep 30=\\ If you do not remember all of the submit statements to write this file, or just to go faster, find a similar submit file from yesterday, copy and rename it here, and make sure the argument to sleep is 30 (although the exact value does not matter for the test). Use the queue in syntax to submit 25 jobs requesting 2, 4, 8, 16, and 32GB of memory, e.g. you should have 5 jobs requesting 2GB, 5 jobs requesting 4GB, etc. Save the submit file and exit your editor Submit your jobs Monitoring the local jobs \u00b6 Every few minutes, run condor_q and see how your sleep jobs are doing. To easily see how many jobs of each type you have left, run the following command: %UCL_PROMPT_SHORT% condor_q <span style=\"background-color: #D1CAF2;\"><Cluster ID></span> -af RequestMemory | sort -n | uniq -c The numbers in the left column are the number of jobs left of that type and the number on the right is the amount of memory you requested in MB. Consider making a little table like the one below to track progress. Memory Remaining #1 Remaining #2 Remaining #3 2 GB 5 3 4 GB 5 3 8 GB 5 4 16 GB 5 4 32 GB 5 5 In the meantime, between checking on your local jobs, start Part 2 \u2013 taking a break every few minutes to record progress on your local jobs. Part 2: Checking on the availability of memory (remotely) \u00b6 For the second part of the exercise, you will just copy over the directory from part 1 on learn.chtc.wisc.edu to osg-learn.chtc.wisc.edu and resubmit your jobs to the OSG. You should have plenty of experience copying over files with scp and submitting them to the OSG so I won't provide instructions here. If you get stuck at any point in this process, refer to exercise 1.2 . Monitoring the remote jobs \u00b6 As you did in part 1, use condor_q to track how your sleep jobs are doing. You can move onto the next exercise but keep tracking the status of your jobs. After you are done with the rest of the exercises, come back to this exercise, and move onto part 3 Part 3: Analyzing the results \u00b6 Now that you've finished the other exercise, how many jobs have completed locally? How many have completed remotely? Due to the dynamic nature of the remote pool, the OSG may have noticed the demand for higher memory jobs and leased more high memory slots for our pool. That being said, 8GB+ slots are a high-demand, low-availability resource in the OSG so it's unlikely that all of your 8GB+ jobs matched and ran to completion. On the other hand, the local cluster has plenty of 8GB+ slots so all your jobs have a high chance of running. Next exercise \u00b6 Once completed, move onto the next exercise: Software Differences in the OSG","title":"2.1. Hardware in OSG"},{"location":"materials/day2/part2-ex1-hardware-diffs/#tuesday-exercise-21-hardware-differences-in-the-osg","text":"The goal of this exercise is to compare hardware differences between our local cluster (CHTC here at UW\u2013Madison) and an OSG glidein pool. Specifically, we will look at how easy it is to get access to resources in terms of the amount of memory that is requested. This will not be a very careful study, but should give you some idea of one way in which the pools are different. In the first two parts of the exercise, you will submit a bunch of jobs that differ only in how much memory each one requests; we call this a parameter sweep , in that we are testing many possible values of a parameter. We will request memory from 1\u201316 GB, doubling the memory each time. One set of jobs will be submitted locally, and the other, identical set of jobs will be submitted to OSG. You will check the queue periodically to see how many jobs have completed and how many are still waiting to run.","title":"Tuesday Exercise 2.1: Hardware Differences in the OSG"},{"location":"materials/day2/part2-ex1-hardware-diffs/#part-1-checking-on-the-availability-of-memory-locally","text":"In this first part, you will create the submit file for both the local and OSG jobs, then submit the local set.","title":"Part 1: Checking on the availability of memory (locally)"},{"location":"materials/day2/part2-ex1-hardware-diffs/#yet-another-queue-syntax","text":"Yesterday, you learned about the queue statement and some of the different ways it can be invoked to submit multiple jobs. Similar to the queue from statement to submit jobs based on lines from a specific file, you can use queue in to submit jobs based on a list directly from your submit file: queue <# of jobs> <variable> in ( <item 1> <item 2> <item 3> ... ) For example, to submit 6 jobs that sleep for 5 , 5 , 10 , 10 , 15 , and 15 seconds, you could write a submit file like the following: executable = /bin/sleep queue 2 arguments in ( 5 10 15 ) Try submitting this file yourself and checking the jobs that end up in the queue with condor_q -nobatch .","title":"Yet another queue syntax"},{"location":"materials/day2/part2-ex1-hardware-diffs/#create-the-submit-files","text":"To create our parameter sweep, we will create one submit file with multiple queue statements and change the value of our parameter ( request_memory ) for each batch of jobs. If not already, log on to learn.chtc.wisc.edu Create and change into a new subdirectory called tuesday-2.1 \u2013 doing things this way will make Part 2 much easier Create a submit file that is named sleep.sub and that executes the command =/bin/sleep 30=\\ If you do not remember all of the submit statements to write this file, or just to go faster, find a similar submit file from yesterday, copy and rename it here, and make sure the argument to sleep is 30 (although the exact value does not matter for the test). Use the queue in syntax to submit 25 jobs requesting 2, 4, 8, 16, and 32GB of memory, e.g. you should have 5 jobs requesting 2GB, 5 jobs requesting 4GB, etc. Save the submit file and exit your editor Submit your jobs","title":"Create the submit files"},{"location":"materials/day2/part2-ex1-hardware-diffs/#monitoring-the-local-jobs","text":"Every few minutes, run condor_q and see how your sleep jobs are doing. To easily see how many jobs of each type you have left, run the following command: %UCL_PROMPT_SHORT% condor_q <span style=\"background-color: #D1CAF2;\"><Cluster ID></span> -af RequestMemory | sort -n | uniq -c The numbers in the left column are the number of jobs left of that type and the number on the right is the amount of memory you requested in MB. Consider making a little table like the one below to track progress. Memory Remaining #1 Remaining #2 Remaining #3 2 GB 5 3 4 GB 5 3 8 GB 5 4 16 GB 5 4 32 GB 5 5 In the meantime, between checking on your local jobs, start Part 2 \u2013 taking a break every few minutes to record progress on your local jobs.","title":"Monitoring the local jobs"},{"location":"materials/day2/part2-ex1-hardware-diffs/#part-2-checking-on-the-availability-of-memory-remotely","text":"For the second part of the exercise, you will just copy over the directory from part 1 on learn.chtc.wisc.edu to osg-learn.chtc.wisc.edu and resubmit your jobs to the OSG. You should have plenty of experience copying over files with scp and submitting them to the OSG so I won't provide instructions here. If you get stuck at any point in this process, refer to exercise 1.2 .","title":"Part 2: Checking on the availability of memory (remotely)"},{"location":"materials/day2/part2-ex1-hardware-diffs/#monitoring-the-remote-jobs","text":"As you did in part 1, use condor_q to track how your sleep jobs are doing. You can move onto the next exercise but keep tracking the status of your jobs. After you are done with the rest of the exercises, come back to this exercise, and move onto part 3","title":"Monitoring the remote jobs"},{"location":"materials/day2/part2-ex1-hardware-diffs/#part-3-analyzing-the-results","text":"Now that you've finished the other exercise, how many jobs have completed locally? How many have completed remotely? Due to the dynamic nature of the remote pool, the OSG may have noticed the demand for higher memory jobs and leased more high memory slots for our pool. That being said, 8GB+ slots are a high-demand, low-availability resource in the OSG so it's unlikely that all of your 8GB+ jobs matched and ran to completion. On the other hand, the local cluster has plenty of 8GB+ slots so all your jobs have a high chance of running.","title":"Part 3: Analyzing the results"},{"location":"materials/day2/part2-ex1-hardware-diffs/#next-exercise","text":"Once completed, move onto the next exercise: Software Differences in the OSG","title":"Next exercise"},{"location":"materials/day2/part2-ex2-software-diffs/","text":"Tuesday Exercise 2.2: Software Differences in the OSG \u00b6 The goal of this exercise is to see the differences in availability of software in the OSG. At your local cluster, you may be used to having certain versions of software but out on the OSG, it's possible that the software you need won't even be installed. Refresher - condor_status \u00b6 The OSG pool, like the local pool you used yesterday, is just another HTCondor pool. This means that the commands you use will be the same and the jobs you submit can have similar payloads but there is one major difference: the slots are different! You will can use the condor_status command just as you did yesterday to inspect these differences. Open two terminal windows side-by-side Log in to learn.chtc.wisc.edu in one window and osg-learn.chtc.wisc.edu in the other Run condor_status in both windows Notice any differences? Comparing operating systems \u00b6 To really see differences between slots in the local cluster vs the OSG, you will want to compare the slot ClassAds between the two pools. Rather than inspecting the very long ClassAd for each slot, you will look at a specific attribute called OpSysAndVer , which tells us the operating system version of the machine where a slot resides. An easy way to show this attribute for all slots is by using condor_status in conjunction with the -autoformat option. -autoformat like the -format option you learned about yesterday will print out the attributes you're interested in for each slot but as you probably guessed, perform the formatting in an automatic way. So to show the operating system and version of each slot, run the following command in both of your terminal windows: %UCL_PROMPT_SHORT% condor_status -autoformat OpSysAndVer You will see many values with the type of operating system at the front and the version number at the end (i.e. SL6 stands for Scientific Linux 6). The only problem is that with hundreds or thousands of slots, it's difficult to get a feel for the composition of each pool from this output. You can find a count for each operating system by passing the condor_status output into the sort and uniq commands. Your command line should look something like this: %UCL_PROMPT_SHORT% condor_status -autoformat OpSysAndVer | sort | uniq -c Can you spot the differences between the two pools now? Submitting probe jobs \u00b6 Knowing the different operating systems is a step in the right direction to knowing what kind of software will be available on the machines that your jobs land on but only serves as a proxy to the information that you want: does the machine have the software that you want? Does it have the correct version? Software probe code \u00b6 The following shell script probes for software and returns the version if it is installed: #!/bin/sh get_version(){ program=$1 $program --version > /dev/null 2>&1 double_dash_rc=$? $program -version > /dev/null 2>&1 single_dash_rc=$? which $program > /dev/null 2>&1 which_rc=$? if [ $double_dash_rc -eq 0 ]; then $program --version 2>&1 elif [ $single_dash_rc -eq 0 ]; then $program -version 2>&1 elif [ $which_rc -eq 0 ]; then echo \"$program installed but could not find version information\" else echo \"$program not installed\" fi } get_version 'R' get_version 'cmake' get_version 'python' If there's a specific command line program that your research requires, feel free to add it to the script! For example, if you wanted to test for the existence and version of nslookup , you would add the following to the end of the script: get_version '%RED%nslookup%ENDCOLOR%' Probing several machines \u00b6 For this part of the exercise, try creating a submit file without referring to previous exercises! Log on to osg-learn.chtc.wisc.edu Create and change into a new folder for this exercise, e.g. tuesday-2.2 Save the above script as a file named sw_probe.sh As always (), make sure that you can run your script from the command line before asking HTCondor to do so Create a submit file that runs sw_probe.sh 100 times and uses macros to write different output , error , and log files Submit your job and wait for the results Will you be able to do your research on the OSG with what's available? Don't fret if it doesn't look like you can: over the next few days, you'll learn how to make your jobs portable enough so that they can run anywhere!","title":"2.2. Software in OSG"},{"location":"materials/day2/part2-ex2-software-diffs/#tuesday-exercise-22-software-differences-in-the-osg","text":"The goal of this exercise is to see the differences in availability of software in the OSG. At your local cluster, you may be used to having certain versions of software but out on the OSG, it's possible that the software you need won't even be installed.","title":"Tuesday Exercise 2.2: Software Differences in the OSG"},{"location":"materials/day2/part2-ex2-software-diffs/#refresher-condor_status","text":"The OSG pool, like the local pool you used yesterday, is just another HTCondor pool. This means that the commands you use will be the same and the jobs you submit can have similar payloads but there is one major difference: the slots are different! You will can use the condor_status command just as you did yesterday to inspect these differences. Open two terminal windows side-by-side Log in to learn.chtc.wisc.edu in one window and osg-learn.chtc.wisc.edu in the other Run condor_status in both windows Notice any differences?","title":"Refresher - condor_status"},{"location":"materials/day2/part2-ex2-software-diffs/#comparing-operating-systems","text":"To really see differences between slots in the local cluster vs the OSG, you will want to compare the slot ClassAds between the two pools. Rather than inspecting the very long ClassAd for each slot, you will look at a specific attribute called OpSysAndVer , which tells us the operating system version of the machine where a slot resides. An easy way to show this attribute for all slots is by using condor_status in conjunction with the -autoformat option. -autoformat like the -format option you learned about yesterday will print out the attributes you're interested in for each slot but as you probably guessed, perform the formatting in an automatic way. So to show the operating system and version of each slot, run the following command in both of your terminal windows: %UCL_PROMPT_SHORT% condor_status -autoformat OpSysAndVer You will see many values with the type of operating system at the front and the version number at the end (i.e. SL6 stands for Scientific Linux 6). The only problem is that with hundreds or thousands of slots, it's difficult to get a feel for the composition of each pool from this output. You can find a count for each operating system by passing the condor_status output into the sort and uniq commands. Your command line should look something like this: %UCL_PROMPT_SHORT% condor_status -autoformat OpSysAndVer | sort | uniq -c Can you spot the differences between the two pools now?","title":"Comparing operating systems"},{"location":"materials/day2/part2-ex2-software-diffs/#submitting-probe-jobs","text":"Knowing the different operating systems is a step in the right direction to knowing what kind of software will be available on the machines that your jobs land on but only serves as a proxy to the information that you want: does the machine have the software that you want? Does it have the correct version?","title":"Submitting probe jobs"},{"location":"materials/day2/part2-ex2-software-diffs/#software-probe-code","text":"The following shell script probes for software and returns the version if it is installed: #!/bin/sh get_version(){ program=$1 $program --version > /dev/null 2>&1 double_dash_rc=$? $program -version > /dev/null 2>&1 single_dash_rc=$? which $program > /dev/null 2>&1 which_rc=$? if [ $double_dash_rc -eq 0 ]; then $program --version 2>&1 elif [ $single_dash_rc -eq 0 ]; then $program -version 2>&1 elif [ $which_rc -eq 0 ]; then echo \"$program installed but could not find version information\" else echo \"$program not installed\" fi } get_version 'R' get_version 'cmake' get_version 'python' If there's a specific command line program that your research requires, feel free to add it to the script! For example, if you wanted to test for the existence and version of nslookup , you would add the following to the end of the script: get_version '%RED%nslookup%ENDCOLOR%'","title":"Software probe code"},{"location":"materials/day2/part2-ex2-software-diffs/#probing-several-machines","text":"For this part of the exercise, try creating a submit file without referring to previous exercises! Log on to osg-learn.chtc.wisc.edu Create and change into a new folder for this exercise, e.g. tuesday-2.2 Save the above script as a file named sw_probe.sh As always (), make sure that you can run your script from the command line before asking HTCondor to do so Create a submit file that runs sw_probe.sh 100 times and uses macros to write different output , error , and log files Submit your job and wait for the results Will you be able to do your research on the OSG with what's available? Don't fret if it doesn't look like you can: over the next few days, you'll learn how to make your jobs portable enough so that they can run anywhere!","title":"Probing several machines"},{"location":"materials/day2/part4-ex1-troubleshooting/","text":"Tuesday Exercise 3.1: Troubleshooting a DAG \u00b6 The goal of this exercise is to troubleshoot some common problems that you may encounter when submitting a DAG using HTCondor. This exercise will likely take you longer than the allotted one hour of time; don't fret, there is an answer key provided, so work at your own pace. Part 1: Acquiring the materials \u00b6 The materials for this exercise are located on our web server. Log into learn.chtc.wisc.edu Use wget to retrieve the materials from the web server:\\ wget http://proxy.chtc.wisc.edu/SQUID/osgschool17/tues_31.tar.gz Extract the tarball using the commands you learned earlier today Change into the directory extracted from the tarball and explore its contents anagram.dag is the main DAG that you will be submitting and although it may look like a simple, linear DAG, it's actually more like the DAG from Monday's exercise 4.2 because of the SUBDAG within it! There is only one .dag file in the extracted folder, so where does the SUBDAG come from? Part 2: Finding anagramic squares \u00b6 The contents of the tarball that you've extracted contain a DAG that is designed to solve Project Euler problem 98 : By replacing each of the letters in the word CARE with 1, 2, 9, and 6 respectively, we form a square number: 1296 = 36<sup>2</sup>. What is remarkable is that, by using the same digital substitutions, the anagram, RACE, also forms a square number: 9216 = 96<sup>2</sup>. We shall call CARE (and RACE) a square anagram word pair and specify further that leading zeroes are not permitted, neither may a different letter have the same digital value as another letter. Using p098_words.txt, a 16K text file containing nearly two-thousand common English words, find all the square anagram word pairs (a palindromic word is NOT considered to be an anagram of itself). What is the largest square number formed by any member of such a pair? NOTE: All anagrams formed must be contained in the given text file. Unfortunately, there are many issues with the DAG and its submit files that you will have to work through before you can you can obtain the solution to the problem (the code itself should be bug-free)! Submit the DAG: %UCL_PROMPT_SHORT% condor_submit_dag anagrams.dag Then use your newfound HTCondor troubleshooting knowledge to find the answer to the Euler problem that ends up in result.out when you've successfully completed this exercise. Answer key \u00b6 There is also a working solution on our web server that can be retrieved with %UCL_PROMPT_SHORT% <strong>wget http://proxy.chtc.wisc.edu/SQUID/osgschool17/tues_31_answer.tar.gz</strong> It contains comments labeled SOLUTION that you can consult in case you get stuck. Like any answer key, it's mainly useful as a verification tool, so try to only use it as a last resort or for additional illumination.","title":"3.1. Troubleshooting"},{"location":"materials/day2/part4-ex1-troubleshooting/#tuesday-exercise-31-troubleshooting-a-dag","text":"The goal of this exercise is to troubleshoot some common problems that you may encounter when submitting a DAG using HTCondor. This exercise will likely take you longer than the allotted one hour of time; don't fret, there is an answer key provided, so work at your own pace.","title":"Tuesday Exercise 3.1: Troubleshooting a DAG"},{"location":"materials/day2/part4-ex1-troubleshooting/#part-1-acquiring-the-materials","text":"The materials for this exercise are located on our web server. Log into learn.chtc.wisc.edu Use wget to retrieve the materials from the web server:\\ wget http://proxy.chtc.wisc.edu/SQUID/osgschool17/tues_31.tar.gz Extract the tarball using the commands you learned earlier today Change into the directory extracted from the tarball and explore its contents anagram.dag is the main DAG that you will be submitting and although it may look like a simple, linear DAG, it's actually more like the DAG from Monday's exercise 4.2 because of the SUBDAG within it! There is only one .dag file in the extracted folder, so where does the SUBDAG come from?","title":"Part 1: Acquiring the materials"},{"location":"materials/day2/part4-ex1-troubleshooting/#part-2-finding-anagramic-squares","text":"The contents of the tarball that you've extracted contain a DAG that is designed to solve Project Euler problem 98 : By replacing each of the letters in the word CARE with 1, 2, 9, and 6 respectively, we form a square number: 1296 = 36<sup>2</sup>. What is remarkable is that, by using the same digital substitutions, the anagram, RACE, also forms a square number: 9216 = 96<sup>2</sup>. We shall call CARE (and RACE) a square anagram word pair and specify further that leading zeroes are not permitted, neither may a different letter have the same digital value as another letter. Using p098_words.txt, a 16K text file containing nearly two-thousand common English words, find all the square anagram word pairs (a palindromic word is NOT considered to be an anagram of itself). What is the largest square number formed by any member of such a pair? NOTE: All anagrams formed must be contained in the given text file. Unfortunately, there are many issues with the DAG and its submit files that you will have to work through before you can you can obtain the solution to the problem (the code itself should be bug-free)! Submit the DAG: %UCL_PROMPT_SHORT% condor_submit_dag anagrams.dag Then use your newfound HTCondor troubleshooting knowledge to find the answer to the Euler problem that ends up in result.out when you've successfully completed this exercise.","title":"Part 2: Finding anagramic squares"},{"location":"materials/day2/part4-ex1-troubleshooting/#answer-key","text":"There is also a working solution on our web server that can be retrieved with %UCL_PROMPT_SHORT% <strong>wget http://proxy.chtc.wisc.edu/SQUID/osgschool17/tues_31_answer.tar.gz</strong> It contains comments labeled SOLUTION that you can consult in case you get stuck. Like any answer key, it's mainly useful as a verification tool, so try to only use it as a last resort or for additional illumination.","title":"Answer key"},{"location":"materials/day3/part1-ex1-compiling/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Wednesday Exercise 1.1: Compiling Programs for Portability \u00b6 The goal of this exercise is to compile and statically link a piece of code and then submit it as a job. This exercise should take 5-10 minutes. Background \u00b6 There is a large amount of scientific software that is available as source code. Source code is usually a group of text files (code) meant to be downloaded and then compiled into a binary file which a computer can understand. Sometimes the source code depends on other pieces of code called libraries. If the source code is linked statically , these libraries are bundled into the compilation with the source code, creating a static binary which can be run on any computer with the same operating system. Our Software Example \u00b6 For this compiling example, we will use a script written in C. C code depends on libraries and therefore will benefit from being statically linked. Our C code prints 7 rows of Pascal's triangle. Log into the OSG submit node osg-learn.chtc.wisc.edu . Create a directory for this exercise and cd into it. Copy and paste the following code into a file named pascal.c . \\ \\ #include &ltstdio.h&gt long factorial(int); int main() { int i, n, c; n=7; for (i = 0; i < n; i++){ for (c = 0; c <= (n - i - 2); c++) printf(\" \"); for (c = 0 ; c <= i; c++) printf(\"%ld \",factorial(i)/(factorial(c)*factorial(i-c))); printf(\"\\n\"); } return 0; } long factorial(int n) { int c; long result = 1; for (c = 1; c <= n; c++) result = result*c; return result; } Compiling \u00b6 In order to use this code in a job, we will first need to statically compile the code. Recall the slide from the lecture - where can we compile and where should we compile? In particular: Where is the compiler available? How computationally intensive will this compilation be? Think about these questions before moving on. Where do you think we should compile? \\ 2. Most linux servers (including our submit node) have the gcc (GNU compiler collection) installed, so we already have a compiler on the submit node. Furthermore, this is a simple piece of C code, so the compilation will not be computationally intensive. Thus, we should be able to compile directly on the submit node. \\ Compile the code, using the command: \\ gcc -static pascal.c -o pascal \\ \\ Note that we have added the -static option to make sure that the compiled binary includes the necessary libraries. This will allow the code to run on any Linux machine, no matter where those libraries are located. \\ 4. Verify that the compiled binary was statically linked:\\ file pascal \\ The Linux file command provides information about the type or kind of file that is given as an argument. In this case, you should get output like this: \\ pascal: ELF 64-bit LSB executable, x86-64, version 1 (GNU/Linux), statically linked , for GNU/Linux 2.6.18, not stripped \\ Note the highlighted part, which clearly states that this executable (software) is statically linked. The same command run on a non-statically linked executable file would include the text dynamically linked (uses shared libs) instead. So with this simple verification step, which could even be run on files that you did not compile yourself, you have some further reassurance that it is safe to use on other Linux machines. (Bonus exercise: Try the file command on lots of other files) Submit the Job \u00b6 Now that our code is compiled, we can use it to submit a job. Think about what submit file lines we need to use to run this job: Are there input files? Are there command line arguments? Where is its output written? \\ Based on what you thought about in 1., find a submit file from earlier in the week that you can modify to run our compiled pascal code. \\ Copy it to the directory with the pascal binary and make those changes. \\ 2. Submit the job using condor_submit . \\ 3. Once the job has run and left the queue, you should be able to see the results (seven rows of \\ Pascal's triangle) in the .out file created by the job.","title":"1.1. Compiling programs"},{"location":"materials/day3/part1-ex1-compiling/#wednesday-exercise-11-compiling-programs-for-portability","text":"The goal of this exercise is to compile and statically link a piece of code and then submit it as a job. This exercise should take 5-10 minutes.","title":"Wednesday Exercise 1.1: Compiling Programs for Portability"},{"location":"materials/day3/part1-ex1-compiling/#background","text":"There is a large amount of scientific software that is available as source code. Source code is usually a group of text files (code) meant to be downloaded and then compiled into a binary file which a computer can understand. Sometimes the source code depends on other pieces of code called libraries. If the source code is linked statically , these libraries are bundled into the compilation with the source code, creating a static binary which can be run on any computer with the same operating system.","title":"Background"},{"location":"materials/day3/part1-ex1-compiling/#our-software-example","text":"For this compiling example, we will use a script written in C. C code depends on libraries and therefore will benefit from being statically linked. Our C code prints 7 rows of Pascal's triangle. Log into the OSG submit node osg-learn.chtc.wisc.edu . Create a directory for this exercise and cd into it. Copy and paste the following code into a file named pascal.c . \\ \\ #include &ltstdio.h&gt long factorial(int); int main() { int i, n, c; n=7; for (i = 0; i < n; i++){ for (c = 0; c <= (n - i - 2); c++) printf(\" \"); for (c = 0 ; c <= i; c++) printf(\"%ld \",factorial(i)/(factorial(c)*factorial(i-c))); printf(\"\\n\"); } return 0; } long factorial(int n) { int c; long result = 1; for (c = 1; c <= n; c++) result = result*c; return result; }","title":"Our Software Example"},{"location":"materials/day3/part1-ex1-compiling/#compiling","text":"In order to use this code in a job, we will first need to statically compile the code. Recall the slide from the lecture - where can we compile and where should we compile? In particular: Where is the compiler available? How computationally intensive will this compilation be? Think about these questions before moving on. Where do you think we should compile? \\ 2. Most linux servers (including our submit node) have the gcc (GNU compiler collection) installed, so we already have a compiler on the submit node. Furthermore, this is a simple piece of C code, so the compilation will not be computationally intensive. Thus, we should be able to compile directly on the submit node. \\ Compile the code, using the command: \\ gcc -static pascal.c -o pascal \\ \\ Note that we have added the -static option to make sure that the compiled binary includes the necessary libraries. This will allow the code to run on any Linux machine, no matter where those libraries are located. \\ 4. Verify that the compiled binary was statically linked:\\ file pascal \\ The Linux file command provides information about the type or kind of file that is given as an argument. In this case, you should get output like this: \\ pascal: ELF 64-bit LSB executable, x86-64, version 1 (GNU/Linux), statically linked , for GNU/Linux 2.6.18, not stripped \\ Note the highlighted part, which clearly states that this executable (software) is statically linked. The same command run on a non-statically linked executable file would include the text dynamically linked (uses shared libs) instead. So with this simple verification step, which could even be run on files that you did not compile yourself, you have some further reassurance that it is safe to use on other Linux machines. (Bonus exercise: Try the file command on lots of other files)","title":"Compiling"},{"location":"materials/day3/part1-ex1-compiling/#submit-the-job","text":"Now that our code is compiled, we can use it to submit a job. Think about what submit file lines we need to use to run this job: Are there input files? Are there command line arguments? Where is its output written? \\ Based on what you thought about in 1., find a submit file from earlier in the week that you can modify to run our compiled pascal code. \\ Copy it to the directory with the pascal binary and make those changes. \\ 2. Submit the job using condor_submit . \\ 3. Once the job has run and left the queue, you should be able to see the results (seven rows of \\ Pascal's triangle) in the .out file created by the job.","title":"Submit the Job"},{"location":"materials/day3/part1-ex2-precompiled/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Wednesday Exercise 1.2: Using a Pre-compiled Binary \u00b6 In this exercise, we will run a job using a downloaded, pre-compiled binary. This exercise should take 10-15 minutes. Background \u00b6 In the previous exercise, we used a piece of software that was available as source code, and could be compiled to a static binary. However, software is available in many different forms. You may also encounter scientific software provided as a static binary. Here, the software provider has compiled the software for you (typically on several operating systems). Using a pre-compiled binary means you can avoid compiling the code yourself; accessing the software and getting it ready to run is as simple as downloading the binary. Our Software Example \u00b6 The software we will be using for this example is a common tool for aligning genome and protein sequences against a reference database, the BLAST program. Search the internet for the BLAST software. Searches might include \"blast executable or \"download blast software\". Hopefully these searches will lead you to a BLAST website page that looks like this: Click on the title that says \"Download BLAST\" and then look for the link that has the latest installation and source code. You should end up on a page with a list of each version of BLAST that is available for different operating systems. We could download the source and compile it ourselves, but instead, we're going to use \\ one of the pre-built binaries. \\ Before proceeding, look at the list of downloads and try to determine which one you want. \\ Based on our operating system, we want to use the Linux binary, which is \\ labelled with the x64-linux suffix. All the other links are either for source code or other operating systems. \\ While logged into osg-learn.chtc.wisc.edu , create a directory for this exercise. Then download the appropriate tar.gz file and un-tar it. \\ You can download the file directly from the BLAST website using wget or download our local copy with the command below: \\ %UCL_PROMPT_SHORT% <strong>wget http://proxy.chtc.wisc.edu/SQUID/osgschool17/ncbi-blast-2.6.0+-x64-linux.tar.gz</strong> %UCL_PROMPT_SHORT% <strong>tar -xzf ncbi-blast-2.6.0+-x64-linux.tar.gz</strong> We're going to be using the blastx binary in our job. Where is \\ it in the directory you just downloaded? Copy the Input Files \u00b6 To run BLAST, we need an input file and reference database. For this example, we'll use the \"pdbaa\" database, which contains sequences for the protein structure from the Protein Data Bank. For our input file, we'll use an abbreviated fasta file with mouse genome information. Download these files to your current directory: \\ %UCL_PROMPT_SHORT% <strong>wget http://proxy.chtc.wisc.edu/SQUID/osgschool17/pdbaa.tar.gz</strong> %UCL_PROMPT_SHORT% <strong>wget http://proxy.chtc.wisc.edu/SQUID/osgschool17/mouse.fa</strong> Untar the pdbaa database: \\ %UCL_PROMPT_SHORT% <strong>tar -xzf pdbaa.tar.gz</strong> Submitting the Job \u00b6 We now have our program (the pre-compiled blastx binary) and our input files, so all that remains is to create the submit file. A typical blastx command looks something like this: %UCL_PROMPT_SHORT% <strong> blastx -db database -query input_file -out results.txt</strong> Copy the submit file from the last exercise into your current directory. 2. Think about which lines you will need to change or add to your submit file in order to submit \\ the job successfully. In particular: What is the executable? How can you indicate the entire command line sequence above? Which files need to be transferred in addition to the executable? Does this job require a certain type of operating system? \\ 3. Try to answer these questions and modify your submit file appropriately. 4. Once you have done all you can, check your submit file against the lines below, which contain the necessary changes \\ to run this particular job. The executable is blastx , which is located in the bin directory of our downloaded BLAST \\ directory. We need to use the arguments line in the submit file to express the rest of the \\ command. \\ \\ executable = ncbi-blast-2.6.0+/bin/blastx arguments = -db pdbaa/pdbaa -query mouse.fa -out results.txt \\ The BLAST program requires our input file and database, so they must be transferred with transfer_input_files . \\ \\ transfer_input_files = pdbaa, mouse.fa \\ Because we downloaded a Linux-specific binary, we need to request machines that are running Linux. \\ \\ requirements = (OpSys == \"LINUX\") \\ Submit the blast job using condor_submit . Once the job starts, it should run in just a few minutes and produce a file called results.txt .","title":"1.2. Pre-compiled binary"},{"location":"materials/day3/part1-ex2-precompiled/#wednesday-exercise-12-using-a-pre-compiled-binary","text":"In this exercise, we will run a job using a downloaded, pre-compiled binary. This exercise should take 10-15 minutes.","title":"Wednesday Exercise 1.2: Using a Pre-compiled Binary"},{"location":"materials/day3/part1-ex2-precompiled/#background","text":"In the previous exercise, we used a piece of software that was available as source code, and could be compiled to a static binary. However, software is available in many different forms. You may also encounter scientific software provided as a static binary. Here, the software provider has compiled the software for you (typically on several operating systems). Using a pre-compiled binary means you can avoid compiling the code yourself; accessing the software and getting it ready to run is as simple as downloading the binary.","title":"Background"},{"location":"materials/day3/part1-ex2-precompiled/#our-software-example","text":"The software we will be using for this example is a common tool for aligning genome and protein sequences against a reference database, the BLAST program. Search the internet for the BLAST software. Searches might include \"blast executable or \"download blast software\". Hopefully these searches will lead you to a BLAST website page that looks like this: Click on the title that says \"Download BLAST\" and then look for the link that has the latest installation and source code. You should end up on a page with a list of each version of BLAST that is available for different operating systems. We could download the source and compile it ourselves, but instead, we're going to use \\ one of the pre-built binaries. \\ Before proceeding, look at the list of downloads and try to determine which one you want. \\ Based on our operating system, we want to use the Linux binary, which is \\ labelled with the x64-linux suffix. All the other links are either for source code or other operating systems. \\ While logged into osg-learn.chtc.wisc.edu , create a directory for this exercise. Then download the appropriate tar.gz file and un-tar it. \\ You can download the file directly from the BLAST website using wget or download our local copy with the command below: \\ %UCL_PROMPT_SHORT% <strong>wget http://proxy.chtc.wisc.edu/SQUID/osgschool17/ncbi-blast-2.6.0+-x64-linux.tar.gz</strong> %UCL_PROMPT_SHORT% <strong>tar -xzf ncbi-blast-2.6.0+-x64-linux.tar.gz</strong> We're going to be using the blastx binary in our job. Where is \\ it in the directory you just downloaded?","title":"Our Software Example"},{"location":"materials/day3/part1-ex2-precompiled/#copy-the-input-files","text":"To run BLAST, we need an input file and reference database. For this example, we'll use the \"pdbaa\" database, which contains sequences for the protein structure from the Protein Data Bank. For our input file, we'll use an abbreviated fasta file with mouse genome information. Download these files to your current directory: \\ %UCL_PROMPT_SHORT% <strong>wget http://proxy.chtc.wisc.edu/SQUID/osgschool17/pdbaa.tar.gz</strong> %UCL_PROMPT_SHORT% <strong>wget http://proxy.chtc.wisc.edu/SQUID/osgschool17/mouse.fa</strong> Untar the pdbaa database: \\ %UCL_PROMPT_SHORT% <strong>tar -xzf pdbaa.tar.gz</strong>","title":"Copy the Input Files"},{"location":"materials/day3/part1-ex2-precompiled/#submitting-the-job","text":"We now have our program (the pre-compiled blastx binary) and our input files, so all that remains is to create the submit file. A typical blastx command looks something like this: %UCL_PROMPT_SHORT% <strong> blastx -db database -query input_file -out results.txt</strong> Copy the submit file from the last exercise into your current directory. 2. Think about which lines you will need to change or add to your submit file in order to submit \\ the job successfully. In particular: What is the executable? How can you indicate the entire command line sequence above? Which files need to be transferred in addition to the executable? Does this job require a certain type of operating system? \\ 3. Try to answer these questions and modify your submit file appropriately. 4. Once you have done all you can, check your submit file against the lines below, which contain the necessary changes \\ to run this particular job. The executable is blastx , which is located in the bin directory of our downloaded BLAST \\ directory. We need to use the arguments line in the submit file to express the rest of the \\ command. \\ \\ executable = ncbi-blast-2.6.0+/bin/blastx arguments = -db pdbaa/pdbaa -query mouse.fa -out results.txt \\ The BLAST program requires our input file and database, so they must be transferred with transfer_input_files . \\ \\ transfer_input_files = pdbaa, mouse.fa \\ Because we downloaded a Linux-specific binary, we need to request machines that are running Linux. \\ \\ requirements = (OpSys == \"LINUX\") \\ Submit the blast job using condor_submit . Once the job starts, it should run in just a few minutes and produce a file called results.txt .","title":"Submitting the Job"},{"location":"materials/day3/part1-ex3-wrapper/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Wednesday Exercise 1.3: Using Wrapper Scripts to Submit Jobs \u00b6 In this exercise, you will create a wrapper script to run the same program ( blastx ) as \\ the previous exercise . Background \u00b6 Wrapper scripts are a useful tool for running software that can't be \\ compiled into one piece, needs to be installed with every job, or just for running extra steps. \\ A wrapper script can either install the software from the source code, or use an already existing software (as in this exercise). Not only does this portability technique work with almost any kind of software that can be locally installed, it also allows for a great deal of control and flexibility for what happens within your job. Once you can write a script to handle your software (and often your data as well), you can submit a large variety of workflows to a distributed computing system like the Open Science Grid. Wrapper Script, part 1 \u00b6 Our wrapper script will be a bash script that runs several commands. In the same directory as the last exercise (still logged into osg-learn.chtc.wisc.edu ) make a file called run_blast.sh . \\ The first line we'll place in the script is the basic command for running blast. Based on our previous submit file, what command needs to go into the \\ script? Once you have an idea, check against the example below: \\ #/bin/bash ncbi-blast-2.6.0+/bin/blastx -db pdbaa/pdbaa -query mouse.fa -out results.txt \\ ( Note : the \"header\" of #!/bin/bash will tell the computer that this is a bash shell script and can be run in the same way that \\ you would run individual commands on the command line.) \\ Submit File Changes \u00b6 We now need to make some changes to our submit file. Make a copy of your previous submit file and open it. \\ Since we are now using a wrapper script, that will be our job's executable. Replace the original blastx exeuctable with the name \\ of our wrapper script and comment out the arguments line. \\ executable = run_blast.sh #arguments = \\ Note that since the blastx program is no longer listed as the executable, it will be need to be included in transfer_input_files . Instead of \\ transferring just that program, we will transfer the original downloaded tar.gz file. \\ transfer_input_files = pdbaa, mouse.fa, ncbi-blast-2.6.0+-x64-linux.tar.gz \\ To achieve efficiency, we'll also transfer the pdbaa database as the original tar.gz file instead of as the unzipped folder: \\ transfer_input_files = pdbaa.tar.gz , mouse.fa, ncbi-blast-2.6.0+-x64-linux.tar.gz \\ Wrapper Script, part 2 \u00b6 Now that our database and BLAST software are being transferred to the job as tar.gz files, our script needs to accommodate. Opening your run_blast.sh script, add two commands at the start to un-tar the BLAST and pdbaa tar.gz files. See the \\ previous exercise if you're not sure what this command looks like. \\ In order to distinguish this job from our previous job, change the output file name to something besides results.txt . \\ The completed script run_blast.sh should look like this: \\ #/bin/bash tar -xzf ncbi-blast-2.6.0+-x64-linux.tar.gz tar -xzf pdbaa.tar.gz ncbi-blast-2.6.0+/bin/blastx -db pdbaa/pdbaa -query mouse.fa -out results2.txt \\ While not strictly necessary, it's a good idea to enable executable permissions on the wrapper script, like so: \\ %UCL_PROMPT_SHORT% <strong>chmod u+x run_blast.sh</strong> Your job is now ready to submit. Submit it using condor_submit and monitor using condor_q .","title":"1.3. Wrapper script"},{"location":"materials/day3/part1-ex3-wrapper/#wednesday-exercise-13-using-wrapper-scripts-to-submit-jobs","text":"In this exercise, you will create a wrapper script to run the same program ( blastx ) as \\ the previous exercise .","title":"Wednesday Exercise 1.3: Using Wrapper Scripts to Submit Jobs"},{"location":"materials/day3/part1-ex3-wrapper/#background","text":"Wrapper scripts are a useful tool for running software that can't be \\ compiled into one piece, needs to be installed with every job, or just for running extra steps. \\ A wrapper script can either install the software from the source code, or use an already existing software (as in this exercise). Not only does this portability technique work with almost any kind of software that can be locally installed, it also allows for a great deal of control and flexibility for what happens within your job. Once you can write a script to handle your software (and often your data as well), you can submit a large variety of workflows to a distributed computing system like the Open Science Grid.","title":"Background"},{"location":"materials/day3/part1-ex3-wrapper/#wrapper-script-part-1","text":"Our wrapper script will be a bash script that runs several commands. In the same directory as the last exercise (still logged into osg-learn.chtc.wisc.edu ) make a file called run_blast.sh . \\ The first line we'll place in the script is the basic command for running blast. Based on our previous submit file, what command needs to go into the \\ script? Once you have an idea, check against the example below: \\ #/bin/bash ncbi-blast-2.6.0+/bin/blastx -db pdbaa/pdbaa -query mouse.fa -out results.txt \\ ( Note : the \"header\" of #!/bin/bash will tell the computer that this is a bash shell script and can be run in the same way that \\ you would run individual commands on the command line.) \\","title":"Wrapper Script, part 1"},{"location":"materials/day3/part1-ex3-wrapper/#submit-file-changes","text":"We now need to make some changes to our submit file. Make a copy of your previous submit file and open it. \\ Since we are now using a wrapper script, that will be our job's executable. Replace the original blastx exeuctable with the name \\ of our wrapper script and comment out the arguments line. \\ executable = run_blast.sh #arguments = \\ Note that since the blastx program is no longer listed as the executable, it will be need to be included in transfer_input_files . Instead of \\ transferring just that program, we will transfer the original downloaded tar.gz file. \\ transfer_input_files = pdbaa, mouse.fa, ncbi-blast-2.6.0+-x64-linux.tar.gz \\ To achieve efficiency, we'll also transfer the pdbaa database as the original tar.gz file instead of as the unzipped folder: \\ transfer_input_files = pdbaa.tar.gz , mouse.fa, ncbi-blast-2.6.0+-x64-linux.tar.gz \\","title":"Submit File Changes"},{"location":"materials/day3/part1-ex3-wrapper/#wrapper-script-part-2","text":"Now that our database and BLAST software are being transferred to the job as tar.gz files, our script needs to accommodate. Opening your run_blast.sh script, add two commands at the start to un-tar the BLAST and pdbaa tar.gz files. See the \\ previous exercise if you're not sure what this command looks like. \\ In order to distinguish this job from our previous job, change the output file name to something besides results.txt . \\ The completed script run_blast.sh should look like this: \\ #/bin/bash tar -xzf ncbi-blast-2.6.0+-x64-linux.tar.gz tar -xzf pdbaa.tar.gz ncbi-blast-2.6.0+/bin/blastx -db pdbaa/pdbaa -query mouse.fa -out results2.txt \\ While not strictly necessary, it's a good idea to enable executable permissions on the wrapper script, like so: \\ %UCL_PROMPT_SHORT% <strong>chmod u+x run_blast.sh</strong> Your job is now ready to submit. Submit it using condor_submit and monitor using condor_q .","title":"Wrapper Script, part 2"},{"location":"materials/day3/part1-ex4-prepackaged/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Wednesday Exercise 1.4: Pre-packaging Code \u00b6 In this exercise, you will create an installation of a Bayesian inference package (OpenBUGS) and then create \\ a wrapper script to unpack that installation to run jobs. It should take 30-35 minutes. Background \u00b6 Some software cannot be compiled into a single executable, whether you compile it yourself (as in Exercise 1.1 ) or download it already compiled (as in Exercise 1.2 ). In this case, it is necessary to download or create a portable copy of the software and then use a wrapper script (as in the previous exercise ) to \"install\" the software on a per job basis. This script can either install the software from the source code, or (as in this exercise), unpack a portable software package that you've pre-built yourself. Our Software Example \u00b6 For this exercise, we will be using the Bayseian inference package Open BUGS. Open BUGS is a good example of software that is not compiled to a single executable; it has multiple executables as well as a helper library. Do an internet search to find the Open BUGS software downloads page. Create a directory for this exercise on the CHTC submit server learn.chtc.wisc.edu ( not osg-learn.chtc.wisc.edu ), download the source code ( OpenBUGS-3.2.3.tar.gz ), and place it in this directory. Note: The OpenBUGS source tarball cannot be downloaded via wget=; you will need to download the source to your own computer and then transfer it to =learn.chtc.wisc.edu . Where to Prepare \u00b6 Our goal is to pre-build an Open BUGS installation, and then write a script that will unpack that installation and run a simulation. Where can we create this pre-built installation? Based on the end of the lecture, what are our options and which would be most appropriate? Make a guess before moving on. Because we're on the CHTC-based submit node ( learn.chtc.wisc.edu ), we have the option of using an interactive job to build the Open BUGS installation. This is a good option because the submit server is already busy with lots of users and we don't know how long the Open BUGS install will take. To submit an interactive job do the following: Copy the following lines into a file named build.submit \\ universe = vanilla output = build.out error = build.err log = build.log should_transfer_files = YES when_to_transfer_output = ON_EXIT transfer_input_files = request_disk = 2GB request_memory = 2GB queue Note the lack of executable. Condor doesn't need an executable for this job because it will be interactive, meaning you are running the commands instead of Condor. In order to create the installation, we will need the source code to come with us. The transfer_input_files line is blank - fill it in with the name of our Open BUGS source tarball . To request an interactive job, we will add a -i flag to the condor_submit command. The whole command you enter should look like this: \\ %UCL_PROMPT_SHORT% <strong>condor_submit -i build.submit</strong> Read Through Installation Documentation \u00b6 While you're waiting for the interactive job to start, you can start reading the Open BUGS installation documentation online. Find the installation instructions for Open BUGS. On the downloads page, there are short instructions for how to install Open BUGS. There are two options shown for installation -- which should we use? The first installation option given uses sudo -- which is an administrative permission that you won't have as a normal user. Luckily, as described in the \\ instructions, you can use the --prefix option to set where Open BUGS will be installed, which will allow us to install it without administrative permissions. Installation \u00b6 Your interactive job should have started by now and we've learned about installing our program. Let's test it out. Before we follow the installation instructions, we should create a directory to hold our installation. You can create this in the current directory. \\ %UCL_PROMPT_SHORT% <strong>mkdir openbugs</strong> \\ Now run the commands to unpack the source code: \\ %UCL_PROMPT_SHORT% <strong>tar zxvf OpenBUGS-3.2.3.tar.gz</strong> %UCL_PROMPT_SHORT% <strong>cd OpenBUGS-3.2.3</strong> \\ Now we can follow the second set of installation instructions. For the prefix, we'll use the command $(pwd) to capture the name of our \\ current working directory and then a relative path to the openbugs directory we created in step 1: \\ %UCL_PROMPT_SHORT% <strong>./configure --prefix=$(pwd)/../openbugs</strong> %UCL_PROMPT_SHORT% <strong>make</strong> %UCL_PROMPT_SHORT% <strong>make install</strong> \\ Go back to the job's main working directory : \\ %UCL_PROMPT_SHORT% <strong>cd ..</strong> \\ and confirm that our installation procedure created bin , \\ lib , and share directories. \\ %UCL_PROMPT_SHORT% <strong>ls openbugs</strong> bin lib share \\ Now we want to package up our installation, so we can use it in other jobs. We can do this simply by compressing any necessary directories \\ into a single gzipped tarball. \\ %UCL_PROMPT_SHORT% <strong>tar -czf openbugs.tar.gz openbugs/</strong> \\ Once everything is complete, type exit to leave the interactive job. Make sure that your tarball is in the main working directory - it will be transferred back to the submit server automatically. \\ %UCL_PROMPT_SHORT% <strong>exit</strong> Note that we now have two tarballs in our directory -- the source tarball ( OpenBUGS-3.2.3.tar.gz ), which we will no longer need and our newly built installation ( openbugs.tar.gz ) which is what we will actually be using to run jobs. Wrapper Script \u00b6 Now that we've created our portable installation, we need to write a script that opens and uses the installation, similar to the process we used in the \\ previous exercise . These steps should be performed back on the submit server ( learn.chtc.wisc.edu ). Create a script called run_openbugs.sh . The script will first need to untar our installation, so the script should start out like this: \\ #/bin/bash tar -xzf openbugs.tar.gz \\ We're going to use the same $(pwd) trick from the installation in order to tell the computer how to find Open BUGS. We will \\ do this by setting the PATH environment variable, to include the directory where Open BUGS is installed: \\ export PATH=$(pwd)/openbugs/bin:$PATH \\ Finally, the wrapper script needs to not only setup Open BUGS, but actually run the program. Add the following lines to your run_openbugs.sh wrapper script. \\ OpenBUGS < input.txt > results.txt \\ Make sure the wrapper script has executable permissions: \\ %UCL_PROMPT_SHORT% <strong>chmod u+x run_openbugs.sh</strong> Run a Open BUGS job \u00b6 We're almost ready! We need two more pieces to run a OpenBUGS job. Download the necessary input files to your directory on the submit server and then untar them. \\ %UCL_PROMPT_SHORT% <strong>wget http://proxy.chtc.wisc.edu/SQUID/osgschool17/openbugs_files.tar.gz</strong> %UCL_PROMPT_SHORT% <strong>tar -xzf openbugs_files.tar.gz</strong> Our last step is to create a submit file for our Open BUGS job. Think about which lines this submit file will need. Make a copy of a previous submit file (you could use the blast submit file from the previous exercise as a base) and modify it as you think necessary. The two most important lines to modify for this job are listed below; check them against your own submit file: \\ executable = run_openbugs.sh transfer_input_files = openbugs.tar.gz, openbugs_files/ \\ A wrapper script will always be a job's executable . When using a wrapper script, you must also always remember to transfer the software/source code using transfer_input_files . \\ Note: the / in the transfer_input_files line indicates that we are transferring the contents of that directory (which in this case, is what we want), rather than the directory itself. We also need to add a requirement to ensure that the job will run on a Linux system, running major version 6. This can be done with the line: \\ requirements = (OpSys == \"LINUX\") && (OpSysMajorVer == 6) Submit the job with condor_submit . Once the job completes, it should produce a results.txt file.","title":"1.4. Pre-packaging code"},{"location":"materials/day3/part1-ex4-prepackaged/#wednesday-exercise-14-pre-packaging-code","text":"In this exercise, you will create an installation of a Bayesian inference package (OpenBUGS) and then create \\ a wrapper script to unpack that installation to run jobs. It should take 30-35 minutes.","title":"Wednesday Exercise 1.4: Pre-packaging Code"},{"location":"materials/day3/part1-ex4-prepackaged/#background","text":"Some software cannot be compiled into a single executable, whether you compile it yourself (as in Exercise 1.1 ) or download it already compiled (as in Exercise 1.2 ). In this case, it is necessary to download or create a portable copy of the software and then use a wrapper script (as in the previous exercise ) to \"install\" the software on a per job basis. This script can either install the software from the source code, or (as in this exercise), unpack a portable software package that you've pre-built yourself.","title":"Background"},{"location":"materials/day3/part1-ex4-prepackaged/#our-software-example","text":"For this exercise, we will be using the Bayseian inference package Open BUGS. Open BUGS is a good example of software that is not compiled to a single executable; it has multiple executables as well as a helper library. Do an internet search to find the Open BUGS software downloads page. Create a directory for this exercise on the CHTC submit server learn.chtc.wisc.edu ( not osg-learn.chtc.wisc.edu ), download the source code ( OpenBUGS-3.2.3.tar.gz ), and place it in this directory. Note: The OpenBUGS source tarball cannot be downloaded via wget=; you will need to download the source to your own computer and then transfer it to =learn.chtc.wisc.edu .","title":"Our Software Example"},{"location":"materials/day3/part1-ex4-prepackaged/#where-to-prepare","text":"Our goal is to pre-build an Open BUGS installation, and then write a script that will unpack that installation and run a simulation. Where can we create this pre-built installation? Based on the end of the lecture, what are our options and which would be most appropriate? Make a guess before moving on. Because we're on the CHTC-based submit node ( learn.chtc.wisc.edu ), we have the option of using an interactive job to build the Open BUGS installation. This is a good option because the submit server is already busy with lots of users and we don't know how long the Open BUGS install will take. To submit an interactive job do the following: Copy the following lines into a file named build.submit \\ universe = vanilla output = build.out error = build.err log = build.log should_transfer_files = YES when_to_transfer_output = ON_EXIT transfer_input_files = request_disk = 2GB request_memory = 2GB queue Note the lack of executable. Condor doesn't need an executable for this job because it will be interactive, meaning you are running the commands instead of Condor. In order to create the installation, we will need the source code to come with us. The transfer_input_files line is blank - fill it in with the name of our Open BUGS source tarball . To request an interactive job, we will add a -i flag to the condor_submit command. The whole command you enter should look like this: \\ %UCL_PROMPT_SHORT% <strong>condor_submit -i build.submit</strong>","title":"Where to Prepare"},{"location":"materials/day3/part1-ex4-prepackaged/#read-through-installation-documentation","text":"While you're waiting for the interactive job to start, you can start reading the Open BUGS installation documentation online. Find the installation instructions for Open BUGS. On the downloads page, there are short instructions for how to install Open BUGS. There are two options shown for installation -- which should we use? The first installation option given uses sudo -- which is an administrative permission that you won't have as a normal user. Luckily, as described in the \\ instructions, you can use the --prefix option to set where Open BUGS will be installed, which will allow us to install it without administrative permissions.","title":"Read Through Installation Documentation"},{"location":"materials/day3/part1-ex4-prepackaged/#installation","text":"Your interactive job should have started by now and we've learned about installing our program. Let's test it out. Before we follow the installation instructions, we should create a directory to hold our installation. You can create this in the current directory. \\ %UCL_PROMPT_SHORT% <strong>mkdir openbugs</strong> \\ Now run the commands to unpack the source code: \\ %UCL_PROMPT_SHORT% <strong>tar zxvf OpenBUGS-3.2.3.tar.gz</strong> %UCL_PROMPT_SHORT% <strong>cd OpenBUGS-3.2.3</strong> \\ Now we can follow the second set of installation instructions. For the prefix, we'll use the command $(pwd) to capture the name of our \\ current working directory and then a relative path to the openbugs directory we created in step 1: \\ %UCL_PROMPT_SHORT% <strong>./configure --prefix=$(pwd)/../openbugs</strong> %UCL_PROMPT_SHORT% <strong>make</strong> %UCL_PROMPT_SHORT% <strong>make install</strong> \\ Go back to the job's main working directory : \\ %UCL_PROMPT_SHORT% <strong>cd ..</strong> \\ and confirm that our installation procedure created bin , \\ lib , and share directories. \\ %UCL_PROMPT_SHORT% <strong>ls openbugs</strong> bin lib share \\ Now we want to package up our installation, so we can use it in other jobs. We can do this simply by compressing any necessary directories \\ into a single gzipped tarball. \\ %UCL_PROMPT_SHORT% <strong>tar -czf openbugs.tar.gz openbugs/</strong> \\ Once everything is complete, type exit to leave the interactive job. Make sure that your tarball is in the main working directory - it will be transferred back to the submit server automatically. \\ %UCL_PROMPT_SHORT% <strong>exit</strong> Note that we now have two tarballs in our directory -- the source tarball ( OpenBUGS-3.2.3.tar.gz ), which we will no longer need and our newly built installation ( openbugs.tar.gz ) which is what we will actually be using to run jobs.","title":"Installation"},{"location":"materials/day3/part1-ex4-prepackaged/#wrapper-script","text":"Now that we've created our portable installation, we need to write a script that opens and uses the installation, similar to the process we used in the \\ previous exercise . These steps should be performed back on the submit server ( learn.chtc.wisc.edu ). Create a script called run_openbugs.sh . The script will first need to untar our installation, so the script should start out like this: \\ #/bin/bash tar -xzf openbugs.tar.gz \\ We're going to use the same $(pwd) trick from the installation in order to tell the computer how to find Open BUGS. We will \\ do this by setting the PATH environment variable, to include the directory where Open BUGS is installed: \\ export PATH=$(pwd)/openbugs/bin:$PATH \\ Finally, the wrapper script needs to not only setup Open BUGS, but actually run the program. Add the following lines to your run_openbugs.sh wrapper script. \\ OpenBUGS < input.txt > results.txt \\ Make sure the wrapper script has executable permissions: \\ %UCL_PROMPT_SHORT% <strong>chmod u+x run_openbugs.sh</strong>","title":"Wrapper Script"},{"location":"materials/day3/part1-ex4-prepackaged/#run-a-open-bugs-job","text":"We're almost ready! We need two more pieces to run a OpenBUGS job. Download the necessary input files to your directory on the submit server and then untar them. \\ %UCL_PROMPT_SHORT% <strong>wget http://proxy.chtc.wisc.edu/SQUID/osgschool17/openbugs_files.tar.gz</strong> %UCL_PROMPT_SHORT% <strong>tar -xzf openbugs_files.tar.gz</strong> Our last step is to create a submit file for our Open BUGS job. Think about which lines this submit file will need. Make a copy of a previous submit file (you could use the blast submit file from the previous exercise as a base) and modify it as you think necessary. The two most important lines to modify for this job are listed below; check them against your own submit file: \\ executable = run_openbugs.sh transfer_input_files = openbugs.tar.gz, openbugs_files/ \\ A wrapper script will always be a job's executable . When using a wrapper script, you must also always remember to transfer the software/source code using transfer_input_files . \\ Note: the / in the transfer_input_files line indicates that we are transferring the contents of that directory (which in this case, is what we want), rather than the directory itself. We also need to add a requirement to ensure that the job will run on a Linux system, running major version 6. This can be done with the line: \\ requirements = (OpSys == \"LINUX\") && (OpSysMajorVer == 6) Submit the job with condor_submit . Once the job completes, it should produce a results.txt file.","title":"Run a Open BUGS job"},{"location":"materials/day3/part1-ex5-arguments/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Bonus: Passing Arguments Through the Wrapper Script \u00b6 In this exercise, you will change the wrapper script and submit file from the previous exercise to use arguments. Background \u00b6 So far, our wrapper scripts have had all files and options written out explicitly. However, imagine if you wanted to run the same job multiple times, or even just try out one or two different options or inputs. Instead of writing new wrapper scripts for each job, you can modify the script so that some of the values are set by arguments . Using script arguments will allow you to use the same script for multiple jobs, by providing different inputs or parameters. These arguments are normally passed on the command line: But in our world of job submission, the arguments will be listed in the submit file, in the arguments line. Identifying Potential Arguments \u00b6 Find the directory you used to submit Open BUGS jobs and open your run_openbugs.sh wrapper script. What values might we want to input to the script via arguments? Hint: anything that we might want to change if we were to run the script many times. In this example, some values we might want to change are the name of the input and output file. These will be the arguments for our script. Modifying Files \u00b6 Note the name of the input and output files and open the submit file. Add an arguments line if it doesn't already exist, and fill it with our two chosen arguments: the name of the input file and the name of the output file: \\ arguments = input.txt results.txt \\ Now go back to the wrapper script. Each scripting language (bash, perl, python, R, etc.) will have its own particular syntax \\ for capturing command line arguments. For bash (the language of our current wrapper script), the variables $1 and $2 represent \\ the first and second arguments, respectively. (If our script needed three arguments, we would use $3 for the third one). Thus, in \\ the main command of the script, replace the file names with these variables: \\ OpenBUGS < $1 > $2 \\ Once these changes are made, submit your jobs with condor_submit . Use condor_q -nobatch to see what the job command looks like to HTCondor.","title":"1.5. (opt) Arguments"},{"location":"materials/day3/part1-ex5-arguments/#bonus-passing-arguments-through-the-wrapper-script","text":"In this exercise, you will change the wrapper script and submit file from the previous exercise to use arguments.","title":"Bonus: Passing Arguments Through the Wrapper Script"},{"location":"materials/day3/part1-ex5-arguments/#background","text":"So far, our wrapper scripts have had all files and options written out explicitly. However, imagine if you wanted to run the same job multiple times, or even just try out one or two different options or inputs. Instead of writing new wrapper scripts for each job, you can modify the script so that some of the values are set by arguments . Using script arguments will allow you to use the same script for multiple jobs, by providing different inputs or parameters. These arguments are normally passed on the command line: But in our world of job submission, the arguments will be listed in the submit file, in the arguments line.","title":"Background"},{"location":"materials/day3/part1-ex5-arguments/#identifying-potential-arguments","text":"Find the directory you used to submit Open BUGS jobs and open your run_openbugs.sh wrapper script. What values might we want to input to the script via arguments? Hint: anything that we might want to change if we were to run the script many times. In this example, some values we might want to change are the name of the input and output file. These will be the arguments for our script.","title":"Identifying Potential Arguments"},{"location":"materials/day3/part1-ex5-arguments/#modifying-files","text":"Note the name of the input and output files and open the submit file. Add an arguments line if it doesn't already exist, and fill it with our two chosen arguments: the name of the input file and the name of the output file: \\ arguments = input.txt results.txt \\ Now go back to the wrapper script. Each scripting language (bash, perl, python, R, etc.) will have its own particular syntax \\ for capturing command line arguments. For bash (the language of our current wrapper script), the variables $1 and $2 represent \\ the first and second arguments, respectively. (If our script needed three arguments, we would use $3 for the third one). Thus, in \\ the main command of the script, replace the file names with these variables: \\ OpenBUGS < $1 > $2 \\ Once these changes are made, submit your jobs with condor_submit . Use condor_q -nobatch to see what the job command looks like to HTCondor.","title":"Modifying Files"},{"location":"materials/day3/part2-ex1-matlab/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Wednesday Exercise 1.6: Matlab \u00b6 The goal of this exercise is to compile Matlab code and run it. This exercise will draw on the idea of writing a wrapper script to install and run code, first introduced in Exercise 1.3 and should take 25-30 minutes. Background \u00b6 Matlab is licensed; however, unlike most licensed software, it has the ability to be compiled and the compiled code can be run without a license. We will be compiling Matlab .m files into a binary file and running that binary using a set of files called the Matlab runtime. Matlab Code \u00b6 Log in to the CHTC submit server ( learn.chtc.wisc.edu ). Create a directory for this exercise and cd into it . 2. Copy the following code into a file called matrix.m \\ \\ A = randi(100,4,4) b = randi(100,4,1); x = A\\b save results.txt x -ascii \\ Compiling Matlab Code \u00b6 The first step in making Matlab portable is compiling our Matlab script. To compile this code, we need to access the machines with the Matlab compiler installed. For this exercise, we will use the compilers installed on special CHTC build machines. In the CHTC pool, you can't use ssh to directly connect to these machines. Instead, you must submit an interactive job (like in Exercise 1.4 ) that specifically requests these build machines. Create a file called compile.submit with the lines below: \\ \\ universe = vanilla output = compile.out error = compile.err log = compile.log should_transfer_files = YES when_to_transfer_output = ON_EXIT transfer_input_files = matrix.m +IsBuildJob = true requirements = (OpSysMajorVer == 6) && ( IsBuildSlot == true ) request_memory = 1GB request_disk = 100MB queue \\ You can initiate the interactive job by using condor_submit 's -i option. Enter the following command: \\ \\ %UCL_PROMPT_SHORT% <strong>condor_submit -i compile.submit</strong> \\ Make sure you've submitted this command from learn.chtc.wisc.edu ! Once the job starts, continue with the following instructions. Since you are a guest user on our system, you will need to set your HOME directory by running this command: \\ %UCL_PROMPT_SHORT% <strong>export HOME=$PWD</strong> \\ Compile your Matlab code with version 2015b. \\ %UCL_PROMPT_SHORT% <strong>/usr/local/MATLAB/R2015b/bin/mcc -m -R -singleCompThread -R -nodisplay -R -nojvm matrix.m</strong> \\ The extra arguments to the mcc command are very important here. Matlab, by default, will run on as many CPUs as it can find. This can be a big problem \\ when running on someone else's computers, because your Matlab code might interfere with what the owner wants. The -singleCompThread option \\ compiles the code to run on a single CPU, avoiding this problem. In addition, the -nodisplay and -nojvm options turn off the display (which won't exist \\ where the code runs). \\ To exit the interactive session, type exit Now that you're back on the submit server, look at the files that were created by the Matlab compiler. Which one is the compiled binary? Matlab Runtime \u00b6 The newly compiled binary will require the 2015b Matlab runtime to run. You can download the runtime from the Mathworks website and build it yourself, but to save time, for this exercise you can download the pre-built runtimes hosted by CHTC. Download the 2015b Matlab runtime hosted by CHTC: \\ %UCL_PROMPT_SHORT% <strong>wget http://proxy.chtc.wisc.edu/SQUID/r2015b.tar.gz </strong> \\ Wrapper Script \u00b6 Like the OpenBUGS example from earlier this morning, we will need a wrapper script to open the Matlab runtime and then run our compiled Matlab code. Our wrapper script will need to accomplish the following steps: Unpack the transferred runtime Set the environment variables Run our compiled matlab code Fortunately, the Matlab compiler has pre-written most of this wrapper script for us! Take a look at run_matrix.sh . Which of the above steps do we need to add? Once you have an idea, move to the next step.\\ We'll need to add commands to unpack the runtime (which will have been transferred with the job). Add this line to the beginning of the run_matrix.sh file, after #!/bin/bash but before exe_name=$0 : \\ tar -xzf r2015b.tar.gz \\ Look at readme.txt to determine what arguments our wrapper script requires. Once you have an idea, move to the next step.\\ The name of the Matlab runtime directory is a required argument to the wrapper script run_matrix.sh . We'll have to do a little extra work to find out the name of that directory. 2. Untar the runtime tarball we downloaded to the submit server. 3. What directory has been created? This is the name of the runtime directory, and the argument you should pass to run_matrix.sh . Submitting the Job \u00b6 Copy an existing submit file into your current directory. The submit file we used for the Open Bugs example would be a good candidate, as that example also used a wrapper script. 2. Modify your submit file for this job. 3. Check your changes against the list below. The executable for this job is going to be our wrapper script run_matrix.sh . \\ executable = run_matrix.sh You need to transfer the compiled binary matrix , as well as the runtime .tar.gz file, using transfer_input_files . \\ transfer_input_files = matrix, r2015b.tar.gz The argument for the executable ( run_matrix.sh ) is \"v90\", as that is the name of the un-tarred runtime directory. \\ arguments = v90 Look at the size of the untarred runtime directory by running: \\ %UCL_PROMPT_SHORT% <strong>du -sh v90/</strong> \\ We need to request at least this much \\ disk space in our submit file: \\ request_disk = 2GB \\ Submit the job using condor_submit . 4. After it completes, the job should have produced a file called results.txt","title":"2.1. Using MATLAB"},{"location":"materials/day3/part2-ex1-matlab/#wednesday-exercise-16-matlab","text":"The goal of this exercise is to compile Matlab code and run it. This exercise will draw on the idea of writing a wrapper script to install and run code, first introduced in Exercise 1.3 and should take 25-30 minutes.","title":"Wednesday Exercise 1.6: Matlab"},{"location":"materials/day3/part2-ex1-matlab/#background","text":"Matlab is licensed; however, unlike most licensed software, it has the ability to be compiled and the compiled code can be run without a license. We will be compiling Matlab .m files into a binary file and running that binary using a set of files called the Matlab runtime.","title":"Background"},{"location":"materials/day3/part2-ex1-matlab/#matlab-code","text":"Log in to the CHTC submit server ( learn.chtc.wisc.edu ). Create a directory for this exercise and cd into it . 2. Copy the following code into a file called matrix.m \\ \\ A = randi(100,4,4) b = randi(100,4,1); x = A\\b save results.txt x -ascii \\","title":"Matlab Code"},{"location":"materials/day3/part2-ex1-matlab/#compiling-matlab-code","text":"The first step in making Matlab portable is compiling our Matlab script. To compile this code, we need to access the machines with the Matlab compiler installed. For this exercise, we will use the compilers installed on special CHTC build machines. In the CHTC pool, you can't use ssh to directly connect to these machines. Instead, you must submit an interactive job (like in Exercise 1.4 ) that specifically requests these build machines. Create a file called compile.submit with the lines below: \\ \\ universe = vanilla output = compile.out error = compile.err log = compile.log should_transfer_files = YES when_to_transfer_output = ON_EXIT transfer_input_files = matrix.m +IsBuildJob = true requirements = (OpSysMajorVer == 6) && ( IsBuildSlot == true ) request_memory = 1GB request_disk = 100MB queue \\ You can initiate the interactive job by using condor_submit 's -i option. Enter the following command: \\ \\ %UCL_PROMPT_SHORT% <strong>condor_submit -i compile.submit</strong> \\ Make sure you've submitted this command from learn.chtc.wisc.edu ! Once the job starts, continue with the following instructions. Since you are a guest user on our system, you will need to set your HOME directory by running this command: \\ %UCL_PROMPT_SHORT% <strong>export HOME=$PWD</strong> \\ Compile your Matlab code with version 2015b. \\ %UCL_PROMPT_SHORT% <strong>/usr/local/MATLAB/R2015b/bin/mcc -m -R -singleCompThread -R -nodisplay -R -nojvm matrix.m</strong> \\ The extra arguments to the mcc command are very important here. Matlab, by default, will run on as many CPUs as it can find. This can be a big problem \\ when running on someone else's computers, because your Matlab code might interfere with what the owner wants. The -singleCompThread option \\ compiles the code to run on a single CPU, avoiding this problem. In addition, the -nodisplay and -nojvm options turn off the display (which won't exist \\ where the code runs). \\ To exit the interactive session, type exit Now that you're back on the submit server, look at the files that were created by the Matlab compiler. Which one is the compiled binary?","title":"Compiling Matlab Code"},{"location":"materials/day3/part2-ex1-matlab/#matlab-runtime","text":"The newly compiled binary will require the 2015b Matlab runtime to run. You can download the runtime from the Mathworks website and build it yourself, but to save time, for this exercise you can download the pre-built runtimes hosted by CHTC. Download the 2015b Matlab runtime hosted by CHTC: \\ %UCL_PROMPT_SHORT% <strong>wget http://proxy.chtc.wisc.edu/SQUID/r2015b.tar.gz </strong> \\","title":"Matlab Runtime"},{"location":"materials/day3/part2-ex1-matlab/#wrapper-script","text":"Like the OpenBUGS example from earlier this morning, we will need a wrapper script to open the Matlab runtime and then run our compiled Matlab code. Our wrapper script will need to accomplish the following steps: Unpack the transferred runtime Set the environment variables Run our compiled matlab code Fortunately, the Matlab compiler has pre-written most of this wrapper script for us! Take a look at run_matrix.sh . Which of the above steps do we need to add? Once you have an idea, move to the next step.\\ We'll need to add commands to unpack the runtime (which will have been transferred with the job). Add this line to the beginning of the run_matrix.sh file, after #!/bin/bash but before exe_name=$0 : \\ tar -xzf r2015b.tar.gz \\ Look at readme.txt to determine what arguments our wrapper script requires. Once you have an idea, move to the next step.\\ The name of the Matlab runtime directory is a required argument to the wrapper script run_matrix.sh . We'll have to do a little extra work to find out the name of that directory. 2. Untar the runtime tarball we downloaded to the submit server. 3. What directory has been created? This is the name of the runtime directory, and the argument you should pass to run_matrix.sh .","title":"Wrapper Script"},{"location":"materials/day3/part2-ex1-matlab/#submitting-the-job","text":"Copy an existing submit file into your current directory. The submit file we used for the Open Bugs example would be a good candidate, as that example also used a wrapper script. 2. Modify your submit file for this job. 3. Check your changes against the list below. The executable for this job is going to be our wrapper script run_matrix.sh . \\ executable = run_matrix.sh You need to transfer the compiled binary matrix , as well as the runtime .tar.gz file, using transfer_input_files . \\ transfer_input_files = matrix, r2015b.tar.gz The argument for the executable ( run_matrix.sh ) is \"v90\", as that is the name of the un-tarred runtime directory. \\ arguments = v90 Look at the size of the untarred runtime directory by running: \\ %UCL_PROMPT_SHORT% <strong>du -sh v90/</strong> \\ We need to request at least this much \\ disk space in our submit file: \\ request_disk = 2GB \\ Submit the job using condor_submit . 4. After it completes, the job should have produced a file called results.txt","title":"Submitting the Job"},{"location":"materials/day3/part2-ex2-python-built/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Wednesday Exercise 1.7: Using Python, Pre-Built \u00b6 In this exercise, you will install Python, package your installation, and then use it to run jobs. It should take about 20 minutes. Background \u00b6 We chose Python as the language for this example because: a) it is a common language used for scientific computing and b) it has a straightforward installation process and is fairly portable. Running any Python script requires an installation of the Python interpreter. The Python interpreter is what we're using when we type python at the command line. In order to run Python jobs on a distributed system, you will need to install the Python interpreter (what we often refer to as just \"installing Python\"), within the job, then run your Python script. There are two installation approaches. The approach we will cover in this exercise is that of \"pre-building\" the installation (much like we did with Open BUGS thir morning ). We will install Python to a specific directory, and then create a tarball of that installation directory. We can then use our tarball within jobs to run Python scripts. Interactive Job for Pre-Building \u00b6 The first step in our job process is building a Python installation that we can package up. Create a directory for this exercise on learn.chtc.wisc.edu and cd into it. Download the Python source code from https://www.python.org/ . \\ %UCL_PROMPT_SHORT% <strong>wget https://www.python.org/ftp/python/3.6.1/Python-3.6.1.tgz</strong> Of our options - submit server, interactive job, personal computer - which should we use for this installation/packaging process? Once you have a guess, move to the next step. Due to the number of people on our submit server, we shouldn't use the submit server. Your own computer probably doesn't have the right operating system. \\ The best place to install will be an interactive job. For this job, we can use the same interactive submit file as Exercise 1.4, with one change. What is it? Make a copy of the interactive submit file from Exercise 1.4 and change the transfer_input_files line to the Python tarball you just downloaded. Then submit it using the -i flag. \\ %UCL_PROMPT_SHORT% <strong>condor_submit -i build.submit</strong> Once the interactive job begins, we can start our installation process. First, we have to determine how to install Python to a specific location in our working directory. Untar the Python source tarball and look at the README in the Python-3.6.1 directory. What will the main installation steps be? Where will Python be installed by default? Once you've tried to answer these questions, move to the next step. There are some basic installation \\ instructions near the top of the README . Based on that short introduction, we can see the main steps of installation will be: \\ ./configure make make install \\ \\ This looks a lot like the Open BUGS installation from earlier today! It turns out that this three-stage process (configure, make, make install) is a common \\ way to install many software packages. \\ \\ Also like the Open BUGS installation, the default installation \\ location for Python requires sudo (administrative privileges) to install. However, we'd like to install to a specific location in the working directory \\ so that we can compress that installation directory into a tarball. How did we do this with Open BUGS? \\ With Open BUGS we used the -prefix option with the configure script. Let's see if the Python \\ configure script has this option by using the \"help\" option. \\ \\ %UCL_PROMPT_SHORT% <strong>./configure --help</strong> \\ \\ Sure enough, there's a list of all the different options that can be passed to the configure script, which includes -prefix . Therefore, we can use the \\ $(pwd) command in order to set the path correctly, just as we did earlier today. Now let's actually install Python! From the job's main working directory , create a directory to hold the installation. \\ %UCL_PROMPT_SHORT% <strong>cd $_CONDOR_SCRATCH_DIR</strong> %UCL_PROMPT_SHORT% <strong>mkdir python</strong> Move into the Python 3.6.1 directory and run the installation commands. These may take a few minutes each. \\ %UCL_PROMPT_SHORT% <strong>cd Python-3.6.1</strong> %UCL_PROMPT_SHORT% <strong>./configure --prefix=$(pwd)/../python</strong> %UCL_PROMPT_SHORT% <strong>make</strong> %UCL_PROMPT_SHORT% <strong>make install</strong> If I move back to the main job working directory, and look in the python subdirectory, I should see a Python installation. \\ %UCL_PROMPT_SHORT% <strong>cd ..</strong> %UCL_PROMPT_SHORT% <strong>ls python/</strong> bin include lib share I have successfully created a self-contained Python installation. Now it just needs to be tarred up! \\ %UCL_PROMPT_SHORT% <strong>tar -czf prebuilt_python.tar.gz python/</strong> Before exiting, we might want to know how we installed Python for later reference. \\ Enter the following commands to save our history to a file: \\ %UCL_PROMPT_SHORT% <strong>history > python_install.txt</strong> Exit the interactive job. \\ %UCL_PROMPT_SHORT% <strong>exit</strong> Python Script \u00b6 Create a script with the following lines called fib.py . \\ import sys import os if len(sys.argv) != 2: print('Usage: %s MAXIMUM' % (os.path.basename(sys.argv[0]))) sys.exit(1) maximum = int(sys.argv[1]) n1 = n2 = 1 while n2 <= maximum: n1, n2 = n2, n1 + n2 print('The greatest Fibonacci number up to %d is %d' % (maximum, n1)) What command line arguments does this script take? Try running it on the submit server. Wrapper Script \u00b6 We now have our Python installation and our Python script - we just need to write a wrapper script to run them. What steps do you think the wrapper script needs to perform? Create a file called run_fib.sh and write them out in plain English before moving to the next step. Our script will need to untar our prebuilt_python.tar.gz file access the python command from our installation to run our fib.py script Try turning your plain English steps into commands that the computer can run. Your final run_fib.sh script should look something like this: \\ #/bin/bash tar xzf prebuilt_python.tar.gz python/bin/python3 fib.py 5 \\ or\\ #/bin/bash tar xzf prebuilt_python.tar.gz export PATH=$(pwd)/python/bin:$PATH python3 fib.py 5 \\ Make sure your run_fib.sh script is executable. Submit File \u00b6 Make a copy of a previous submit file in your local directory (the Open BUGS submit file could be a good starting point). What changes need to be made to run this Python job? 2. Modify your submit file, then make sure you've included the key lines below: \\ executable = run_fib.sh transfer_input_files = fib.py, prebuilt_python.tar.gz Because we pre-built our Python installation on a machine running Scientific Linux, version 6.something, we should request machines with similar characteristics. Add the following line to your submit file as well: \\ requirements = (OpSys == \"LINUX\" && OpSysMajorVer == 6 ) Submit the job using condor_submit . 4. Check the .out file to see if the job completed.","title":"2.2. Packaging Python"},{"location":"materials/day3/part2-ex2-python-built/#wednesday-exercise-17-using-python-pre-built","text":"In this exercise, you will install Python, package your installation, and then use it to run jobs. It should take about 20 minutes.","title":"Wednesday Exercise 1.7: Using Python, Pre-Built"},{"location":"materials/day3/part2-ex2-python-built/#background","text":"We chose Python as the language for this example because: a) it is a common language used for scientific computing and b) it has a straightforward installation process and is fairly portable. Running any Python script requires an installation of the Python interpreter. The Python interpreter is what we're using when we type python at the command line. In order to run Python jobs on a distributed system, you will need to install the Python interpreter (what we often refer to as just \"installing Python\"), within the job, then run your Python script. There are two installation approaches. The approach we will cover in this exercise is that of \"pre-building\" the installation (much like we did with Open BUGS thir morning ). We will install Python to a specific directory, and then create a tarball of that installation directory. We can then use our tarball within jobs to run Python scripts.","title":"Background"},{"location":"materials/day3/part2-ex2-python-built/#interactive-job-for-pre-building","text":"The first step in our job process is building a Python installation that we can package up. Create a directory for this exercise on learn.chtc.wisc.edu and cd into it. Download the Python source code from https://www.python.org/ . \\ %UCL_PROMPT_SHORT% <strong>wget https://www.python.org/ftp/python/3.6.1/Python-3.6.1.tgz</strong> Of our options - submit server, interactive job, personal computer - which should we use for this installation/packaging process? Once you have a guess, move to the next step. Due to the number of people on our submit server, we shouldn't use the submit server. Your own computer probably doesn't have the right operating system. \\ The best place to install will be an interactive job. For this job, we can use the same interactive submit file as Exercise 1.4, with one change. What is it? Make a copy of the interactive submit file from Exercise 1.4 and change the transfer_input_files line to the Python tarball you just downloaded. Then submit it using the -i flag. \\ %UCL_PROMPT_SHORT% <strong>condor_submit -i build.submit</strong> Once the interactive job begins, we can start our installation process. First, we have to determine how to install Python to a specific location in our working directory. Untar the Python source tarball and look at the README in the Python-3.6.1 directory. What will the main installation steps be? Where will Python be installed by default? Once you've tried to answer these questions, move to the next step. There are some basic installation \\ instructions near the top of the README . Based on that short introduction, we can see the main steps of installation will be: \\ ./configure make make install \\ \\ This looks a lot like the Open BUGS installation from earlier today! It turns out that this three-stage process (configure, make, make install) is a common \\ way to install many software packages. \\ \\ Also like the Open BUGS installation, the default installation \\ location for Python requires sudo (administrative privileges) to install. However, we'd like to install to a specific location in the working directory \\ so that we can compress that installation directory into a tarball. How did we do this with Open BUGS? \\ With Open BUGS we used the -prefix option with the configure script. Let's see if the Python \\ configure script has this option by using the \"help\" option. \\ \\ %UCL_PROMPT_SHORT% <strong>./configure --help</strong> \\ \\ Sure enough, there's a list of all the different options that can be passed to the configure script, which includes -prefix . Therefore, we can use the \\ $(pwd) command in order to set the path correctly, just as we did earlier today. Now let's actually install Python! From the job's main working directory , create a directory to hold the installation. \\ %UCL_PROMPT_SHORT% <strong>cd $_CONDOR_SCRATCH_DIR</strong> %UCL_PROMPT_SHORT% <strong>mkdir python</strong> Move into the Python 3.6.1 directory and run the installation commands. These may take a few minutes each. \\ %UCL_PROMPT_SHORT% <strong>cd Python-3.6.1</strong> %UCL_PROMPT_SHORT% <strong>./configure --prefix=$(pwd)/../python</strong> %UCL_PROMPT_SHORT% <strong>make</strong> %UCL_PROMPT_SHORT% <strong>make install</strong> If I move back to the main job working directory, and look in the python subdirectory, I should see a Python installation. \\ %UCL_PROMPT_SHORT% <strong>cd ..</strong> %UCL_PROMPT_SHORT% <strong>ls python/</strong> bin include lib share I have successfully created a self-contained Python installation. Now it just needs to be tarred up! \\ %UCL_PROMPT_SHORT% <strong>tar -czf prebuilt_python.tar.gz python/</strong> Before exiting, we might want to know how we installed Python for later reference. \\ Enter the following commands to save our history to a file: \\ %UCL_PROMPT_SHORT% <strong>history > python_install.txt</strong> Exit the interactive job. \\ %UCL_PROMPT_SHORT% <strong>exit</strong>","title":"Interactive Job for Pre-Building"},{"location":"materials/day3/part2-ex2-python-built/#python-script","text":"Create a script with the following lines called fib.py . \\ import sys import os if len(sys.argv) != 2: print('Usage: %s MAXIMUM' % (os.path.basename(sys.argv[0]))) sys.exit(1) maximum = int(sys.argv[1]) n1 = n2 = 1 while n2 <= maximum: n1, n2 = n2, n1 + n2 print('The greatest Fibonacci number up to %d is %d' % (maximum, n1)) What command line arguments does this script take? Try running it on the submit server.","title":"Python Script"},{"location":"materials/day3/part2-ex2-python-built/#wrapper-script","text":"We now have our Python installation and our Python script - we just need to write a wrapper script to run them. What steps do you think the wrapper script needs to perform? Create a file called run_fib.sh and write them out in plain English before moving to the next step. Our script will need to untar our prebuilt_python.tar.gz file access the python command from our installation to run our fib.py script Try turning your plain English steps into commands that the computer can run. Your final run_fib.sh script should look something like this: \\ #/bin/bash tar xzf prebuilt_python.tar.gz python/bin/python3 fib.py 5 \\ or\\ #/bin/bash tar xzf prebuilt_python.tar.gz export PATH=$(pwd)/python/bin:$PATH python3 fib.py 5 \\ Make sure your run_fib.sh script is executable.","title":"Wrapper Script"},{"location":"materials/day3/part2-ex2-python-built/#submit-file","text":"Make a copy of a previous submit file in your local directory (the Open BUGS submit file could be a good starting point). What changes need to be made to run this Python job? 2. Modify your submit file, then make sure you've included the key lines below: \\ executable = run_fib.sh transfer_input_files = fib.py, prebuilt_python.tar.gz Because we pre-built our Python installation on a machine running Scientific Linux, version 6.something, we should request machines with similar characteristics. Add the following line to your submit file as well: \\ requirements = (OpSys == \"LINUX\" && OpSysMajorVer == 6 ) Submit the job using condor_submit . 4. Check the .out file to see if the job completed.","title":"Submit File"},{"location":"materials/day3/part2-ex3-python-install/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Wednesday Exercise 1.8: Python Installation \u00b6 In this exercise, you will write a wrapper script that installs Python and then use it to run jobs. This exercise should take about 10 minutes. Background \u00b6 In the previous exercise, we used a method that pre-built Python and then used that pre-built package to run Python scripts. In this exercise, we will use an alternative method for running Python jobs, by writing a wrapper script that installs Python with every job. This exercise should be done in the same directory as the previous exercise - you will need the same Python source code and fib.py script. Wrapper script \u00b6 Our wrapper script will need to install Python from the source code and then run our fib.py script. Based on the previous exercise, what are the steps we need to install Python? What file can we use for reference? We put our installation steps from the previous exercise into a file called python_install.txt . Based on this, put the installation steps into a script called run_py.sh 2. Check your script against the file below \\ #!/bin/bash export PATH mkdir python tar xzf Python-3.6.1.tgz cd Python-3.6.1 ./configure --prefix=$(pwd)/../python make make install cd .. \\ We also need to run our fib.py script. We can do so by adding our installation location to the PATH , or by referencing the installation directly: export PATH=$(pwd)/python/bin:$PATH python3 fib.py 5 \\ or\\ python/bin/python3 fib.py 5 \\ Choose whichever method you prefer, and add it to your run_py.sh script. Make your run_py.sh script executable. Submit file \u00b6 The submit file for this exercise can be very similar to the last one from Exercise 1.7 . Make a copy of the submit file from the last exercise. What lines need to change? Make changes as appropriate. You need to change the transferred tarball (the Python source, instead of our prebuilt_python.tar.gz ) and the job's executable. Once you've made these changes, submit the job using condor_submit . Check for the results in the .out file.","title":"2.3. Installing Python"},{"location":"materials/day3/part2-ex3-python-install/#wednesday-exercise-18-python-installation","text":"In this exercise, you will write a wrapper script that installs Python and then use it to run jobs. This exercise should take about 10 minutes.","title":"Wednesday Exercise 1.8: Python Installation"},{"location":"materials/day3/part2-ex3-python-install/#background","text":"In the previous exercise, we used a method that pre-built Python and then used that pre-built package to run Python scripts. In this exercise, we will use an alternative method for running Python jobs, by writing a wrapper script that installs Python with every job. This exercise should be done in the same directory as the previous exercise - you will need the same Python source code and fib.py script.","title":"Background"},{"location":"materials/day3/part2-ex3-python-install/#wrapper-script","text":"Our wrapper script will need to install Python from the source code and then run our fib.py script. Based on the previous exercise, what are the steps we need to install Python? What file can we use for reference? We put our installation steps from the previous exercise into a file called python_install.txt . Based on this, put the installation steps into a script called run_py.sh 2. Check your script against the file below \\ #!/bin/bash export PATH mkdir python tar xzf Python-3.6.1.tgz cd Python-3.6.1 ./configure --prefix=$(pwd)/../python make make install cd .. \\ We also need to run our fib.py script. We can do so by adding our installation location to the PATH , or by referencing the installation directly: export PATH=$(pwd)/python/bin:$PATH python3 fib.py 5 \\ or\\ python/bin/python3 fib.py 5 \\ Choose whichever method you prefer, and add it to your run_py.sh script. Make your run_py.sh script executable.","title":"Wrapper script"},{"location":"materials/day3/part2-ex3-python-install/#submit-file","text":"The submit file for this exercise can be very similar to the last one from Exercise 1.7 . Make a copy of the submit file from the last exercise. What lines need to change? Make changes as appropriate. You need to change the transferred tarball (the Python source, instead of our prebuilt_python.tar.gz ) and the job's executable. Once you've made these changes, submit the job using condor_submit . Check for the results in the .out file.","title":"Submit file"},{"location":"materials/day3/part2-ex4-containers/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Wednesday Exercise 1.9: Using Docker \u00b6 In this exercise, you will install run the same Python script as the previous exercises, but using a Docker container. Background \u00b6 Containers are another way to manage software installations. We don't have the time to go fully into the details of building and using containers, but can use a publicly available Python container to run our Python job. One caveat for using containers: not all systems will support them. HTCondor has built-in features for using Docker and many Open Science Grid resources have Singularity installed, but they are still not in widespread use. Submit File Changes \u00b6 Make a copy of your submit file from the previous exercise . Add the following three lines to the submit file or modify existing lines to match the lines below: \\ universe = docker docker_image = python requirements = (OpSysMajorVer == 7) \\ Here we are requesting HTCondor's Docker universe and using a pre-built python image that, by default, will be pulled from a public website of Docker images called DockerHub. \\ The requirements line will ensure that we run on computers whose operating system can support Docker. Adjust the executable and arguments lines. The executable can now be the Python script itself, with the appropriate arguments: \\ executable = fib.py arguments = 5 Finally, we no longer need to transfer a Python tarball (whether source code or pre-built) or our Python script. You can remove both from the transfer_input_files \\ line of the submit file. Python Script \u00b6 Open the Python script and add the following line at the top: \\ #!/usr/bin/env python \\ This will ensure that the script uses the version of Python that comes in the Docker container. Once these steps are done, submit the job.","title":"2.4. (opt) Containers"},{"location":"materials/day3/part2-ex4-containers/#wednesday-exercise-19-using-docker","text":"In this exercise, you will install run the same Python script as the previous exercises, but using a Docker container.","title":"Wednesday Exercise 1.9: Using Docker"},{"location":"materials/day3/part2-ex4-containers/#background","text":"Containers are another way to manage software installations. We don't have the time to go fully into the details of building and using containers, but can use a publicly available Python container to run our Python job. One caveat for using containers: not all systems will support them. HTCondor has built-in features for using Docker and many Open Science Grid resources have Singularity installed, but they are still not in widespread use.","title":"Background"},{"location":"materials/day3/part2-ex4-containers/#submit-file-changes","text":"Make a copy of your submit file from the previous exercise . Add the following three lines to the submit file or modify existing lines to match the lines below: \\ universe = docker docker_image = python requirements = (OpSysMajorVer == 7) \\ Here we are requesting HTCondor's Docker universe and using a pre-built python image that, by default, will be pulled from a public website of Docker images called DockerHub. \\ The requirements line will ensure that we run on computers whose operating system can support Docker. Adjust the executable and arguments lines. The executable can now be the Python script itself, with the appropriate arguments: \\ executable = fib.py arguments = 5 Finally, we no longer need to transfer a Python tarball (whether source code or pre-built) or our Python script. You can remove both from the transfer_input_files \\ line of the submit file.","title":"Submit File Changes"},{"location":"materials/day3/part2-ex4-containers/#python-script","text":"Open the Python script and add the following line at the top: \\ #!/usr/bin/env python \\ This will ensure that the script uses the version of Python that comes in the Docker container. Once these steps are done, submit the job.","title":"Python Script"},{"location":"materials/day4/part1-ex1-connect-intro/","text":"Thursday Exercise 1.1: Getting Acquainted with OSG Connect \u00b6 In this exercise, you'll get acquainted with the OSG Connect submit server (user-training.osgconnect.net), and the OSG Connect helpdesk. Get Acquainted with the OSG Connect Submit Server (user-training.osgconnect.net) \u00b6 SSH to username@user-training.osgconnect.net in a command-line terminal. Look around your 'home' directory \u00b6 You'll notice the 'public', and 'stash' directories, which may be used in various specific OSG Connect guides. You'll use these spaces for different reasons. Make sure to read the OSG Connect helpdesk guides on data management and storage solutions . Run condor_status \u00b6 Running condor_status by itself on the OSG Connect submit host will give no output, but you can otherwise check with: %UCL_PROMPT_SHORT% <strong>condor_status -pool osg-flock.grid.iu.edu</strong> Visit the Helpdesk Materials \u00b6 Visit the Help desk page ( https://support.opensciencegrid.org/ ) and take a look at the knowledge base articles. You will be using some of these in the next exercises. Move on to the next exercise when you're ready to submit jobs! \u00b6","title":"1.1. OSG Connect"},{"location":"materials/day4/part1-ex1-connect-intro/#thursday-exercise-11-getting-acquainted-with-osg-connect","text":"In this exercise, you'll get acquainted with the OSG Connect submit server (user-training.osgconnect.net), and the OSG Connect helpdesk.","title":"Thursday Exercise 1.1: Getting Acquainted with OSG Connect"},{"location":"materials/day4/part1-ex1-connect-intro/#get-acquainted-with-the-osg-connect-submit-server-user-trainingosgconnectnet","text":"SSH to username@user-training.osgconnect.net in a command-line terminal.","title":"Get Acquainted with the OSG Connect Submit Server (user-training.osgconnect.net)"},{"location":"materials/day4/part1-ex1-connect-intro/#look-around-your-home-directory","text":"You'll notice the 'public', and 'stash' directories, which may be used in various specific OSG Connect guides. You'll use these spaces for different reasons. Make sure to read the OSG Connect helpdesk guides on data management and storage solutions .","title":"Look around your 'home' directory"},{"location":"materials/day4/part1-ex1-connect-intro/#run-condor_status","text":"Running condor_status by itself on the OSG Connect submit host will give no output, but you can otherwise check with: %UCL_PROMPT_SHORT% <strong>condor_status -pool osg-flock.grid.iu.edu</strong>","title":"Run condor_status"},{"location":"materials/day4/part1-ex1-connect-intro/#visit-the-helpdesk-materials","text":"Visit the Help desk page ( https://support.opensciencegrid.org/ ) and take a look at the knowledge base articles. You will be using some of these in the next exercises.","title":"Visit the Helpdesk Materials"},{"location":"materials/day4/part1-ex1-connect-intro/#move-on-to-the-next-exercise-when-youre-ready-to-submit-jobs","text":"","title":"Move on to the next exercise when you're ready to submit jobs!"},{"location":"materials/day4/part1-ex2-connect-quickstart/","text":"--+ !Thursday Exercise 1.2: Do the OSG Connect Quickstart For this exercise, you can also follow the online guide from the OSG Connect helpdesk that will acquaint you with submission on the OSG Connect submit server. Please use the submit host user-training.osgconnect.net instead of login.osgconnect.net for the workshop. Setup \u00b6 SSH into user-training.osgconnect.net (the OSG Connect submit server for this workshop). Get the example files for the tutorial \"OSG Connect Quickstart\" via tutorial command \u00b6 We will get the example files using the tutorial command. %UCL_PROMPT_SHORT% <strong>tutorial quickstart</strong> This creates a directory tutorial-quickstart . Go inside the directory and see what is inside. %UCL_PROMPT_SHORT% <strong>cd tutorial-quickstart</strong> %UCL_PROMPT_SHORT% <strong>ls -F </strong> You will see the following contents: Images/ osg-template-job.submit short.sh tutorial02.submit log/ README.md tutorial01.submit tutorial03.submit We will focus our attention on short.sh (execution file) and tutorial01.submit . We will not worry about other files in this exercise. Feel free to take a look at other files if you are interested. Job Execution File \u00b6 Take a look at the job execution file short.sh . %UCL_PROMPT_SHORT% <strong>cat short.sh </strong> This is a shell script, quite ordinary . Run this shell script locally to see what it does. %UCL_PROMPT_SHORT% <strong>chmod + short.sh </strong> %UCL_PROMPT_SHORT% <strong>./short.sh </strong> Submitting the job on the OSG \u00b6 The job description file tutorial01.submit executes the shell script short.sh as a vanilla universe job. Take a look at the job description file. %UCL_PROMPT_SHORT% <strong>cat tutorial01.submit </strong> Now run this job on the OSG. %UCL_PROMPT_SHORT% <strong>condor_submit tutorial01.submit </strong> Once your job has finished, you can look at the files that HTCondor has returned to the working directory. If everything was successful, it should have returned: job.output (An output file for each job's output) job.error: (An error file for each job's errors) job.log: (A log file for each job's log) Read the output file. %UCL_PROMPT_SHORT% <strong>cat job.output </strong> Observe the difference between the outputs from running the job on the OSG and running locally. (Hint: Check the username, id, work directory etc.)","title":"1.2. Quickstart"},{"location":"materials/day4/part1-ex2-connect-quickstart/#setup","text":"SSH into user-training.osgconnect.net (the OSG Connect submit server for this workshop).","title":"Setup"},{"location":"materials/day4/part1-ex2-connect-quickstart/#get-the-example-files-for-the-tutorial-osg-connect-quickstart-via-tutorial-command","text":"We will get the example files using the tutorial command. %UCL_PROMPT_SHORT% <strong>tutorial quickstart</strong> This creates a directory tutorial-quickstart . Go inside the directory and see what is inside. %UCL_PROMPT_SHORT% <strong>cd tutorial-quickstart</strong> %UCL_PROMPT_SHORT% <strong>ls -F </strong> You will see the following contents: Images/ osg-template-job.submit short.sh tutorial02.submit log/ README.md tutorial01.submit tutorial03.submit We will focus our attention on short.sh (execution file) and tutorial01.submit . We will not worry about other files in this exercise. Feel free to take a look at other files if you are interested.","title":"Get the example files for the tutorial \"OSG Connect Quickstart\" via tutorial command"},{"location":"materials/day4/part1-ex2-connect-quickstart/#job-execution-file","text":"Take a look at the job execution file short.sh . %UCL_PROMPT_SHORT% <strong>cat short.sh </strong> This is a shell script, quite ordinary . Run this shell script locally to see what it does. %UCL_PROMPT_SHORT% <strong>chmod + short.sh </strong> %UCL_PROMPT_SHORT% <strong>./short.sh </strong>","title":"Job Execution File"},{"location":"materials/day4/part1-ex2-connect-quickstart/#submitting-the-job-on-the-osg","text":"The job description file tutorial01.submit executes the shell script short.sh as a vanilla universe job. Take a look at the job description file. %UCL_PROMPT_SHORT% <strong>cat tutorial01.submit </strong> Now run this job on the OSG. %UCL_PROMPT_SHORT% <strong>condor_submit tutorial01.submit </strong> Once your job has finished, you can look at the files that HTCondor has returned to the working directory. If everything was successful, it should have returned: job.output (An output file for each job's output) job.error: (An error file for each job's errors) job.log: (A log file for each job's log) Read the output file. %UCL_PROMPT_SHORT% <strong>cat job.output </strong> Observe the difference between the outputs from running the job on the OSG and running locally. (Hint: Check the username, id, work directory etc.)","title":"Submitting the job on the OSG"},{"location":"materials/day4/part1-ex3-connect-modules/","text":"Thursday Exercise 1.3: Try an OSG Connect Software Module \u00b6 In this exercise, you'll use a software package called GROMACS that was already installed on OASIS. GROMACS is a molecular dynamics simulation program. Setup \u00b6 Make sure you are logged into user-training.osgconnect.net (the OSG Connect submit server for this workshop). Practice Loading the GROMACS Module on OSG Connect submit host \u00b6 Before proceeding, make sure to read over the OSG Connect help desk guide on Accessing Software using Distributed Environment Modules . Run the module avail command to see available modules and determine which gromacs version will be the default (designated by a (D) ). Before loading the gromacs module, check your \"PATH\" variable with: %UCL_PROMPT_SHORT% <strong>echo $PATH</strong> Now load the gromacs module, which will give you the default version, and check your list of modules: %UCL_PROMPT_SHORT% <strong>module load gromacs</strong> %UCL_PROMPT_SHORT% <strong>module list</strong> Recheck your \"PATH\" variable. What has changed? Is the default version of gromacs visible? When you're done, unload the module and check that you no longer have the module loaded: %UCL_PROMPT_SHORT% <strong>module unload gromacs</strong> %UCL_PROMPT_SHORT% <strong>module list</strong> %UCL_PROMPT_SHORT% <strong>echo $PATH</strong> Submit a job that uses GROMACS from OASIS \u00b6 Now let us see how to do a job submission on the OSG that uses GROMACS on OASIS (you don't have to transfer GROMACS source or the binary along with the job). We will get the example files using the tutorial command. %UCL_PROMPT_SHORT% <strong>tutorial gromacs</strong> This creates a directory tutorial-quickstart . Go inside the directory and see what is inside. %UCL_PROMPT_SHORT% <strong>cd tutorial-gromacs</strong> %UCL_PROMPT_SHORT% <strong>ls -F </strong> You will see the following files: 1cta_nvt.tpr Figs/ gromacs_job.sh gromacs_job.submit README.md Take a look at the job description file %UCL_PROMPT_SHORT% <strong>cat gromacs_job.submit </strong> The modules are loaded in the wrapper script before executing the actual job. In this example, gromacs is loaded before running the actual simulation. %UCL_PROMPT_SHORT% <strong>cat gromacs_job.sh</strong> The following requirement is added to find a machine that has the modules: requirements = HAS_MODULES == True Check the job description file. %UCL_PROMPT_SHORT% <strong>cat gromacs_job.submit </strong> Let's submit the job. %UCL_PROMPT_SHORT% <strong>condor_submit gromacs_job.submit </strong> After the simulation is completed, you will see the output files (including gro, cpt and trr files) from GROMACS in your work directory.","title":"1.3. Software modules"},{"location":"materials/day4/part1-ex3-connect-modules/#thursday-exercise-13-try-an-osg-connect-software-module","text":"In this exercise, you'll use a software package called GROMACS that was already installed on OASIS. GROMACS is a molecular dynamics simulation program.","title":"Thursday Exercise 1.3: Try an OSG Connect Software Module"},{"location":"materials/day4/part1-ex3-connect-modules/#setup","text":"Make sure you are logged into user-training.osgconnect.net (the OSG Connect submit server for this workshop).","title":"Setup"},{"location":"materials/day4/part1-ex3-connect-modules/#practice-loading-the-gromacs-module-on-osg-connect-submit-host","text":"Before proceeding, make sure to read over the OSG Connect help desk guide on Accessing Software using Distributed Environment Modules . Run the module avail command to see available modules and determine which gromacs version will be the default (designated by a (D) ). Before loading the gromacs module, check your \"PATH\" variable with: %UCL_PROMPT_SHORT% <strong>echo $PATH</strong> Now load the gromacs module, which will give you the default version, and check your list of modules: %UCL_PROMPT_SHORT% <strong>module load gromacs</strong> %UCL_PROMPT_SHORT% <strong>module list</strong> Recheck your \"PATH\" variable. What has changed? Is the default version of gromacs visible? When you're done, unload the module and check that you no longer have the module loaded: %UCL_PROMPT_SHORT% <strong>module unload gromacs</strong> %UCL_PROMPT_SHORT% <strong>module list</strong> %UCL_PROMPT_SHORT% <strong>echo $PATH</strong>","title":"Practice Loading the GROMACS Module on OSG Connect submit host"},{"location":"materials/day4/part1-ex3-connect-modules/#submit-a-job-that-uses-gromacs-from-oasis","text":"Now let us see how to do a job submission on the OSG that uses GROMACS on OASIS (you don't have to transfer GROMACS source or the binary along with the job). We will get the example files using the tutorial command. %UCL_PROMPT_SHORT% <strong>tutorial gromacs</strong> This creates a directory tutorial-quickstart . Go inside the directory and see what is inside. %UCL_PROMPT_SHORT% <strong>cd tutorial-gromacs</strong> %UCL_PROMPT_SHORT% <strong>ls -F </strong> You will see the following files: 1cta_nvt.tpr Figs/ gromacs_job.sh gromacs_job.submit README.md Take a look at the job description file %UCL_PROMPT_SHORT% <strong>cat gromacs_job.submit </strong> The modules are loaded in the wrapper script before executing the actual job. In this example, gromacs is loaded before running the actual simulation. %UCL_PROMPT_SHORT% <strong>cat gromacs_job.sh</strong> The following requirement is added to find a machine that has the modules: requirements = HAS_MODULES == True Check the job description file. %UCL_PROMPT_SHORT% <strong>cat gromacs_job.submit </strong> Let's submit the job. %UCL_PROMPT_SHORT% <strong>condor_submit gromacs_job.submit </strong> After the simulation is completed, you will see the output files (including gro, cpt and trr files) from GROMACS in your work directory.","title":"Submit a job that uses GROMACS from OASIS"},{"location":"materials/day4/part1-ex4-singularity/","text":"Thursday Exercise 1.4: Try Singularity on the OSG (Optional) \u00b6 In this tutorial, we see how to submit a [tensorflow]( https://www.tensorflow.org/ ) job on the OSG through [Singularity containers]( https://support.opensciencegrid.org/solution/articles/12000024676-singularity-containers ). We currently offer CPU and GPU containers for tensorflow (both based on Ubuntu). Here, we focus on CPU container. Setup \u00b6 Make sure you are logged into user-training.osgconnect.net (the OSG Connect submit server for this workshop). Get the example files and understand the job requirements. \u00b6 Let us utilize the tutorial command. In the command prompt, type %UCL_PROMPT_SHORT% <strong>tutorial tf-matmul </strong> This creates a directory tutorial-tf-matmul . Go inside the directory and see what is inside. %UCL_PROMPT_SHORT% <strong>cd tutorial-tf-matmul</strong> %UCL_PROMPT_SHORT% <strong>ls -F</strong> You will see the following files tf_matmul.py (Python program to multiply two matrices using tensorflow package) tf_matmul.submit (HTCondor Job description file) tf_matmul_wrapper.sh (Job wrapper shell script that executes the python program) tf_matmul_gpu.submit (HTCondor Job description file targeting gpus) NOTE: The file tf_matmul_gpu.submit is for gpus, but we will not focus on gpus in this exercise. You are welcome to take a look. The python script `tf_matmul.py` uses tensorflow to perform the matrix multiplication of a `2x2` matrix. Indeed, this is not the best use case of tensorflow. For our purpose of showing how to submit the tensorflow job on the OSG this example is just fine. We want to run the program on a remote worker machine on the OSG that supports the singularity container. So we set the requirement in our HTCondor description. Requirements = HAS_SINGULARITY == True In addition, we also provide the full path of the image via the keyword +SingularityImage . +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/tensorflow/tensorflow:latest\" The image is distributed to the remote worker machines through `cvmfs`. Although there are multiple ways to obtain the image file for a job on the OSG machine, the image distributed through `cvmfs` is preferred. Submit the tensorflow example job \u00b6 Submit the job. %UCL_PROMPT_SHORT% <strong>condor_submit tf_matmul.submit </strong> The job will look for a machine on the OSG that has singularity installed, creates the singularity container with the image /cvmfs/singularity.opensciencegrid.org/tensorflow/tensorflow:latest and executes the program tf_matmul.py . After the job is completed, you will see the output file tf_matmul.output .","title":"1.4. (opt) Singularity"},{"location":"materials/day4/part1-ex4-singularity/#thursday-exercise-14-try-singularity-on-the-osg-optional","text":"In this tutorial, we see how to submit a [tensorflow]( https://www.tensorflow.org/ ) job on the OSG through [Singularity containers]( https://support.opensciencegrid.org/solution/articles/12000024676-singularity-containers ). We currently offer CPU and GPU containers for tensorflow (both based on Ubuntu). Here, we focus on CPU container.","title":"Thursday Exercise 1.4: Try Singularity on the OSG (Optional)"},{"location":"materials/day4/part1-ex4-singularity/#setup","text":"Make sure you are logged into user-training.osgconnect.net (the OSG Connect submit server for this workshop).","title":"Setup"},{"location":"materials/day4/part1-ex4-singularity/#get-the-example-files-and-understand-the-job-requirements","text":"Let us utilize the tutorial command. In the command prompt, type %UCL_PROMPT_SHORT% <strong>tutorial tf-matmul </strong> This creates a directory tutorial-tf-matmul . Go inside the directory and see what is inside. %UCL_PROMPT_SHORT% <strong>cd tutorial-tf-matmul</strong> %UCL_PROMPT_SHORT% <strong>ls -F</strong> You will see the following files tf_matmul.py (Python program to multiply two matrices using tensorflow package) tf_matmul.submit (HTCondor Job description file) tf_matmul_wrapper.sh (Job wrapper shell script that executes the python program) tf_matmul_gpu.submit (HTCondor Job description file targeting gpus) NOTE: The file tf_matmul_gpu.submit is for gpus, but we will not focus on gpus in this exercise. You are welcome to take a look. The python script `tf_matmul.py` uses tensorflow to perform the matrix multiplication of a `2x2` matrix. Indeed, this is not the best use case of tensorflow. For our purpose of showing how to submit the tensorflow job on the OSG this example is just fine. We want to run the program on a remote worker machine on the OSG that supports the singularity container. So we set the requirement in our HTCondor description. Requirements = HAS_SINGULARITY == True In addition, we also provide the full path of the image via the keyword +SingularityImage . +SingularityImage = \"/cvmfs/singularity.opensciencegrid.org/tensorflow/tensorflow:latest\" The image is distributed to the remote worker machines through `cvmfs`. Although there are multiple ways to obtain the image file for a job on the OSG machine, the image distributed through `cvmfs` is preferred.","title":"Get the example files and understand the job requirements."},{"location":"materials/day4/part1-ex4-singularity/#submit-the-tensorflow-example-job","text":"Submit the job. %UCL_PROMPT_SHORT% <strong>condor_submit tf_matmul.submit </strong> The job will look for a machine on the OSG that has singularity installed, creates the singularity container with the image /cvmfs/singularity.opensciencegrid.org/tensorflow/tensorflow:latest and executes the program tf_matmul.py . After the job is completed, you will see the output file tf_matmul.output .","title":"Submit the tensorflow example job"},{"location":"materials/day4/part2-ex1-data-needs/","text":"Understanding Data Requirements \u00b6 Background \u00b6 This exercise's goal is to learn to think critically about an application's data needs, especially before submitting a large batch of jobs or using tools for delivering large data to jobs. In this exercise we will attempt to understand the input and output of the bioinformatics application BLAST , which you used yesterday in Exercise 1.2. Setup \u00b6 Make sure you are logged into user-training.osgconnect.net . Navigate to your stash directory within the home directory, and create a directory for this exercise named thur-blast. Copy the blast executables: \u00b6 %UCL_PROMPT_SHORT% <strong>wget http://proxy.chtc.wisc.edu/SQUID/osgschool17/ncbi-blast-2.6.0+-x64-linux.tar.gz</strong> %UCL_PROMPT_SHORT% <strong>tar -xzf ncbi-blast-2.6.0+-x64-linux.tar.gz</strong> Copy the Input Files \u00b6 To run BLAST, we need an input file and reference database. For this example, we'll use the \"pdbaa\" database, which contains sequences for the protein structure from the Protein Data Bank. For our input file, we'll use an abbreviated fasta file with mouse genome information. Download these files to your current directory: \\ %UCL_PROMPT_SHORT% <strong>wget http://proxy.chtc.wisc.edu/SQUID/osgschool17/pdbaa.tar.gz</strong> %UCL_PROMPT_SHORT% <strong>wget http://proxy.chtc.wisc.edu/SQUID/osgschool17/mouse.fa</strong> Untar the pdbaa database: \\ %UCL_PROMPT_SHORT% <strong>tar -xzf pdbaa.tar.gz</strong> Understanding blast \u00b6 Remember that blastx is executed in a command like the following: %UCL_PROMPT_SHORT% <strong>blastx -db %RED%database_rootname%ENDCOLOR% -query %RED%input_file%ENDCOLOR% -out %RED%results_file%ENDCOLOR%</strong> In the above, the input_file is a file containing a number of genetic sequences, and the database that these are compared against is made up of several files that begin with the same root name (we previously used the \"pdbaa\" database, whose files all begin with pdbaa ). The output from this analysis will be printed to a results file that is also indicated in the command. Adding up data needs \u00b6 Looking at the files from the blastx jobs you ran yesterday, add up the amount of data that was needed for running the job. (If you deleted any of the files from yesterday, just resubmit the job before proceeding.) Here are the commands that will be most useful to you: How to see the size of a specific file: user@host $ ls -lh %RED%file How to see the size of all files in the current directory: user@host $ ls -lh How to determine the total amount of data in the current directory: user@host $ du -sh %RED%directory Input requirements \u00b6 Looking at yesterday's exercise, total up the amount of data in all of the files necessary to run the blastx job (which will include the executable, itself). Write down this number. Also take note of how much total data in in the pdbaa directory. Output requirements \u00b6 The output that we care about from blastx is saved in the file whose name is indicated after the -out argument to blastx . However, remember that HTCondor also creates the error, output, and log files, which you'll need to add up, too. Are there any other files? Total all of these together, as well. Talk about this as a group! \u00b6 Once you have completed the above tasks, we'll talk about the totals as a group. How much disk space is required on the submit server for one blastx run with the input file you used before? How many files are needed and created for each run? Assuming that each file is read completely by BLAST, and since you know how long blastx runs (time it): At what rate are files read in? How many MB/s? How much total disk space would be necessary on the submit server to run 10 jobs? (remember that some of the files will be shared by all 10 jobs, and will not be multiplied) Up next! \u00b6 Next you will create a HTCondor submit script to transfer the Blast input files in order to run Blast on a worker nodes. Next Exercise","title":"2.1. Data requirements"},{"location":"materials/day4/part2-ex1-data-needs/#understanding-data-requirements","text":"","title":"Understanding Data Requirements"},{"location":"materials/day4/part2-ex1-data-needs/#background","text":"This exercise's goal is to learn to think critically about an application's data needs, especially before submitting a large batch of jobs or using tools for delivering large data to jobs. In this exercise we will attempt to understand the input and output of the bioinformatics application BLAST , which you used yesterday in Exercise 1.2.","title":"Background"},{"location":"materials/day4/part2-ex1-data-needs/#setup","text":"Make sure you are logged into user-training.osgconnect.net . Navigate to your stash directory within the home directory, and create a directory for this exercise named thur-blast.","title":"Setup"},{"location":"materials/day4/part2-ex1-data-needs/#copy-the-blast-executables","text":"%UCL_PROMPT_SHORT% <strong>wget http://proxy.chtc.wisc.edu/SQUID/osgschool17/ncbi-blast-2.6.0+-x64-linux.tar.gz</strong> %UCL_PROMPT_SHORT% <strong>tar -xzf ncbi-blast-2.6.0+-x64-linux.tar.gz</strong>","title":"Copy the blast executables:"},{"location":"materials/day4/part2-ex1-data-needs/#copy-the-input-files","text":"To run BLAST, we need an input file and reference database. For this example, we'll use the \"pdbaa\" database, which contains sequences for the protein structure from the Protein Data Bank. For our input file, we'll use an abbreviated fasta file with mouse genome information. Download these files to your current directory: \\ %UCL_PROMPT_SHORT% <strong>wget http://proxy.chtc.wisc.edu/SQUID/osgschool17/pdbaa.tar.gz</strong> %UCL_PROMPT_SHORT% <strong>wget http://proxy.chtc.wisc.edu/SQUID/osgschool17/mouse.fa</strong> Untar the pdbaa database: \\ %UCL_PROMPT_SHORT% <strong>tar -xzf pdbaa.tar.gz</strong>","title":"Copy the Input Files"},{"location":"materials/day4/part2-ex1-data-needs/#understanding-blast","text":"Remember that blastx is executed in a command like the following: %UCL_PROMPT_SHORT% <strong>blastx -db %RED%database_rootname%ENDCOLOR% -query %RED%input_file%ENDCOLOR% -out %RED%results_file%ENDCOLOR%</strong> In the above, the input_file is a file containing a number of genetic sequences, and the database that these are compared against is made up of several files that begin with the same root name (we previously used the \"pdbaa\" database, whose files all begin with pdbaa ). The output from this analysis will be printed to a results file that is also indicated in the command.","title":"Understanding blast"},{"location":"materials/day4/part2-ex1-data-needs/#adding-up-data-needs","text":"Looking at the files from the blastx jobs you ran yesterday, add up the amount of data that was needed for running the job. (If you deleted any of the files from yesterday, just resubmit the job before proceeding.) Here are the commands that will be most useful to you: How to see the size of a specific file: user@host $ ls -lh %RED%file How to see the size of all files in the current directory: user@host $ ls -lh How to determine the total amount of data in the current directory: user@host $ du -sh %RED%directory","title":"Adding up data needs"},{"location":"materials/day4/part2-ex1-data-needs/#input-requirements","text":"Looking at yesterday's exercise, total up the amount of data in all of the files necessary to run the blastx job (which will include the executable, itself). Write down this number. Also take note of how much total data in in the pdbaa directory.","title":"Input requirements"},{"location":"materials/day4/part2-ex1-data-needs/#output-requirements","text":"The output that we care about from blastx is saved in the file whose name is indicated after the -out argument to blastx . However, remember that HTCondor also creates the error, output, and log files, which you'll need to add up, too. Are there any other files? Total all of these together, as well.","title":"Output requirements"},{"location":"materials/day4/part2-ex1-data-needs/#talk-about-this-as-a-group","text":"Once you have completed the above tasks, we'll talk about the totals as a group. How much disk space is required on the submit server for one blastx run with the input file you used before? How many files are needed and created for each run? Assuming that each file is read completely by BLAST, and since you know how long blastx runs (time it): At what rate are files read in? How many MB/s? How much total disk space would be necessary on the submit server to run 10 jobs? (remember that some of the files will be shared by all 10 jobs, and will not be multiplied)","title":"Talk about this as a group!"},{"location":"materials/day4/part2-ex1-data-needs/#up-next","text":"Next you will create a HTCondor submit script to transfer the Blast input files in order to run Blast on a worker nodes. Next Exercise","title":"Up next!"},{"location":"materials/day4/part2-ex2-file-transfer/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } File Compression and Testing Resource Requirements \u00b6 The objective of this exercise is to refresh yourself on HTCondor file transfer, to implement file compression, and to begin examining the memory and disk space used by your jobs in order to plan larger batches, which we'll tackle in later exercises today. Setup \u00b6 Make sure you are still logged into user-training.osgconnect.net Make a directory for today's blast exercises named thur-blast-data , and change into it. The executable we'll use in this exercise and later today is the same blastx executable from the previous exercise. Also copy the data from the last exercise into the thur-blast-data directory. You'll need the mouse.fa file and the pdbaa directory from the last exercise, but you'll end up making a new submit file. Review: HTCondor File Transfer \u00b6 Recall that OSG does not have a shared filesystem! Instead, HTCondor transfers your executable and input files (listed with transfer_input_files ) to a working directory on the execute node, regardless of how these files were arranged on the submit node. In this exercise we'll use the same blastx example job that we used previously, but modify the submit file and test how much memory and disk space it uses on the execute node. Start with a test submit file \u00b6 We've started a submit file for you, below, which you'll add to in the remaining steps. executable = transfer_input_files = output = test.out error = test.error log = test.log request_memory = request_disk = request_cpus = 1 requirements = (OpSys == \"LINUX\") queue Implement file compression \u00b6 In our first blast job from yesterday, the database files in the pdbaa directory were all transferred, as is, but we could instead transfer them as a single, compressed file using tar . For a second test job, let's compress our blast database files to send them to the submit node as a single tar.gz file, by following the below steps Change into the pdbaa directory and compress the database files into a single file called pdbaa_files.tar.gz using the tar command. (NOTE: This file will be different from the pdbaa.tar.gz files you downloaded yesterday, because it will only contain the pdbaa files, and not the pdbaa directory, itself.) A typical command for creating a tar file is: %UCL_PROMPT_SHORT% <strong>tar -cvzf [compressed filename] [list of files]</strong> Move this file to the thur-blast-data directory. Create a wrapper script that will first decompress the pdbaa_files.tar.gz file, and then run blast. Because this file will now be our submit file executable , we'll also end up transferring the blastx executable with transfer_input_files . In the thur-blast-data directory, create a new file, called blast_wrapper.sh , with the following contents: #!/bin/bash tar xvzf pdbaa_files.tar.gz ./blastx -db pdbaa -query mouse.fa -out mouse.fa.result rm pdbaa.* IMPORTANT: The last line removes the resulting database files that came from pdbaa_files.tar.gz , as these files would otherwise be copied back to the submit server as perceived output (because they're new files that HTCondor didn't transfer over as input). List the executable and input files \u00b6 Make sure to update the submit file with the following: Add the new executable (the wrapper script you created above) List the blastx binary, the pdbaa_files.tar.gz file, and the input query file in transfer_input_files HINT: Remember that transfer_input_files accepts a comma separated list of files, and that you need to list the full location of the blastx executable ( blastx ). There will be no arguments, since the arguments to the blastx command are now captured in the wrapper script. Predict memory and disk requests from your data \u00b6 Also, think about how much memory and disk to request for this job. It's good to start with values that are a little higher than you think a test job will need, but think about: how much memory blastx would use if it loaded all of the database files and the query input file into memory. how much disk space will be necessary on the execute server for the executable, all input files, and all output files (hint: the log file only exists on the submit node). whether you'd like to request some extra memory or disk space, just in case Look at the log file for your blastx job from yesterday, and compare the memory and disk \"Usage\" to what you predicted from the files. Make sure to update the submit file with more accurate memory and disk requests (you may still want to request slightly more than the job actually used). Run the test job \u00b6 Once you have finished editing the submit file, go ahead and submit the job. It should take a few minutes to complete, and then you can check to make sure that no unwanted files (especially the pdbaa database files) were copied back at the end of the job. Run a du -sh on the directory with this job's input. How does it compare to the directory from yesterday, and why? When you've completed the above, continue with the next exercise .","title":"2.2. HTCondor transfers"},{"location":"materials/day4/part2-ex2-file-transfer/#file-compression-and-testing-resource-requirements","text":"The objective of this exercise is to refresh yourself on HTCondor file transfer, to implement file compression, and to begin examining the memory and disk space used by your jobs in order to plan larger batches, which we'll tackle in later exercises today.","title":"File Compression and Testing Resource Requirements"},{"location":"materials/day4/part2-ex2-file-transfer/#setup","text":"Make sure you are still logged into user-training.osgconnect.net Make a directory for today's blast exercises named thur-blast-data , and change into it. The executable we'll use in this exercise and later today is the same blastx executable from the previous exercise. Also copy the data from the last exercise into the thur-blast-data directory. You'll need the mouse.fa file and the pdbaa directory from the last exercise, but you'll end up making a new submit file.","title":"Setup"},{"location":"materials/day4/part2-ex2-file-transfer/#review-htcondor-file-transfer","text":"Recall that OSG does not have a shared filesystem! Instead, HTCondor transfers your executable and input files (listed with transfer_input_files ) to a working directory on the execute node, regardless of how these files were arranged on the submit node. In this exercise we'll use the same blastx example job that we used previously, but modify the submit file and test how much memory and disk space it uses on the execute node.","title":"Review: HTCondor File Transfer"},{"location":"materials/day4/part2-ex2-file-transfer/#start-with-a-test-submit-file","text":"We've started a submit file for you, below, which you'll add to in the remaining steps. executable = transfer_input_files = output = test.out error = test.error log = test.log request_memory = request_disk = request_cpus = 1 requirements = (OpSys == \"LINUX\") queue","title":"Start with a test submit file"},{"location":"materials/day4/part2-ex2-file-transfer/#implement-file-compression","text":"In our first blast job from yesterday, the database files in the pdbaa directory were all transferred, as is, but we could instead transfer them as a single, compressed file using tar . For a second test job, let's compress our blast database files to send them to the submit node as a single tar.gz file, by following the below steps Change into the pdbaa directory and compress the database files into a single file called pdbaa_files.tar.gz using the tar command. (NOTE: This file will be different from the pdbaa.tar.gz files you downloaded yesterday, because it will only contain the pdbaa files, and not the pdbaa directory, itself.) A typical command for creating a tar file is: %UCL_PROMPT_SHORT% <strong>tar -cvzf [compressed filename] [list of files]</strong> Move this file to the thur-blast-data directory. Create a wrapper script that will first decompress the pdbaa_files.tar.gz file, and then run blast. Because this file will now be our submit file executable , we'll also end up transferring the blastx executable with transfer_input_files . In the thur-blast-data directory, create a new file, called blast_wrapper.sh , with the following contents: #!/bin/bash tar xvzf pdbaa_files.tar.gz ./blastx -db pdbaa -query mouse.fa -out mouse.fa.result rm pdbaa.* IMPORTANT: The last line removes the resulting database files that came from pdbaa_files.tar.gz , as these files would otherwise be copied back to the submit server as perceived output (because they're new files that HTCondor didn't transfer over as input).","title":"Implement file compression"},{"location":"materials/day4/part2-ex2-file-transfer/#list-the-executable-and-input-files","text":"Make sure to update the submit file with the following: Add the new executable (the wrapper script you created above) List the blastx binary, the pdbaa_files.tar.gz file, and the input query file in transfer_input_files HINT: Remember that transfer_input_files accepts a comma separated list of files, and that you need to list the full location of the blastx executable ( blastx ). There will be no arguments, since the arguments to the blastx command are now captured in the wrapper script.","title":"List the executable and input files"},{"location":"materials/day4/part2-ex2-file-transfer/#predict-memory-and-disk-requests-from-your-data","text":"Also, think about how much memory and disk to request for this job. It's good to start with values that are a little higher than you think a test job will need, but think about: how much memory blastx would use if it loaded all of the database files and the query input file into memory. how much disk space will be necessary on the execute server for the executable, all input files, and all output files (hint: the log file only exists on the submit node). whether you'd like to request some extra memory or disk space, just in case Look at the log file for your blastx job from yesterday, and compare the memory and disk \"Usage\" to what you predicted from the files. Make sure to update the submit file with more accurate memory and disk requests (you may still want to request slightly more than the job actually used).","title":"Predict memory and disk requests from your data"},{"location":"materials/day4/part2-ex2-file-transfer/#run-the-test-job","text":"Once you have finished editing the submit file, go ahead and submit the job. It should take a few minutes to complete, and then you can check to make sure that no unwanted files (especially the pdbaa database files) were copied back at the end of the job. Run a du -sh on the directory with this job's input. How does it compare to the directory from yesterday, and why? When you've completed the above, continue with the next exercise .","title":"Run the test job"},{"location":"materials/day4/part2-ex3-blast-split/","text":"Thursday Exercise 2.3: Splitting Large Input for Better Throughput \u00b6 The objective of this exercise is to prepare for blasting a much larger input query file by splitting the input for greater throughput and lower memory and disk requirements. Splitting the input will also mean that we don't have to rely on additional large-data measures for the input query files. Setup \u00b6 Make sure you are still logged into user-training.osgconnect.net Make sure you are in the directory named thur-blast-data under the stash filesystem. Obtain the large input \u00b6 We've previously used blastx to analyze a relatively small input file of test data, mouse.fa , but let's imagine that you now need to blast a much larger dataset for your research. This dataset can be downloaded with the following command: %UCL_PROMPT_SHORT% <strong>wget http://proxy.chtc.wisc.edu/SQUID/osgschool17/mouse_rna.tar.gz</strong> After un-tar'ing the file, you should be able to confirm that it's size is roughly 100 MB. Not only is this a bit large for file transfer, but it would take hours to complete a single blastx analysis for it. Also, the single output file would be huge. Compare for yourself to the time and output file size for the mouse.fa input file, according to your test job in the last exercise. Split the input file \u00b6 For blast , it's scientifically valid to split up the input query file, analyze the pieces, and then put the results back together at the end! (Importantly, blast databases should not be split, because the blast output includes a score value for each sequence that is calculated relative to the entire length of the database.) Because genetic sequence data is used heavily across the life science, there are also tools for splitting up the data into smaller files. One of these is called genome tools , and you can download a package of precompiled binaries (just like blast) using the following command: %UCL_PROMPT_SHORT% <strong>wget http://genometools.org/pub/binary_distributions/gt-1.5.9-Linux_x86_64-64bit-complete.tar.gz</strong> Un-tar the gt package ( tar -xzvf ... ), then run it's sequence file splitter as follows, with the target file size of 1 MB: %UCL_PROMPT_SHORT% <strong>./gt-1.5.9-Linux_x86_64-64bit-complete/bin/gt splitfasta -targetsize 1 mouse_rna.fa</strong> You'll notice that the result is a set of 100 files, all about the size of 1 MB, and numbered 1 through 100. Test a split job \u00b6 Now, you'll run a test job to prepare for submitting many jobs, later, where each will use a different input file. Modify the submit file \u00b6 First, you'll create a new submit file that passes the input filename as an argument and use a list of applicable filenames. Follow the below steps: Copy the submit file to a new file called blast_split.sub and modify the \"queue\" line of the submit file to the following: queue inputfile matching mouse_rna.fa.1 (If this queue line looks like we should be scanning all of the rna files, wait until the next exercise) Replace the mouse.fa instances in the submit file with $(inputfile) , and rename the output, log, and error files to use the same inputfile variable: output = $(inputfile).out error = $(inputfile).err log = $(inputfile).log Add an arguments line to the submit file so it will pass the name of the input file to the wrapper script arguments = $(inputfile) Update the memory and disk requests, since the new input file is larger and will also produce larger output. It may be best to overestimate to something like 1 GB for each. (After completing this test, you'll be able to update them to a more accurate value.) Modify the wrapper file \u00b6 Replace instances of the input file name in the blast_wrapper.sh script so that it will insert the first argument in place of the input filename, like so: ./blastx -db pdbaa -query $1 -out $1.result NOTE: bash shell scripts will use the first argument in place of $1 , the second argument as $2 , etc. Submit the test job \u00b6 This job will take a bit longer than the job in the last exercise, since the input file is larger (by about 3-fold). Again, make sure that only the desired output , error , and result files come back at the end of the job. Update the resource requests \u00b6 After the job finishes successfully, examine the log file for memory and disk usage, and update the requests in the submit file. In Exercise 3.1 (after the next lecture) you'll submit many jobs at once and use a different method for handling the pdbaa_files.tar.gz file, which is a bit too large to use regular file transfer when submitting many jobs.","title":"2.3. Splitting big data"},{"location":"materials/day4/part2-ex3-blast-split/#thursday-exercise-23-splitting-large-input-for-better-throughput","text":"The objective of this exercise is to prepare for blasting a much larger input query file by splitting the input for greater throughput and lower memory and disk requirements. Splitting the input will also mean that we don't have to rely on additional large-data measures for the input query files.","title":"Thursday Exercise 2.3: Splitting Large Input for Better Throughput"},{"location":"materials/day4/part2-ex3-blast-split/#setup","text":"Make sure you are still logged into user-training.osgconnect.net Make sure you are in the directory named thur-blast-data under the stash filesystem.","title":"Setup"},{"location":"materials/day4/part2-ex3-blast-split/#obtain-the-large-input","text":"We've previously used blastx to analyze a relatively small input file of test data, mouse.fa , but let's imagine that you now need to blast a much larger dataset for your research. This dataset can be downloaded with the following command: %UCL_PROMPT_SHORT% <strong>wget http://proxy.chtc.wisc.edu/SQUID/osgschool17/mouse_rna.tar.gz</strong> After un-tar'ing the file, you should be able to confirm that it's size is roughly 100 MB. Not only is this a bit large for file transfer, but it would take hours to complete a single blastx analysis for it. Also, the single output file would be huge. Compare for yourself to the time and output file size for the mouse.fa input file, according to your test job in the last exercise.","title":"Obtain the large input"},{"location":"materials/day4/part2-ex3-blast-split/#split-the-input-file","text":"For blast , it's scientifically valid to split up the input query file, analyze the pieces, and then put the results back together at the end! (Importantly, blast databases should not be split, because the blast output includes a score value for each sequence that is calculated relative to the entire length of the database.) Because genetic sequence data is used heavily across the life science, there are also tools for splitting up the data into smaller files. One of these is called genome tools , and you can download a package of precompiled binaries (just like blast) using the following command: %UCL_PROMPT_SHORT% <strong>wget http://genometools.org/pub/binary_distributions/gt-1.5.9-Linux_x86_64-64bit-complete.tar.gz</strong> Un-tar the gt package ( tar -xzvf ... ), then run it's sequence file splitter as follows, with the target file size of 1 MB: %UCL_PROMPT_SHORT% <strong>./gt-1.5.9-Linux_x86_64-64bit-complete/bin/gt splitfasta -targetsize 1 mouse_rna.fa</strong> You'll notice that the result is a set of 100 files, all about the size of 1 MB, and numbered 1 through 100.","title":"Split the input file"},{"location":"materials/day4/part2-ex3-blast-split/#test-a-split-job","text":"Now, you'll run a test job to prepare for submitting many jobs, later, where each will use a different input file.","title":"Test a split job"},{"location":"materials/day4/part2-ex3-blast-split/#modify-the-submit-file","text":"First, you'll create a new submit file that passes the input filename as an argument and use a list of applicable filenames. Follow the below steps: Copy the submit file to a new file called blast_split.sub and modify the \"queue\" line of the submit file to the following: queue inputfile matching mouse_rna.fa.1 (If this queue line looks like we should be scanning all of the rna files, wait until the next exercise) Replace the mouse.fa instances in the submit file with $(inputfile) , and rename the output, log, and error files to use the same inputfile variable: output = $(inputfile).out error = $(inputfile).err log = $(inputfile).log Add an arguments line to the submit file so it will pass the name of the input file to the wrapper script arguments = $(inputfile) Update the memory and disk requests, since the new input file is larger and will also produce larger output. It may be best to overestimate to something like 1 GB for each. (After completing this test, you'll be able to update them to a more accurate value.)","title":"Modify the submit file"},{"location":"materials/day4/part2-ex3-blast-split/#modify-the-wrapper-file","text":"Replace instances of the input file name in the blast_wrapper.sh script so that it will insert the first argument in place of the input filename, like so: ./blastx -db pdbaa -query $1 -out $1.result NOTE: bash shell scripts will use the first argument in place of $1 , the second argument as $2 , etc.","title":"Modify the wrapper file"},{"location":"materials/day4/part2-ex3-blast-split/#submit-the-test-job","text":"This job will take a bit longer than the job in the last exercise, since the input file is larger (by about 3-fold). Again, make sure that only the desired output , error , and result files come back at the end of the job.","title":"Submit the test job"},{"location":"materials/day4/part2-ex3-blast-split/#update-the-resource-requests","text":"After the job finishes successfully, examine the log file for memory and disk usage, and update the requests in the submit file. In Exercise 3.1 (after the next lecture) you'll submit many jobs at once and use a different method for handling the pdbaa_files.tar.gz file, which is a bit too large to use regular file transfer when submitting many jobs.","title":"Update the resource requests"},{"location":"materials/day4/part3-ex1-blast-proxy/","text":"Thursday Exercise 3.1: Using a Web Proxy for Large Shared Input \u00b6 Continuing the series of exercises blasting mouse genetic sequences, the objective of this exercise is to use a web proxy to stage the large database, which will be downloaded into each of many jobs that use the split input files from the last exercise (Exercise 2.3). Setup \u00b6 Make sure you are logged into user-training.osgconnect.net Make sure you are in the same directory as the previous exercise, Exercise 2.3 directory named thur-blast-data . Place the Large File on the Proxy \u00b6 First, you'll need to put the pdbaa_files.tar.gz file onto the Stash web directory. Use the following command: %UCL_PROMPT_SHORT% cp pdbaa_files.tar.gz ~/stash/public/ Test a download of the file \u00b6 Once the file is placed in the ~/stash/public directory, it can be downloaded from a corresponding URL such as http://stash.osgconnect.net/~%RED%username%ENDCOLOR%/pdbaa_files.tar.gz , where %RED%username is your username on user-training.osgconnect.net . Using the above convention (and from a different directory on user-training.osgconnect.net , any directory), you can test the download of your pdbaa_files.tar.gz file with a command like the following: %UCL_PROMPT_SHORT% <strong>wget http://stash.osgconnect.net/~%RED%username%ENDCOLOR%/pdbaa_files.tar.gz</strong> You may realize that you've been using wget to download files from a web proxy for many of the previous exercises at the school! Run a New Test Job \u00b6 Now, you'll repeat the last exercise (with a single input query file) but have HTCondor download the pdbaa_files.tar.gz file from the web proxy, instead of having the file transferred from the submit server. Modify the submit file and wrapper script \u00b6 In the wrapper script, we have to add some special lines so that we can pull from Stash and use the HTTP proxy. In blast_wrapper.sh , we will have to add commands to pull the data file: #!/bin/bash %RED% # Set an environment variable to point to the local Proxy location at the cluster export http_proxy=$OSG_SQUID_LOCATION # Copy the pdbaa_files.tar.gz to the worker node # Add the -S argument, so we can see if it was a cache HIT or MISS wget -S http://stash.osgconnect.net/~username/pdbaa_files.tar.gz %ENDCOLOR% tar xvzf pdbaa_files.tar.gz tar xvzf blastx.tar.gz ./blastx -db pdbaa -query $1 -out $1.result rm pdbaa.* rm blastx The first line will set a special environment variable that wget will look at. $OSG_SQUID_LOCATION is a special variable that is set on OSG sites to the hostname of the nearest squid cache. The second line will download the pdbaa_files.tar.gz from stash, using the closes cache (because wget will look at http_proxy for the newest cache=). In your submit file, you will need to remove the pdbaa_files.tar.gz file from the transfer_input_files , because we are using HTTP proxies! Submit the test job \u00b6 You may wish to first remove the log, result, output, and error files from the previous tests, which will be overwritten when the new test job completes. rm *.err *.out *.result *.log Submit the test job! When the job starts, the wrapper will download the pdbaa_files.tar.gz file from the web proxy. If the jobs takes longer than two minutes, you can assume that it will complete successfully, and then continue with the rest of the exercise. After the job completes examine the *.err file generated by the submission. At the top of the file, you will find something like: --2017-07-19 00:36:29-- http://stash.osgconnect.net/~osguser199/pdbaa_files.tar.gz Resolving cmsprxy01.colorado.edu... 192.12.238.152 Connecting to cmsprxy01.colorado.edu|192.12.238.152|:3128... connected. Proxy request sent, awaiting response... HTTP/1.0 200 OK Content-Type: application/octet-stream Content-Length: 22105114 Accept-Ranges: bytes Server: nginx/1.10.2 Date: Wed, 19 Jul 2017 00:36:29 GMT Last-Modified: Wed, 19 Jul 2017 05:03:09 GMT ETag: \"596ee80d-1514c1a\" X-Cache: HIT from cmsprxy01.colorado.edu Via: 1.1 cmsprxy01.colorado.edu:3128 (squid/frontier-squid-2.7.STABLE9-14) Connection: close Length: 22105114 (21M) [application/octet-stream] Saving to: `pdbaa_files.tar.gz' ... Notice the X-Cache line. It says it was a cache HIT from the proxy cmsprxy01.colorado.edu . Yay! You successfully used a proxy to cache data near your worker node! Notice, the name of the cache file may be different. Run all 100 Jobs! \u00b6 If all of the previous tests have gone okay, you can prepare to run all 100 jobs that will use the split input files. To make sure you're not going to generate too much data, use the size of files from the previous test to calculate how much total data you're going to add to the thur-blast-data directory for 100 jobs. In the submit file, you will need to change the queue line to scan all the rna files: ... queue inputfile matching mouse_rna.fa.* Submit all 100 jobs! They may take a while to all complete, but it will still be faster than the many hours it would have taken to blast the single, large mouse_rna.fa file without splitting it up. In the meantime, as long as the first several jobs are running for longer than two minutes, you can move on to the next exercise .","title":"3.1. Web proxies"},{"location":"materials/day4/part3-ex1-blast-proxy/#thursday-exercise-31-using-a-web-proxy-for-large-shared-input","text":"Continuing the series of exercises blasting mouse genetic sequences, the objective of this exercise is to use a web proxy to stage the large database, which will be downloaded into each of many jobs that use the split input files from the last exercise (Exercise 2.3).","title":"Thursday Exercise 3.1: Using a Web Proxy for Large Shared Input"},{"location":"materials/day4/part3-ex1-blast-proxy/#setup","text":"Make sure you are logged into user-training.osgconnect.net Make sure you are in the same directory as the previous exercise, Exercise 2.3 directory named thur-blast-data .","title":"Setup"},{"location":"materials/day4/part3-ex1-blast-proxy/#place-the-large-file-on-the-proxy","text":"First, you'll need to put the pdbaa_files.tar.gz file onto the Stash web directory. Use the following command: %UCL_PROMPT_SHORT% cp pdbaa_files.tar.gz ~/stash/public/","title":"Place the Large File on the Proxy"},{"location":"materials/day4/part3-ex1-blast-proxy/#test-a-download-of-the-file","text":"Once the file is placed in the ~/stash/public directory, it can be downloaded from a corresponding URL such as http://stash.osgconnect.net/~%RED%username%ENDCOLOR%/pdbaa_files.tar.gz , where %RED%username is your username on user-training.osgconnect.net . Using the above convention (and from a different directory on user-training.osgconnect.net , any directory), you can test the download of your pdbaa_files.tar.gz file with a command like the following: %UCL_PROMPT_SHORT% <strong>wget http://stash.osgconnect.net/~%RED%username%ENDCOLOR%/pdbaa_files.tar.gz</strong> You may realize that you've been using wget to download files from a web proxy for many of the previous exercises at the school!","title":"Test a download of the file"},{"location":"materials/day4/part3-ex1-blast-proxy/#run-a-new-test-job","text":"Now, you'll repeat the last exercise (with a single input query file) but have HTCondor download the pdbaa_files.tar.gz file from the web proxy, instead of having the file transferred from the submit server.","title":"Run a New Test Job"},{"location":"materials/day4/part3-ex1-blast-proxy/#modify-the-submit-file-and-wrapper-script","text":"In the wrapper script, we have to add some special lines so that we can pull from Stash and use the HTTP proxy. In blast_wrapper.sh , we will have to add commands to pull the data file: #!/bin/bash %RED% # Set an environment variable to point to the local Proxy location at the cluster export http_proxy=$OSG_SQUID_LOCATION # Copy the pdbaa_files.tar.gz to the worker node # Add the -S argument, so we can see if it was a cache HIT or MISS wget -S http://stash.osgconnect.net/~username/pdbaa_files.tar.gz %ENDCOLOR% tar xvzf pdbaa_files.tar.gz tar xvzf blastx.tar.gz ./blastx -db pdbaa -query $1 -out $1.result rm pdbaa.* rm blastx The first line will set a special environment variable that wget will look at. $OSG_SQUID_LOCATION is a special variable that is set on OSG sites to the hostname of the nearest squid cache. The second line will download the pdbaa_files.tar.gz from stash, using the closes cache (because wget will look at http_proxy for the newest cache=). In your submit file, you will need to remove the pdbaa_files.tar.gz file from the transfer_input_files , because we are using HTTP proxies!","title":"Modify the submit file and wrapper script"},{"location":"materials/day4/part3-ex1-blast-proxy/#submit-the-test-job","text":"You may wish to first remove the log, result, output, and error files from the previous tests, which will be overwritten when the new test job completes. rm *.err *.out *.result *.log Submit the test job! When the job starts, the wrapper will download the pdbaa_files.tar.gz file from the web proxy. If the jobs takes longer than two minutes, you can assume that it will complete successfully, and then continue with the rest of the exercise. After the job completes examine the *.err file generated by the submission. At the top of the file, you will find something like: --2017-07-19 00:36:29-- http://stash.osgconnect.net/~osguser199/pdbaa_files.tar.gz Resolving cmsprxy01.colorado.edu... 192.12.238.152 Connecting to cmsprxy01.colorado.edu|192.12.238.152|:3128... connected. Proxy request sent, awaiting response... HTTP/1.0 200 OK Content-Type: application/octet-stream Content-Length: 22105114 Accept-Ranges: bytes Server: nginx/1.10.2 Date: Wed, 19 Jul 2017 00:36:29 GMT Last-Modified: Wed, 19 Jul 2017 05:03:09 GMT ETag: \"596ee80d-1514c1a\" X-Cache: HIT from cmsprxy01.colorado.edu Via: 1.1 cmsprxy01.colorado.edu:3128 (squid/frontier-squid-2.7.STABLE9-14) Connection: close Length: 22105114 (21M) [application/octet-stream] Saving to: `pdbaa_files.tar.gz' ... Notice the X-Cache line. It says it was a cache HIT from the proxy cmsprxy01.colorado.edu . Yay! You successfully used a proxy to cache data near your worker node! Notice, the name of the cache file may be different.","title":"Submit the test job"},{"location":"materials/day4/part3-ex1-blast-proxy/#run-all-100-jobs","text":"If all of the previous tests have gone okay, you can prepare to run all 100 jobs that will use the split input files. To make sure you're not going to generate too much data, use the size of files from the previous test to calculate how much total data you're going to add to the thur-blast-data directory for 100 jobs. In the submit file, you will need to change the queue line to scan all the rna files: ... queue inputfile matching mouse_rna.fa.* Submit all 100 jobs! They may take a while to all complete, but it will still be faster than the many hours it would have taken to blast the single, large mouse_rna.fa file without splitting it up. In the meantime, as long as the first several jobs are running for longer than two minutes, you can move on to the next exercise .","title":"Run all 100 Jobs!"},{"location":"materials/day4/part3-ex2-stashcache-shared/","text":"Thursday Exercise 3.2: Using StashCache for Large Shared Data \u00b6 This exercise will use a BLAST workflow to demonstrate the functionality of StashCache for transferring input files to jobs on OSG. Because our individual blast jobs from Exercise 3.1 would take a bit longer with a larger database (too long for an workable exercise), we'll imagine for this exercise that our pdbaa_files.tar.gz file is too large for a web proxy (larger than ~1 GB). For this exercise, we will use the input from Exercise 3.1, but instead of using the web proxy for the pdbaa database, we will place it in StashCache via the OSG Connect server. StashCache is a distributed set of caches spread across the U.S. They are connected with high bandwidth connections to each other, and to the data origin servers, where your data is originally placed. There are two methods of pulling data from StashCache, within jobs. We are in the middle of a transition from one to another. They are: stashcp (old): A command line program to copy files from StashCache with cp like syntax. 2. StashCache-over-CVMFS (new): A much more intuitive and fault tolerant method for accessing StashCache files. But only available on a few resources, for now. In the below tutorials, we will use stashcp , which will continue to be supported even after StashCache-over-CVMFS is fully functional. Setup \u00b6 Make sure you're logged in to user-training.osgconnect.net Transfer the following files from Exercise 3.1 to a new directory called thur-data-3.2 : blast_wrapper.sh , mouse_rna.fa.1 , mouse_rna.fa.2 , mouse_rna.fa.3 , and the most recent submit file. Place the Database in StashCache \u00b6 Copy to your public space on OSG Connect \u00b6 StashCache provides a public space for you to store data which can be accessed through the caching servers. First, you need to move your blast database into this public directory. If you remember the \"public\" directory in your home directory on the OSG Connect server user-training.osgconnect.net , it's this location where you will place files that need to end up in the StashCache data origin. You have already placed files in the ~/stash/public directory in the previous exercise in order for it to be accessible to the HTTP proxies. The same directory is accessible to StashCache. As the public directory name indicates *your files placed in the public directory will be accessible to anyone's jobs if they know how to use stashcp , though no one else will be able to edit the files, since only you can place or change files in your public space. For your own work in the future, make sure that you never put any sensitive data in such locations. Check the file on OSG Connect \u00b6 Next, you can check for the file and test the command that we'll use in jobs on the OSG Connect login node: %UCL_PROMPT_SHORT% <strong>ls public</strong> Now, load the stashcp module, which will allow you to test a copy of the file from StashCache into your home directory on login.osgconnect.net : %UCL_PROMPT_SHORT% <strong>module load stashcp</strong> %UCL_PROMPT_SHORT% <strong>module load xrootd</strong> %UCL_PROMPT_SHORT% <strong>stashcp /user/%RED%username%ENDCOLOR%/public/pdbaa_files.tar.gz ./</strong> You should now see the pdbaa_files.tar.gz file in your current directory. Notice that we had to include the /user and username in the file path for stashcp , which make sure you're copying from your public space. Modify the Submit File and Wrapper \u00b6 Return to your thur-data-stash directory on user-training.osgconnect.net (you may already be connected) where you will modify the files as described below: At the top of the wrapper script (after #!/bin/bash ), add the line to activate OSG Connect modules, followed by the above two lines that load the stashcp module and to copy the pdbaa_files.tar.gz file into the current directory of the job: module load xrootd module load stashcp stashcp /user/%RED%username%ENDCOLOR%/public/pdbaa_files.tar.gz ./ Since HTCondor will no longer transfer or download the file for you, make sure to add the following line (or modify your existing rm command, if you're confident) to make sure the pdbaa_files.tar.gz file is also deleted and not copied back as perceived output. rm pdbaa_files.tar.gz Delete the wget line from the job wrapper script blast_wrapper.sh Add the following line to the submit file and update the \"requirements\" statement to require servers with OSG Connect modules (for accessing the stashcp module), somewhere before the word queue . +WantsStashCache = true requirements = (OpSys == \"LINUX\") && (HAS_MODULES =?= true) Confirm that your queue statement is correct for the current directory. It should be something like: queue inputfile matching mouse_rna.fa.* And that mouse_rna.fa.* files exist in the current directory (you should have copied them from the previous exercise directory. Submit the Job \u00b6 Now submit and monitor the job! If your 100 jobs from the previous exercise haven't started running yet, this job will not yet start. However, after it has been running for ~2 minutes, you're safe to continue to the next exercise! Note: Keeping StashCache 'Clean' \u00b6 Just as for data on a web proxy, it is VERY important to remove old files from StashCache when you no longer need them, especially so that you'll have plenty of space for such files in the future. For example, you would delete ( rm ) files from public on user-training.osgconnect.net when you don't need them there anymore, but only after all jobs have finished. The next time you use StashCache after the school, remember to first check for old files that you can delete.","title":"3.2. StashCache \u2160"},{"location":"materials/day4/part3-ex2-stashcache-shared/#thursday-exercise-32-using-stashcache-for-large-shared-data","text":"This exercise will use a BLAST workflow to demonstrate the functionality of StashCache for transferring input files to jobs on OSG. Because our individual blast jobs from Exercise 3.1 would take a bit longer with a larger database (too long for an workable exercise), we'll imagine for this exercise that our pdbaa_files.tar.gz file is too large for a web proxy (larger than ~1 GB). For this exercise, we will use the input from Exercise 3.1, but instead of using the web proxy for the pdbaa database, we will place it in StashCache via the OSG Connect server. StashCache is a distributed set of caches spread across the U.S. They are connected with high bandwidth connections to each other, and to the data origin servers, where your data is originally placed. There are two methods of pulling data from StashCache, within jobs. We are in the middle of a transition from one to another. They are: stashcp (old): A command line program to copy files from StashCache with cp like syntax. 2. StashCache-over-CVMFS (new): A much more intuitive and fault tolerant method for accessing StashCache files. But only available on a few resources, for now. In the below tutorials, we will use stashcp , which will continue to be supported even after StashCache-over-CVMFS is fully functional.","title":"Thursday Exercise 3.2: Using StashCache for Large Shared Data"},{"location":"materials/day4/part3-ex2-stashcache-shared/#setup","text":"Make sure you're logged in to user-training.osgconnect.net Transfer the following files from Exercise 3.1 to a new directory called thur-data-3.2 : blast_wrapper.sh , mouse_rna.fa.1 , mouse_rna.fa.2 , mouse_rna.fa.3 , and the most recent submit file.","title":"Setup"},{"location":"materials/day4/part3-ex2-stashcache-shared/#place-the-database-in-stashcache","text":"","title":"Place the Database in StashCache"},{"location":"materials/day4/part3-ex2-stashcache-shared/#copy-to-your-public-space-on-osg-connect","text":"StashCache provides a public space for you to store data which can be accessed through the caching servers. First, you need to move your blast database into this public directory. If you remember the \"public\" directory in your home directory on the OSG Connect server user-training.osgconnect.net , it's this location where you will place files that need to end up in the StashCache data origin. You have already placed files in the ~/stash/public directory in the previous exercise in order for it to be accessible to the HTTP proxies. The same directory is accessible to StashCache. As the public directory name indicates *your files placed in the public directory will be accessible to anyone's jobs if they know how to use stashcp , though no one else will be able to edit the files, since only you can place or change files in your public space. For your own work in the future, make sure that you never put any sensitive data in such locations.","title":"Copy to your public space on OSG Connect"},{"location":"materials/day4/part3-ex2-stashcache-shared/#check-the-file-on-osg-connect","text":"Next, you can check for the file and test the command that we'll use in jobs on the OSG Connect login node: %UCL_PROMPT_SHORT% <strong>ls public</strong> Now, load the stashcp module, which will allow you to test a copy of the file from StashCache into your home directory on login.osgconnect.net : %UCL_PROMPT_SHORT% <strong>module load stashcp</strong> %UCL_PROMPT_SHORT% <strong>module load xrootd</strong> %UCL_PROMPT_SHORT% <strong>stashcp /user/%RED%username%ENDCOLOR%/public/pdbaa_files.tar.gz ./</strong> You should now see the pdbaa_files.tar.gz file in your current directory. Notice that we had to include the /user and username in the file path for stashcp , which make sure you're copying from your public space.","title":"Check the file on OSG Connect"},{"location":"materials/day4/part3-ex2-stashcache-shared/#modify-the-submit-file-and-wrapper","text":"Return to your thur-data-stash directory on user-training.osgconnect.net (you may already be connected) where you will modify the files as described below: At the top of the wrapper script (after #!/bin/bash ), add the line to activate OSG Connect modules, followed by the above two lines that load the stashcp module and to copy the pdbaa_files.tar.gz file into the current directory of the job: module load xrootd module load stashcp stashcp /user/%RED%username%ENDCOLOR%/public/pdbaa_files.tar.gz ./ Since HTCondor will no longer transfer or download the file for you, make sure to add the following line (or modify your existing rm command, if you're confident) to make sure the pdbaa_files.tar.gz file is also deleted and not copied back as perceived output. rm pdbaa_files.tar.gz Delete the wget line from the job wrapper script blast_wrapper.sh Add the following line to the submit file and update the \"requirements\" statement to require servers with OSG Connect modules (for accessing the stashcp module), somewhere before the word queue . +WantsStashCache = true requirements = (OpSys == \"LINUX\") && (HAS_MODULES =?= true) Confirm that your queue statement is correct for the current directory. It should be something like: queue inputfile matching mouse_rna.fa.* And that mouse_rna.fa.* files exist in the current directory (you should have copied them from the previous exercise directory.","title":"Modify the Submit File and Wrapper"},{"location":"materials/day4/part3-ex2-stashcache-shared/#submit-the-job","text":"Now submit and monitor the job! If your 100 jobs from the previous exercise haven't started running yet, this job will not yet start. However, after it has been running for ~2 minutes, you're safe to continue to the next exercise!","title":"Submit the Job"},{"location":"materials/day4/part3-ex2-stashcache-shared/#note-keeping-stashcache-clean","text":"Just as for data on a web proxy, it is VERY important to remove old files from StashCache when you no longer need them, especially so that you'll have plenty of space for such files in the future. For example, you would delete ( rm ) files from public on user-training.osgconnect.net when you don't need them there anymore, but only after all jobs have finished. The next time you use StashCache after the school, remember to first check for old files that you can delete.","title":"Note: Keeping StashCache 'Clean'"},{"location":"materials/day4/part3-ex3-stashcache-unique/","text":"Thursday Exercise 3.3: Using Stash for unique large input \u00b6 In this exercise, we will run a multimedia program that converts and manipulates video files. In particular, we want to convert large \\ .mov files to smaller (10-100s of MB) mp4 files. \\ Just like the Blast database in the previous exercise , these video files are too large to send to jobs using HTCondor's default \\ file transfer mechanism, so we'll be using the Stash tool to send our data to jobs. This exercise should take 25-30 minutes. Data \u00b6 We'll start by moving our source movie files into Stash, so that they'll be available to our jobs when they run out on OSG. Log into user-training.osgconnect.net and move into the public directory. The video files are currently stored on the squid proxy from the first exercise this afternoon. To place them in Stash, download them \\ using wget : \\ %UCL_PROMPT_SHORT% <strong>wget http://proxy.chtc.wisc.edu/osgschool17/videos.tar.gz</strong> \\ Once downloaded, untar the tar.gz file. It should contain three .mov files. (this may take a while since everyone else is likely doing the same thing) How big are the three files? Which is the smallest? (Find out with ls -lh .) We're going to need a list of these files later. For now, let's save that list to a file in this directory by running ls and redirecting the output to a file: \\ %UCL_PROMPT_SHORT% <strong>ls *.MOV *.mov > movie_list.txt</strong> Once you've examined the three mov files and created the list of files, remove the original tar.gz file. Software \u00b6 We'll be using a multi-purpose media tool called ffmpeg \\ to convert video formats. The basic command to convert a file looks like this: \\ %UCL_PROMPT_SHORT% <strong>./ffmpeg -i input.mov output.mp4</strong> In order to resize our files, we're going to manually set the video bitrate and resize the frames, so that the resulting file is smaller. %UCL_PROMPT_SHORT% <strong>./ffmpeg -i input.mp4 -b:v 400k -s 640x360 output.mp4</strong> To get the ffmpeg program do the following: On user-training.osgconnect.net , create a directory for this exercise and move into it. We'll be downloading the ffmpeg pre-built static binary from this page: http://johnvansickle.com/ffmpeg/ . \\ Look for the x86_64 build. \\ %UCL_PROMPT_SHORT% <strong>wget http://johnvansickle.com/ffmpeg/releases/ffmpeg-release-64bit-static.tar.xz</strong> Once the binary is downloaded, un-tar it, and then copy the main ffmpeg program into your current directory: \\ %UCL_PROMPT_SHORT% <strong>tar -xf ffmpeg-release-64bit-static.tar.xz</strong> %UCL_PROMPT_SHORT% <strong>cp ffmpeg-3.3.2-64bit-static/ffmpeg ./</strong> Script \u00b6 We want to write a script that uses ffmpeg to convert a .mov file to a smaller format. Our script will need to copy \\ that movie file from Stash to the job's current working directory (as in the previous exercise , run the appropriate ffmpeg command, \\ and then remove the original movie file so that it doesn't get transferred back to the submit server. This last step is \\ particularly important, as otherwise you will have large files transferring into the submit server and filling up your home directory space. Create a file called run_ffmpeg.sh , that does the steps described above. Use the name of the smallest .mov file \\ in the ffmpeg command. Once you've written your script, check it against the example below: \\ #!/bin/bash module load stashcp stashcp /user/%RED%username%ENDCOLOR%/public/test_open_terminal.mov ./ ./ffmpeg -i test_open_terminal.mov -b:v 400k -s 640x360 test_open_terminal.mp4 rm test_open_terminal.mov \\ In your script, the username should be replaced by your osg-connect username. Ultimately we'll want to submit several jobs (one for each .mov file), but to start with, we'll run one job to \\ make sure that everything works. Submit File \u00b6 Create a submit file for this job, based on other submit files from the school ( This file, for example .) Things to consider: We'll be copying the video file into the job's working directory, so make sure to request enough disk space for the input mov file and the output mp4 file. \\ If you're aren't sure how much to request, ask a helper in the room. *Important* Don't list the name of the .mov in transfer_input_files . Our job will be interacting with the input \\ .mov files solely from within the script we wrote above. Note that we do need to transfer the ffmpeg program that we downloaded above. \\ transfer_input_files = ffmpeg Add the same requirements as the previous exercise: \\ +WantsStashCache = true requirements = (OpSys == \"LINUX\") && (HAS_MODULES =?= true) Initial Job \u00b6 With everything in place, submit the job. Once it finishes, we should check to make sure everything ran as expected: Check the directory where you submitted the job. Did the output .mp4 file return? Also in the directory where you submitted the job - did the original .mov file return here accidentally? Check file sizes. How big is the returned .mp4 file? How does that compare to the original .mov input? If your job successfully returned the converted .mp4 file and not the .mov file to the submit server, and the .mp4 file was appropriately scaled down, then we can go ahead and convert all of the files we uploaded to Stash. Multiple jobs \u00b6 We wrote the name of the .mov file into our run_ffmpeg.sh executable script. To submit a set of jobs for all of our .mov \\ files, what will we need to change in: the script? 2. the submit file? Once you've thought about it, check your reasoning against \\ the instructions below. Add an argument to your script \u00b6 Look at your run_ffmpeg.sh script. What values will change for every job? The input file will change with every job - and don't forget that the output file will too! Let's make them both into arguments. \\ To add arguments to a bash script, we use the notation $1 for the first argument (our input file) and $2 for the second argument (our output file name). \\ The final script should look like this: \\ #!/bin/bash module load stashcp stashcp /user/%RED%username%ENDCOLOR%/public/$1 ./ ./ffmpeg -i $1 -b:v 400k -s 640x360 $2 rm $1 \\ Note that we use the input file name multiple times in our script, so we'll have to use $1 multiple times as well. Modify your submit file \u00b6 We now need to tell each job what arguments to use. We will do this by adding an arguments line to our submit file. Because we'll only have \\ the input file name, the \"output\" file name will be the input file name with the mp4 extension. That should look like this: \\ arguments = $(mov) $(mov).mp4 To set these arguments, we will use the queue .. matching syntax that we learned on Monday . To \\ do so, we need to create a list of our input files. 3. In our submit file, we can then change our queue statement to: \\ queue mov from movie_list.txt Once you've made these changes, try submitting all the jobs! Bonus \u00b6 If you wanted to set a different output file name, bitrate and/or size for each original movie, how could you modify: movie_list.txt 3. Your submit file 2. run_ffmpeg.sh to do so? Show hint Here's the changes you can make to the various files: 1. `movie_list.txt` \\\\ ducks.MOV ducks.mp4 500k 1280x720 teaching.MOV teaching.mp4 400k 320x180 test_open_terminal.mov terminal.mp4 600k 640x360 2. Submit file\\\\ arguments = $(mov) $(mp4) $(bitrate) $(size) queue mov,mp4,bitrate,size from movie_list.txt 3. `run_ffmpeg.sh` \\\\ #!/bin/bash module load stashcp stashcp /user/%RED%username%ENDCOLOR%/public/$1 ./ ./ffmpeg -i $1 -b:v $3 -s $4 $2 rm $1","title":"3.3. StashCache \u2161"},{"location":"materials/day4/part3-ex3-stashcache-unique/#thursday-exercise-33-using-stash-for-unique-large-input","text":"In this exercise, we will run a multimedia program that converts and manipulates video files. In particular, we want to convert large \\ .mov files to smaller (10-100s of MB) mp4 files. \\ Just like the Blast database in the previous exercise , these video files are too large to send to jobs using HTCondor's default \\ file transfer mechanism, so we'll be using the Stash tool to send our data to jobs. This exercise should take 25-30 minutes.","title":"Thursday Exercise 3.3: Using Stash for unique large input"},{"location":"materials/day4/part3-ex3-stashcache-unique/#data","text":"We'll start by moving our source movie files into Stash, so that they'll be available to our jobs when they run out on OSG. Log into user-training.osgconnect.net and move into the public directory. The video files are currently stored on the squid proxy from the first exercise this afternoon. To place them in Stash, download them \\ using wget : \\ %UCL_PROMPT_SHORT% <strong>wget http://proxy.chtc.wisc.edu/osgschool17/videos.tar.gz</strong> \\ Once downloaded, untar the tar.gz file. It should contain three .mov files. (this may take a while since everyone else is likely doing the same thing) How big are the three files? Which is the smallest? (Find out with ls -lh .) We're going to need a list of these files later. For now, let's save that list to a file in this directory by running ls and redirecting the output to a file: \\ %UCL_PROMPT_SHORT% <strong>ls *.MOV *.mov > movie_list.txt</strong> Once you've examined the three mov files and created the list of files, remove the original tar.gz file.","title":"Data"},{"location":"materials/day4/part3-ex3-stashcache-unique/#software","text":"We'll be using a multi-purpose media tool called ffmpeg \\ to convert video formats. The basic command to convert a file looks like this: \\ %UCL_PROMPT_SHORT% <strong>./ffmpeg -i input.mov output.mp4</strong> In order to resize our files, we're going to manually set the video bitrate and resize the frames, so that the resulting file is smaller. %UCL_PROMPT_SHORT% <strong>./ffmpeg -i input.mp4 -b:v 400k -s 640x360 output.mp4</strong> To get the ffmpeg program do the following: On user-training.osgconnect.net , create a directory for this exercise and move into it. We'll be downloading the ffmpeg pre-built static binary from this page: http://johnvansickle.com/ffmpeg/ . \\ Look for the x86_64 build. \\ %UCL_PROMPT_SHORT% <strong>wget http://johnvansickle.com/ffmpeg/releases/ffmpeg-release-64bit-static.tar.xz</strong> Once the binary is downloaded, un-tar it, and then copy the main ffmpeg program into your current directory: \\ %UCL_PROMPT_SHORT% <strong>tar -xf ffmpeg-release-64bit-static.tar.xz</strong> %UCL_PROMPT_SHORT% <strong>cp ffmpeg-3.3.2-64bit-static/ffmpeg ./</strong>","title":"Software"},{"location":"materials/day4/part3-ex3-stashcache-unique/#script","text":"We want to write a script that uses ffmpeg to convert a .mov file to a smaller format. Our script will need to copy \\ that movie file from Stash to the job's current working directory (as in the previous exercise , run the appropriate ffmpeg command, \\ and then remove the original movie file so that it doesn't get transferred back to the submit server. This last step is \\ particularly important, as otherwise you will have large files transferring into the submit server and filling up your home directory space. Create a file called run_ffmpeg.sh , that does the steps described above. Use the name of the smallest .mov file \\ in the ffmpeg command. Once you've written your script, check it against the example below: \\ #!/bin/bash module load stashcp stashcp /user/%RED%username%ENDCOLOR%/public/test_open_terminal.mov ./ ./ffmpeg -i test_open_terminal.mov -b:v 400k -s 640x360 test_open_terminal.mp4 rm test_open_terminal.mov \\ In your script, the username should be replaced by your osg-connect username. Ultimately we'll want to submit several jobs (one for each .mov file), but to start with, we'll run one job to \\ make sure that everything works.","title":"Script"},{"location":"materials/day4/part3-ex3-stashcache-unique/#submit-file","text":"Create a submit file for this job, based on other submit files from the school ( This file, for example .) Things to consider: We'll be copying the video file into the job's working directory, so make sure to request enough disk space for the input mov file and the output mp4 file. \\ If you're aren't sure how much to request, ask a helper in the room. *Important* Don't list the name of the .mov in transfer_input_files . Our job will be interacting with the input \\ .mov files solely from within the script we wrote above. Note that we do need to transfer the ffmpeg program that we downloaded above. \\ transfer_input_files = ffmpeg Add the same requirements as the previous exercise: \\ +WantsStashCache = true requirements = (OpSys == \"LINUX\") && (HAS_MODULES =?= true)","title":"Submit File"},{"location":"materials/day4/part3-ex3-stashcache-unique/#initial-job","text":"With everything in place, submit the job. Once it finishes, we should check to make sure everything ran as expected: Check the directory where you submitted the job. Did the output .mp4 file return? Also in the directory where you submitted the job - did the original .mov file return here accidentally? Check file sizes. How big is the returned .mp4 file? How does that compare to the original .mov input? If your job successfully returned the converted .mp4 file and not the .mov file to the submit server, and the .mp4 file was appropriately scaled down, then we can go ahead and convert all of the files we uploaded to Stash.","title":"Initial Job"},{"location":"materials/day4/part3-ex3-stashcache-unique/#multiple-jobs","text":"We wrote the name of the .mov file into our run_ffmpeg.sh executable script. To submit a set of jobs for all of our .mov \\ files, what will we need to change in: the script? 2. the submit file? Once you've thought about it, check your reasoning against \\ the instructions below.","title":"Multiple jobs"},{"location":"materials/day4/part3-ex3-stashcache-unique/#add-an-argument-to-your-script","text":"Look at your run_ffmpeg.sh script. What values will change for every job? The input file will change with every job - and don't forget that the output file will too! Let's make them both into arguments. \\ To add arguments to a bash script, we use the notation $1 for the first argument (our input file) and $2 for the second argument (our output file name). \\ The final script should look like this: \\ #!/bin/bash module load stashcp stashcp /user/%RED%username%ENDCOLOR%/public/$1 ./ ./ffmpeg -i $1 -b:v 400k -s 640x360 $2 rm $1 \\ Note that we use the input file name multiple times in our script, so we'll have to use $1 multiple times as well.","title":"Add an argument to your script"},{"location":"materials/day4/part3-ex3-stashcache-unique/#modify-your-submit-file","text":"We now need to tell each job what arguments to use. We will do this by adding an arguments line to our submit file. Because we'll only have \\ the input file name, the \"output\" file name will be the input file name with the mp4 extension. That should look like this: \\ arguments = $(mov) $(mov).mp4 To set these arguments, we will use the queue .. matching syntax that we learned on Monday . To \\ do so, we need to create a list of our input files. 3. In our submit file, we can then change our queue statement to: \\ queue mov from movie_list.txt Once you've made these changes, try submitting all the jobs!","title":"Modify your submit file"},{"location":"materials/day4/part3-ex3-stashcache-unique/#bonus","text":"If you wanted to set a different output file name, bitrate and/or size for each original movie, how could you modify: movie_list.txt 3. Your submit file 2. run_ffmpeg.sh to do so? Show hint Here's the changes you can make to the various files: 1. `movie_list.txt` \\\\ ducks.MOV ducks.mp4 500k 1280x720 teaching.MOV teaching.mp4 400k 320x180 test_open_terminal.mov terminal.mp4 600k 640x360 2. Submit file\\\\ arguments = $(mov) $(mp4) $(bitrate) $(size) queue mov,mp4,bitrate,size from movie_list.txt 3. `run_ffmpeg.sh` \\\\ #!/bin/bash module load stashcp stashcp /user/%RED%username%ENDCOLOR%/public/$1 ./ ./ffmpeg -i $1 -b:v $3 -s $4 $2 rm $1","title":"Bonus"},{"location":"materials/day4/part4-ex1-input/","text":"Thursday Exercise 4.1: Large Input Data \u00b6 In this exercise, we will do a similar version of the previous exercise . This exercise should take 10-15 minutes. Background \u00b6 In the previous exercises, we used two \"web-based\" tools to stage and deliver our files to jobs: the squid web proxy \\ and Stash . Another alternative for handling large files (both input and output), especially if they are unique to each job, is a local \\ shared filesystem. This is a filesystem that all (or most) of the execute servers can access, so data stored there can be copied \\ to the job from that system instead of as a transfer or download. For this example, we'll be submitting the same jobs as the previous exercise , but we will stage our \\ data in a shared filesystem local to CHTC. The name of our shared filesystem is Gluster and user directories are found as sub-directories \\ of the path /mnt/gluster . This is just one example of what it can look like to use a shared filesystem. If you are running \\ jobs at your own institution, the shared filesystem and how to access it may be different. Accessing the Filesystem \u00b6 Because our shared filesystem is only available on the local CHTC HTCondor pool, you'll need to log into our local submit server, learn.chtc.wisc.edu . Once you've logged in, navigate to your Gluster directory. It should be at the location /mnt/gluster/username , where username is your username on learn.chtc.wisc.edu . Previous Files \u00b6 Data \u00b6 Like the previous example, we'll start by downloading our source movie files into the Gluster directory. Run this command in your Gluster directory . %UCL_PROMPT_SHORT% <strong>wget http://proxy.chtc.wisc.edu/SQUID/osgschool17/videos.tar.gz</strong> While the files are copying, feel free to open a second connection to learn.chtc.wisc.edu and follow the instructions below. Once the files have finished downloading, untar them. Software, Executable, Submit File \u00b6 Because these jobs will be similar to the previous exercise, we can copy the software ( ffmpeg ), our executable ( run_ffmpeg.sh ) \\ and submit file from user-training.osgconnect.net to learn.chtc.wisc.edu , or, feel free to replicate these by following the instructions in the previous exercise . These files should go into a sub-directory of your home directory, not your Gluster directory . Ch-ch-ch-changes \u00b6 What changes will we need to make to our previous job submission in order to submit it in CHTC, using the Gluster location? Read on. Script \u00b6 The major actions of our script will be the same: copy \\ the movie file to the job's current working directory, run the appropriate ffmpeg command, \\ and then remove the original movie file. The main difference is that the mov file will be copied from \\ your Gluster directory instead of being downloaded from Stash. Like before, your script should remove \\ that file before the job completes so that it doesn't get transferred back to the submit server. Change the first command of your run_ffmpeg.sh script to: \\ cp /mnt/gluster/%RED%username%ENDCOLOR%/test_open_terminal.mov ./ \\ You should use your username on learn.chtc.wisc.edu in the path above. If you have a version of the script that uses arguments instead of the filenames, \\ that's okay. Submit File \u00b6 Remove any previous requirements and add a line to the file (before the final queue statement) that ensures your job will land on computers that have access to Gluster: \\ requirements = (Target.HasGluster == true) Initial Job \u00b6 As before, we should test our job submission with a single mov file before submitting jobs for all three. Alter your submit file (if necessary) to \\ run a job that converts the test_open_terminal.mov file. Once the job finishes, check to make sure everything ran as expected: Check the directory where you submitted the job. Did the output .mp4 file return? Also in the directory where you submitted the job - did the original .mov file return here accidentally? Check file sizes. How big is the returned .mp4 file? How does that compare to the original .mov input? If your job successfully returned the converted .mp4 file and not the .mov file to the submit server, and the .mp4 file was appropriately scaled down, then our script did what it should have. Multiple jobs \u00b6 Change your submit file as in the previous exercise in order to submit 3 jobs to convert all three files!","title":"4.1. Shared FS input"},{"location":"materials/day4/part4-ex1-input/#thursday-exercise-41-large-input-data","text":"In this exercise, we will do a similar version of the previous exercise . This exercise should take 10-15 minutes.","title":"Thursday Exercise 4.1: Large Input Data"},{"location":"materials/day4/part4-ex1-input/#background","text":"In the previous exercises, we used two \"web-based\" tools to stage and deliver our files to jobs: the squid web proxy \\ and Stash . Another alternative for handling large files (both input and output), especially if they are unique to each job, is a local \\ shared filesystem. This is a filesystem that all (or most) of the execute servers can access, so data stored there can be copied \\ to the job from that system instead of as a transfer or download. For this example, we'll be submitting the same jobs as the previous exercise , but we will stage our \\ data in a shared filesystem local to CHTC. The name of our shared filesystem is Gluster and user directories are found as sub-directories \\ of the path /mnt/gluster . This is just one example of what it can look like to use a shared filesystem. If you are running \\ jobs at your own institution, the shared filesystem and how to access it may be different.","title":"Background"},{"location":"materials/day4/part4-ex1-input/#accessing-the-filesystem","text":"Because our shared filesystem is only available on the local CHTC HTCondor pool, you'll need to log into our local submit server, learn.chtc.wisc.edu . Once you've logged in, navigate to your Gluster directory. It should be at the location /mnt/gluster/username , where username is your username on learn.chtc.wisc.edu .","title":"Accessing the Filesystem"},{"location":"materials/day4/part4-ex1-input/#previous-files","text":"","title":"Previous Files"},{"location":"materials/day4/part4-ex1-input/#data","text":"Like the previous example, we'll start by downloading our source movie files into the Gluster directory. Run this command in your Gluster directory . %UCL_PROMPT_SHORT% <strong>wget http://proxy.chtc.wisc.edu/SQUID/osgschool17/videos.tar.gz</strong> While the files are copying, feel free to open a second connection to learn.chtc.wisc.edu and follow the instructions below. Once the files have finished downloading, untar them.","title":"Data"},{"location":"materials/day4/part4-ex1-input/#software-executable-submit-file","text":"Because these jobs will be similar to the previous exercise, we can copy the software ( ffmpeg ), our executable ( run_ffmpeg.sh ) \\ and submit file from user-training.osgconnect.net to learn.chtc.wisc.edu , or, feel free to replicate these by following the instructions in the previous exercise . These files should go into a sub-directory of your home directory, not your Gluster directory .","title":"Software, Executable, Submit File"},{"location":"materials/day4/part4-ex1-input/#ch-ch-ch-changes","text":"What changes will we need to make to our previous job submission in order to submit it in CHTC, using the Gluster location? Read on.","title":"Ch-ch-ch-changes"},{"location":"materials/day4/part4-ex1-input/#script","text":"The major actions of our script will be the same: copy \\ the movie file to the job's current working directory, run the appropriate ffmpeg command, \\ and then remove the original movie file. The main difference is that the mov file will be copied from \\ your Gluster directory instead of being downloaded from Stash. Like before, your script should remove \\ that file before the job completes so that it doesn't get transferred back to the submit server. Change the first command of your run_ffmpeg.sh script to: \\ cp /mnt/gluster/%RED%username%ENDCOLOR%/test_open_terminal.mov ./ \\ You should use your username on learn.chtc.wisc.edu in the path above. If you have a version of the script that uses arguments instead of the filenames, \\ that's okay.","title":"Script"},{"location":"materials/day4/part4-ex1-input/#submit-file","text":"Remove any previous requirements and add a line to the file (before the final queue statement) that ensures your job will land on computers that have access to Gluster: \\ requirements = (Target.HasGluster == true)","title":"Submit File"},{"location":"materials/day4/part4-ex1-input/#initial-job","text":"As before, we should test our job submission with a single mov file before submitting jobs for all three. Alter your submit file (if necessary) to \\ run a job that converts the test_open_terminal.mov file. Once the job finishes, check to make sure everything ran as expected: Check the directory where you submitted the job. Did the output .mp4 file return? Also in the directory where you submitted the job - did the original .mov file return here accidentally? Check file sizes. How big is the returned .mp4 file? How does that compare to the original .mov input? If your job successfully returned the converted .mp4 file and not the .mov file to the submit server, and the .mp4 file was appropriately scaled down, then our script did what it should have.","title":"Initial Job"},{"location":"materials/day4/part4-ex1-input/#multiple-jobs","text":"Change your submit file as in the previous exercise in order to submit 3 jobs to convert all three files!","title":"Multiple jobs"},{"location":"materials/day4/part4-ex2-output/","text":"Thursday Exercise 4.2: Large Output Data \u00b6 In this exercise, we will run a job that produces a very large output file, based on a few parameters. This exercise should take 15-20 minutes. Background \u00b6 This exercise will be the reverse of the previous exercise! Instead of large input/small output, we will be using a program \\ that has no input except for a few arguments on the command line, but produces a file that is several GB in size. \\ As before, we will need to write a shell script that runs the program and handles the data. The Program \u00b6 If you haven't already, log in to learn.chtc.wisc.edu . Download the software package and untar it. %UCL_PROMPT_SHORT% <strong>wget http://proxy.chtc.wisc.edu/SQUID/osgschool17/motif-flanks.tar.gz</strong> %UCL_PROMPT_SHORT% <strong>tar -xzf motif-flanks.tar.gz</strong> Use the cd command to enter the unpacked motif-flanks directory. Take a look at the README file and then do the following: Compile the code. 2. Run the program without any arguments. 3. Based on the README, what is the largest amount of data we might expect? This program generates all permutations of nucleotide sequences \\ surrounding a given DNA motif. We can choose the length of permutation we want both \\ before and after a motif of our choice. To use this program on the command line and save the output to a FASTA file, we can use the command: %UCL_PROMPT_SHORT% <strong>./motif-flanks 2 AGTTCATGCCT 2 > sequences.fa</strong> According to the usage information and README, the two numerical arguments can add up to 13, at most, and the middle sequence can be any \\ DNA sequence up to 20 characters. The largest output we can expect is around 4 GB. Test Job \u00b6 Having output of up to 4 GB means two things: we will want to run a smaller \\ test before we run the program at its peak, and the output data will need to go into a shared location like Gluster, instead of returning \\ to the submit server. First, we'll create a shell script to serve as the job's executable. What commands do you need to put in the script? What do you need to do with the sequences.fa file before the job exits? Our script needs to run the motif-flanks command as shown above, redirecting the output to a file called sequences.fa . Then, \\ after that command completes, the sequences.fa file should be moved to your Gluster directory, as it is too large to return to the \\ submit server as usual. Write the script and then check it against the script below. Yours might look slightly different. \\ #!/bin/sh ./motif-flanks 4 GATTTTCGATC 4 > sequences.fa mv sequences.fa /mnt/gluster/%RED%username%ENDCOLOR%/ Note that the two arguments in the script (4 and 4) are much smaller than the total possible for the software (two values that add up to 13). This is \\ because we want to run a smaller test before submitting a job with the largest possible combination of arguments. Next, create a submit file for this job, based on other submit files from the school. Some important considerations: We're writing our file to the job's working directory, so make sure to request several GB of disk space. Add a line to the file that ensures your job will land on computers that have access to Gluster (see the file from the last exercise ). The executable will be the script you wrote above. Once you have a submit file that does all these things, submit the test job. Once the job has completed, do the following: Check the directory where you submitted the job. Has the sequences.fa file returned there, accidentally? Check your Gluster directory. Did the sequences.fa file get copied there successfully? Check file size. How big is the sequences.fa file? You can use the ls -lh command with the filename to find out. If your job successfully copied the sequences.fa file to Gluster and did not return it to your submission directory on \\ the submit server, congratulations! Everything is working as it should and you can now submit a full job. Final Job \u00b6 Having done a test, it should be straightforward to run a \"full scale\" job. Edit your run_motif.sh executable so that the motif-flanks command \\ uses larger numerical arguments: ./motif-flanks 6 GATTTTCGATC 7 > sequences.fa Then submit your job. When it completes, check the size of the output file in Gluster.","title":"4.2. Shared FS output"},{"location":"materials/day4/part4-ex2-output/#thursday-exercise-42-large-output-data","text":"In this exercise, we will run a job that produces a very large output file, based on a few parameters. This exercise should take 15-20 minutes.","title":"Thursday Exercise 4.2: Large Output Data"},{"location":"materials/day4/part4-ex2-output/#background","text":"This exercise will be the reverse of the previous exercise! Instead of large input/small output, we will be using a program \\ that has no input except for a few arguments on the command line, but produces a file that is several GB in size. \\ As before, we will need to write a shell script that runs the program and handles the data.","title":"Background"},{"location":"materials/day4/part4-ex2-output/#the-program","text":"If you haven't already, log in to learn.chtc.wisc.edu . Download the software package and untar it. %UCL_PROMPT_SHORT% <strong>wget http://proxy.chtc.wisc.edu/SQUID/osgschool17/motif-flanks.tar.gz</strong> %UCL_PROMPT_SHORT% <strong>tar -xzf motif-flanks.tar.gz</strong> Use the cd command to enter the unpacked motif-flanks directory. Take a look at the README file and then do the following: Compile the code. 2. Run the program without any arguments. 3. Based on the README, what is the largest amount of data we might expect? This program generates all permutations of nucleotide sequences \\ surrounding a given DNA motif. We can choose the length of permutation we want both \\ before and after a motif of our choice. To use this program on the command line and save the output to a FASTA file, we can use the command: %UCL_PROMPT_SHORT% <strong>./motif-flanks 2 AGTTCATGCCT 2 > sequences.fa</strong> According to the usage information and README, the two numerical arguments can add up to 13, at most, and the middle sequence can be any \\ DNA sequence up to 20 characters. The largest output we can expect is around 4 GB.","title":"The Program"},{"location":"materials/day4/part4-ex2-output/#test-job","text":"Having output of up to 4 GB means two things: we will want to run a smaller \\ test before we run the program at its peak, and the output data will need to go into a shared location like Gluster, instead of returning \\ to the submit server. First, we'll create a shell script to serve as the job's executable. What commands do you need to put in the script? What do you need to do with the sequences.fa file before the job exits? Our script needs to run the motif-flanks command as shown above, redirecting the output to a file called sequences.fa . Then, \\ after that command completes, the sequences.fa file should be moved to your Gluster directory, as it is too large to return to the \\ submit server as usual. Write the script and then check it against the script below. Yours might look slightly different. \\ #!/bin/sh ./motif-flanks 4 GATTTTCGATC 4 > sequences.fa mv sequences.fa /mnt/gluster/%RED%username%ENDCOLOR%/ Note that the two arguments in the script (4 and 4) are much smaller than the total possible for the software (two values that add up to 13). This is \\ because we want to run a smaller test before submitting a job with the largest possible combination of arguments. Next, create a submit file for this job, based on other submit files from the school. Some important considerations: We're writing our file to the job's working directory, so make sure to request several GB of disk space. Add a line to the file that ensures your job will land on computers that have access to Gluster (see the file from the last exercise ). The executable will be the script you wrote above. Once you have a submit file that does all these things, submit the test job. Once the job has completed, do the following: Check the directory where you submitted the job. Has the sequences.fa file returned there, accidentally? Check your Gluster directory. Did the sequences.fa file get copied there successfully? Check file size. How big is the sequences.fa file? You can use the ls -lh command with the filename to find out. If your job successfully copied the sequences.fa file to Gluster and did not return it to your submission directory on \\ the submit server, congratulations! Everything is working as it should and you can now submit a full job.","title":"Test Job"},{"location":"materials/day4/part4-ex2-output/#final-job","text":"Having done a test, it should be straightforward to run a \"full scale\" job. Edit your run_motif.sh executable so that the motif-flanks command \\ uses larger numerical arguments: ./motif-flanks 6 GATTTTCGATC 7 > sequences.fa Then submit your job. When it completes, check the size of the output file in Gluster.","title":"Final Job"},{"location":"materials/day5/part1-ex1-science-intro/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Friday Exercise 1.1: Learn about Joe's Desired Computing Work \u00b6 In today's set of hands-on exercises, you'll be examining a set of pre-existing job submissions (scripts and submit files) and then coordinating them to run as a single workflow using a DAG. Along the way, you'll need to test some of the already written submit files. The first part of this exercise introduces the overall steps for the workflow and what pieces already exist (that you won't need to write yourself). Introduction \u00b6 Joe Biologist studies the influence of genetic differences (genotypes) on the growth traits (phenotypes) of a common crop. He has come to you with some computing work he\u2019d like to automate as a workflow, so that he can scale out his research and run it on a much larger HTC compute system. He has used HTCondor to manually submit individual jobs (never used a DAG), but has only run jobs on a smaller, dedicated HTCondor cluster where he doesn\u2019t have to request CPU, disk, or RAM. He\u2019s not familiar with DAGman. He's getting annoyed by how many manual job submissions and summary scripts he has to run, especially as he's thinking of scaling out his work in a way that would be much more cumbersome - so he's asking you to help him organize all the steps into an optimized DAG workflow. (As you learn more details below, it might be helpful to draw a diagram that describes the steps of his workflow.) After asking Joe many questions, you realize the following things: \u00b6 A) Joe starts with a file called input.csv that was made on his desktop. When you open this file in excel, Joe shows you that each row corresponds to an individual plant. For each individual, the first three columns are phenotype measurements for each of three different growth traits (T240, T300, T360), and the remaining columns contain the genotype (A or B) of that individual across multiple genes (columns). B) At present, Joe does the following manual steps (for each of the three traits) to achieve a meaningful result from the data in input.csv : Joe first submits a job that calculates a large number of hypothetical permutations of phenotype-genotype combinations for the trait. This produces a set of files containing all the permutations for that trait. 2. Joe then runs a script for each trait, to create a gzipped tar file of the permutation job results for that trait. 3. Joe then submits a second job for each trait, which uses the permutations that have been zipped together to perform a QTL mapping calculation. This calculation determines which genes were most important for the phenotype trait, relative to other possible genes exemplified in the random permutations. 4. As a final step for each trait, Joe runs a script that creates a gzipped tar file of all of the permutation and QTL mapping data for that trait, so that he can take it back to his desktop computer for final processing. C) For the permutation and QTL mapping calculations, Joe runs a special wrapper that then uses his main R scripts. For each type of calculation this perl script, runR.pl , first installs R using files from Joe's proxy server and then sets up the R environment before running whatever R script has been indicated as an argument to runR.pl . Therefore, the runR.pl wrapper script is run as the HTCondor executable with the name of either R program as an argument to the wrapper (along with arguments to the R program, itself). D) The two tar steps that happen after the permutation and QTL mapping steps are simple shell scripts containing a relevant tar command, and can be run on the submit server because they create little CPU load and finish very quickly. E) When Joe has previously run the first step, of 10,000 permutations, as one Condor job, the job runs for hours on his own small cluster. He has cut the permutations of a single Condor job up into multiple, shorter Condor jobs of fewer permutations before, but he doesn\u2019t remember the details of how many jobs and how many permutations per job worked well. Furthermore, Joe would like to scale up to 100,000 permutations per trait this time. Unlike the permutation step, the QTL step finishes quite quickly; even with 10,000 permutations completed for each trait, the QTL step completes within several minutes. View Joe's files \u00b6 (but don't do anything with them until the next exercise , where you'll plan necessary developments of a workflow for Joe.) Log in to learn.chtc.wisc.edu and move to a desired location in your home directory. Enter the following commands to download and decompress/untar Joe\u2019s job ingredients: %UCL_PROMPT_SHORT% <strong>wget http://proxy.chtc.wisc.edu/SQUID/osgschool17/WorkflowExercise.tar.gz</strong> %UCL_PROMPT_SHORT% <strong>tar -xzf WorkflowExercise.tar.gz</strong> You can now navigate into the WorkflowExercise directory to view the full ingredients for Joe\u2019s Computing work. Review all of these files based upon the information below and refer back to it as you proceed through the remaining exercises (1.2-1.4). (If you've been drawing a diagram of Joe's work so far, feel free to annotate with some of this information.) Based upon Joe\u2019s description and submit files you determine the following details for each step: Permutation Step \u00b6 Note that the files below are the necessary files to submit the permutation calculations for one trait (in this case, trait 1). There are additional submit files to perform this step for the other traits. Submit file: permutation1.submit In the submit file: executable: runR.pl arguments 1_$(Process) run_perm.R 1 $(Process) 10000 input: run_perm.R , input.csv , RLIBS.tar.gz Altogether, the job(s) created by this submit file will execute the command: ./runR.pl 1_$(Process) run_perm.R 1 $(Process) 10000 The arguments mean the following: 1_$(Process) , used by runR.pl to name its log file run_perm.R , which R script we want to run 1 , trait column in input.csv (i.e. this is 3 for permutation3.submit ) $(Process) , used by run_perm.R for naming the permutation output 10000 , indicates that 10,000 permutations should be done for each individual job process As output, this job will create: a series of .log and .out files, named from the first argument ( 1_$(Process) ). This looks like: runR.1_0.out , runR.1_0.log , runR.1_0.err the actual output file, named using the 1 and $(Process) arguments, like so: perm_part.1_0.Rdat Combine Permutation Output \u00b6 tarit.sh is run after each trait's Permutation step (with the column number as an argument) to compress perm_part.1_0.Rdat (or potentially multiple such files named according to \"perm_part.1_*.Rdat\") for the QTL step. Sample execution: * ./tarit.sh 1 , where \"1\" is the trait column number. QTL Mapping Step \u00b6 Again, the files below are for submitting the QTL calculations for one trait (in this case, trait 1). There are additional submit files to perform this step for the other traits. Submit file: qtl1.submit In the submit file: executable: runR.pl arguments qtl_1 qtl.R 1 input: qtl.R , input.csv , RLIBS.tar.gz , perm_combined_1.tar.gz (created by tarit.sh ) Altogether, the job(s) created by this submit file will execute the command: ./runR.pl qtl_1 qtl.R 1 The arguments mean the following: qtl_1 : used by runR.pl to name its log / output / error files qtl.R : which R script we want to run 1 : trait column in input.csv , will also be used to name the output files As output, this job will create: a series of .log and .out files, named from the first argument ( qtl_1 ). several output files named for the trait argument 1 : perm_combined_1.Rdat , perm_summary_1.txt , sim_geno_results_1.Rdat , qtl_1.Rdat , refined_qtl_summary_1.txt , refined qtl_1.Rdat , and fit_qtl_results_1.Rdat Combine All Output \u00b6 results_1.tar.gz is made by running taritall.sh (with the column number as an argument) after the QTL step for that trait finishes, where \"1\" reflects the trait column number in the output above. Sample execution: ./taritall.sh 1 , where \"1\" is the trait column number.","title":"1.1. Learn workflow"},{"location":"materials/day5/part1-ex1-science-intro/#friday-exercise-11-learn-about-joes-desired-computing-work","text":"In today's set of hands-on exercises, you'll be examining a set of pre-existing job submissions (scripts and submit files) and then coordinating them to run as a single workflow using a DAG. Along the way, you'll need to test some of the already written submit files. The first part of this exercise introduces the overall steps for the workflow and what pieces already exist (that you won't need to write yourself).","title":"Friday Exercise 1.1: Learn about Joe's Desired Computing Work"},{"location":"materials/day5/part1-ex1-science-intro/#introduction","text":"Joe Biologist studies the influence of genetic differences (genotypes) on the growth traits (phenotypes) of a common crop. He has come to you with some computing work he\u2019d like to automate as a workflow, so that he can scale out his research and run it on a much larger HTC compute system. He has used HTCondor to manually submit individual jobs (never used a DAG), but has only run jobs on a smaller, dedicated HTCondor cluster where he doesn\u2019t have to request CPU, disk, or RAM. He\u2019s not familiar with DAGman. He's getting annoyed by how many manual job submissions and summary scripts he has to run, especially as he's thinking of scaling out his work in a way that would be much more cumbersome - so he's asking you to help him organize all the steps into an optimized DAG workflow. (As you learn more details below, it might be helpful to draw a diagram that describes the steps of his workflow.)","title":"Introduction"},{"location":"materials/day5/part1-ex1-science-intro/#after-asking-joe-many-questions-you-realize-the-following-things","text":"A) Joe starts with a file called input.csv that was made on his desktop. When you open this file in excel, Joe shows you that each row corresponds to an individual plant. For each individual, the first three columns are phenotype measurements for each of three different growth traits (T240, T300, T360), and the remaining columns contain the genotype (A or B) of that individual across multiple genes (columns). B) At present, Joe does the following manual steps (for each of the three traits) to achieve a meaningful result from the data in input.csv : Joe first submits a job that calculates a large number of hypothetical permutations of phenotype-genotype combinations for the trait. This produces a set of files containing all the permutations for that trait. 2. Joe then runs a script for each trait, to create a gzipped tar file of the permutation job results for that trait. 3. Joe then submits a second job for each trait, which uses the permutations that have been zipped together to perform a QTL mapping calculation. This calculation determines which genes were most important for the phenotype trait, relative to other possible genes exemplified in the random permutations. 4. As a final step for each trait, Joe runs a script that creates a gzipped tar file of all of the permutation and QTL mapping data for that trait, so that he can take it back to his desktop computer for final processing. C) For the permutation and QTL mapping calculations, Joe runs a special wrapper that then uses his main R scripts. For each type of calculation this perl script, runR.pl , first installs R using files from Joe's proxy server and then sets up the R environment before running whatever R script has been indicated as an argument to runR.pl . Therefore, the runR.pl wrapper script is run as the HTCondor executable with the name of either R program as an argument to the wrapper (along with arguments to the R program, itself). D) The two tar steps that happen after the permutation and QTL mapping steps are simple shell scripts containing a relevant tar command, and can be run on the submit server because they create little CPU load and finish very quickly. E) When Joe has previously run the first step, of 10,000 permutations, as one Condor job, the job runs for hours on his own small cluster. He has cut the permutations of a single Condor job up into multiple, shorter Condor jobs of fewer permutations before, but he doesn\u2019t remember the details of how many jobs and how many permutations per job worked well. Furthermore, Joe would like to scale up to 100,000 permutations per trait this time. Unlike the permutation step, the QTL step finishes quite quickly; even with 10,000 permutations completed for each trait, the QTL step completes within several minutes.","title":"After asking Joe many questions, you realize the following things:"},{"location":"materials/day5/part1-ex1-science-intro/#view-joes-files","text":"(but don't do anything with them until the next exercise , where you'll plan necessary developments of a workflow for Joe.) Log in to learn.chtc.wisc.edu and move to a desired location in your home directory. Enter the following commands to download and decompress/untar Joe\u2019s job ingredients: %UCL_PROMPT_SHORT% <strong>wget http://proxy.chtc.wisc.edu/SQUID/osgschool17/WorkflowExercise.tar.gz</strong> %UCL_PROMPT_SHORT% <strong>tar -xzf WorkflowExercise.tar.gz</strong> You can now navigate into the WorkflowExercise directory to view the full ingredients for Joe\u2019s Computing work. Review all of these files based upon the information below and refer back to it as you proceed through the remaining exercises (1.2-1.4). (If you've been drawing a diagram of Joe's work so far, feel free to annotate with some of this information.) Based upon Joe\u2019s description and submit files you determine the following details for each step:","title":"View Joe's files"},{"location":"materials/day5/part1-ex1-science-intro/#permutation-step","text":"Note that the files below are the necessary files to submit the permutation calculations for one trait (in this case, trait 1). There are additional submit files to perform this step for the other traits. Submit file: permutation1.submit In the submit file: executable: runR.pl arguments 1_$(Process) run_perm.R 1 $(Process) 10000 input: run_perm.R , input.csv , RLIBS.tar.gz Altogether, the job(s) created by this submit file will execute the command: ./runR.pl 1_$(Process) run_perm.R 1 $(Process) 10000 The arguments mean the following: 1_$(Process) , used by runR.pl to name its log file run_perm.R , which R script we want to run 1 , trait column in input.csv (i.e. this is 3 for permutation3.submit ) $(Process) , used by run_perm.R for naming the permutation output 10000 , indicates that 10,000 permutations should be done for each individual job process As output, this job will create: a series of .log and .out files, named from the first argument ( 1_$(Process) ). This looks like: runR.1_0.out , runR.1_0.log , runR.1_0.err the actual output file, named using the 1 and $(Process) arguments, like so: perm_part.1_0.Rdat","title":"Permutation Step"},{"location":"materials/day5/part1-ex1-science-intro/#combine-permutation-output","text":"tarit.sh is run after each trait's Permutation step (with the column number as an argument) to compress perm_part.1_0.Rdat (or potentially multiple such files named according to \"perm_part.1_*.Rdat\") for the QTL step. Sample execution: * ./tarit.sh 1 , where \"1\" is the trait column number.","title":"Combine Permutation Output"},{"location":"materials/day5/part1-ex1-science-intro/#qtl-mapping-step","text":"Again, the files below are for submitting the QTL calculations for one trait (in this case, trait 1). There are additional submit files to perform this step for the other traits. Submit file: qtl1.submit In the submit file: executable: runR.pl arguments qtl_1 qtl.R 1 input: qtl.R , input.csv , RLIBS.tar.gz , perm_combined_1.tar.gz (created by tarit.sh ) Altogether, the job(s) created by this submit file will execute the command: ./runR.pl qtl_1 qtl.R 1 The arguments mean the following: qtl_1 : used by runR.pl to name its log / output / error files qtl.R : which R script we want to run 1 : trait column in input.csv , will also be used to name the output files As output, this job will create: a series of .log and .out files, named from the first argument ( qtl_1 ). several output files named for the trait argument 1 : perm_combined_1.Rdat , perm_summary_1.txt , sim_geno_results_1.Rdat , qtl_1.Rdat , refined_qtl_summary_1.txt , refined qtl_1.Rdat , and fit_qtl_results_1.Rdat","title":"QTL Mapping Step"},{"location":"materials/day5/part1-ex1-science-intro/#combine-all-output","text":"results_1.tar.gz is made by running taritall.sh (with the column number as an argument) after the QTL step for that trait finishes, where \"1\" reflects the trait column number in the output above. Sample execution: ./taritall.sh 1 , where \"1\" is the trait column number.","title":"Combine All Output"},{"location":"materials/day5/part1-ex2-plan-workflow/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Friday Exercise 1.2: Plan Joe's Workflow \u00b6 This exercise outlines our goal (creating a production workflow) and the steps needed to achieve it. Before starting this section, make sure to first read Exercise 1.1 , which has important background information about Joe's intended work and how he has submitted jobs so far. Your mission \u00b6 Your goal is to plan out Joe\u2019s workflow based upon small-scale test jobs and, later, to write a full-scale DAG to actually run his workflow in production. In particular, you will need to do the following: Optimize the submit files for the permutation jobs of each trait, such that each trait takes advantage of high-throughput parallelization with some number of job processes . Each process should calculate a portion of the new total of 100,000 permutations per trait (versus the 10,000 permutations per trait that Joe has been working with previously). Optimize submit file values for \"request_memory\" and \"request_cpus\" for each permutation and QTL mapping step, for each trait of the three traits. Create a single DAG file, with permutation and QTL mapping jobs for each of the three traits, including PRE and/or POST scripts for the tar scripts that need to be run. We'll be working on the first two optimization steps during this session, and the second optimization step and DAG creation after the break, in Exercise 1.3 . *NOTE: In what follows, the only files you will need to modify are the submit files (and as specifically instructed). You will not need to modify any of the input files, output files, or scripts/programs. It is advisable that you split up some of the work within your pair or group in order to be time-efficient. \u00b6 Draw a diagram \u00b6 Based upon what you learned from Joe , draw the general workflow that you would make for Joe (on paper), keeping in mind that there are 3 traits for which the permutation and QTL mapping steps need to be completed. (If you started drawing a diagram while reading the previous page, just extend it here). The tar steps will probably need to be PRE or POST scripts (you decide which is best). Think about what Joe's intended workflow means for the shape of the DAG, including PARENT-CHILD dependencies for JOBs in the DAG and the fact that the permutation step could be broken up into multiple processes of fewer total permutations, each. We'll come back to this diagram in the next exercise after the break, when it's time to construct the full DAG. Optimize job components \u00b6 Before we assemble the full DAG, we want to optimize each component of the workflow. This will include 3 optimization steps: 1. Test the HTC optimization of the permutation step. \u00b6 You will need to determine the number of permutations that should be batched in a single job process, if each process needs to run in ~30 minutes for good HTC scaling. To do this, you will need to run some test jobs, to see how long it takes a single job process to run, say, 10, 100, or 1000 permutations. To run these test jobs: Modify one of the permutation submit files to \u201cqueue\u201d 10 jobs (so that you can average time between the 10 test jobs) Add \"request_cpus = 1\" according to Joe's indication Add reasonable first guesses for \"request_memory\" and \"request_disk\" (say, 1 GB?). Make a few copies of this submit file so that you can change the last argument (the number of permutations) from \"10000\" to \"10\", \"100\", or \"1000\". For time's sake, a member of your group should test all three of these variations at the same time! Getting ready for the next step: After each set of permutation tests finishes, you\u2019ll need to use tarit.sh (with the correct argument) before running the test jobs for the QTL step. 2. Test each of the QTL mapping jobs \u00b6 Once you have results from your previous permutation step testing, you can start optimizing the QTL mapping jobs. Submit the three submit files (one for each trait) after adding lines for \"request_memory\", \"request_disk\", and \"request_cpus\". The resource needs (RAM and disk space) and execution time of each QTL mapping job will likely increase with the total number of permutations from the previous permutation step, though the execution time will likely still be short (according to Joe). You'll test optimized permutation and QTL mapping steps later on. 3. Optimization planning \u00b6 In order to optimize Joe's overall workflow, we can optimize the following values, based on our test jobs from steps 1 and 2: Permutation throughput: Calculate the number of permutations that should be run per job process , such that the runtimes per process will be about 30 minutes (not exact, but closer to 30 than to 5 or 60). You can then calculate the number of processes that should be queued such that 100,000 permutations are calculated for each trait. Essentially, you want job processes X permutations to equal 100,000 total permutations for each of the three phenotype traits. *Hint: You can use the \"condor_history\" feature (similar to condor_q, but for completed jobs) to easily view and compare the \"RUNTIME\" for jobs in a \"cluster\" (using the cluster value as an argument to condor_history . Memory and disk for both steps: Make sure to examine the log files of your permutation and QTL test jobs, so that you can extrapolate how much memory and disk should be requested in the submit files for the full-scale DAG. When you're done with #3, move on to Exercise 1.3 \u00b6","title":"1.2. Plan workflow"},{"location":"materials/day5/part1-ex2-plan-workflow/#friday-exercise-12-plan-joes-workflow","text":"This exercise outlines our goal (creating a production workflow) and the steps needed to achieve it. Before starting this section, make sure to first read Exercise 1.1 , which has important background information about Joe's intended work and how he has submitted jobs so far.","title":"Friday Exercise 1.2: Plan Joe's Workflow"},{"location":"materials/day5/part1-ex2-plan-workflow/#your-mission","text":"Your goal is to plan out Joe\u2019s workflow based upon small-scale test jobs and, later, to write a full-scale DAG to actually run his workflow in production. In particular, you will need to do the following: Optimize the submit files for the permutation jobs of each trait, such that each trait takes advantage of high-throughput parallelization with some number of job processes . Each process should calculate a portion of the new total of 100,000 permutations per trait (versus the 10,000 permutations per trait that Joe has been working with previously). Optimize submit file values for \"request_memory\" and \"request_cpus\" for each permutation and QTL mapping step, for each trait of the three traits. Create a single DAG file, with permutation and QTL mapping jobs for each of the three traits, including PRE and/or POST scripts for the tar scripts that need to be run. We'll be working on the first two optimization steps during this session, and the second optimization step and DAG creation after the break, in Exercise 1.3 .","title":"Your mission"},{"location":"materials/day5/part1-ex2-plan-workflow/#note-in-what-follows-the-only-files-you-will-need-to-modify-are-the-submit-files-and-as-specifically-instructed-you-will-not-need-to-modify-any-of-the-input-files-output-files-or-scriptsprograms-it-is-advisable-that-you-split-up-some-of-the-work-within-your-pair-or-group-in-order-to-be-time-efficient","text":"","title":"*NOTE: In what follows, the only files you will need to modify are the submit files (and as specifically instructed). You will not need to modify any of the input files, output files, or scripts/programs. It is advisable that you split up some of the work within your pair or group in order to be time-efficient."},{"location":"materials/day5/part1-ex2-plan-workflow/#draw-a-diagram","text":"Based upon what you learned from Joe , draw the general workflow that you would make for Joe (on paper), keeping in mind that there are 3 traits for which the permutation and QTL mapping steps need to be completed. (If you started drawing a diagram while reading the previous page, just extend it here). The tar steps will probably need to be PRE or POST scripts (you decide which is best). Think about what Joe's intended workflow means for the shape of the DAG, including PARENT-CHILD dependencies for JOBs in the DAG and the fact that the permutation step could be broken up into multiple processes of fewer total permutations, each. We'll come back to this diagram in the next exercise after the break, when it's time to construct the full DAG.","title":"Draw a diagram"},{"location":"materials/day5/part1-ex2-plan-workflow/#optimize-job-components","text":"Before we assemble the full DAG, we want to optimize each component of the workflow. This will include 3 optimization steps:","title":"Optimize job components"},{"location":"materials/day5/part1-ex2-plan-workflow/#1-test-the-htc-optimization-of-the-permutation-step","text":"You will need to determine the number of permutations that should be batched in a single job process, if each process needs to run in ~30 minutes for good HTC scaling. To do this, you will need to run some test jobs, to see how long it takes a single job process to run, say, 10, 100, or 1000 permutations. To run these test jobs: Modify one of the permutation submit files to \u201cqueue\u201d 10 jobs (so that you can average time between the 10 test jobs) Add \"request_cpus = 1\" according to Joe's indication Add reasonable first guesses for \"request_memory\" and \"request_disk\" (say, 1 GB?). Make a few copies of this submit file so that you can change the last argument (the number of permutations) from \"10000\" to \"10\", \"100\", or \"1000\". For time's sake, a member of your group should test all three of these variations at the same time! Getting ready for the next step: After each set of permutation tests finishes, you\u2019ll need to use tarit.sh (with the correct argument) before running the test jobs for the QTL step.","title":"1. Test the HTC optimization of the permutation step."},{"location":"materials/day5/part1-ex2-plan-workflow/#2-test-each-of-the-qtl-mapping-jobs","text":"Once you have results from your previous permutation step testing, you can start optimizing the QTL mapping jobs. Submit the three submit files (one for each trait) after adding lines for \"request_memory\", \"request_disk\", and \"request_cpus\". The resource needs (RAM and disk space) and execution time of each QTL mapping job will likely increase with the total number of permutations from the previous permutation step, though the execution time will likely still be short (according to Joe). You'll test optimized permutation and QTL mapping steps later on.","title":"2. Test each of the QTL mapping jobs"},{"location":"materials/day5/part1-ex2-plan-workflow/#3-optimization-planning","text":"In order to optimize Joe's overall workflow, we can optimize the following values, based on our test jobs from steps 1 and 2: Permutation throughput: Calculate the number of permutations that should be run per job process , such that the runtimes per process will be about 30 minutes (not exact, but closer to 30 than to 5 or 60). You can then calculate the number of processes that should be queued such that 100,000 permutations are calculated for each trait. Essentially, you want job processes X permutations to equal 100,000 total permutations for each of the three phenotype traits. *Hint: You can use the \"condor_history\" feature (similar to condor_q, but for completed jobs) to easily view and compare the \"RUNTIME\" for jobs in a \"cluster\" (using the cluster value as an argument to condor_history . Memory and disk for both steps: Make sure to examine the log files of your permutation and QTL test jobs, so that you can extrapolate how much memory and disk should be requested in the submit files for the full-scale DAG.","title":"3. Optimization planning"},{"location":"materials/day5/part1-ex2-plan-workflow/#when-youre-done-with-3-move-on-to-exercise-13","text":"","title":"When you're done with #3, move on to Exercise 1.3"},{"location":"materials/day5/part2-ex1-execute-workflow/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Friday Exercise 1.3: Execute a Production Workflow \u00b6 In this exercise, you will: finish testing workflow steps, write the DAG for the workflow you planned, and submit this workflow on learn.chtc.wisc.edu There are bonus tasks in Exercise 1.4 , if you get through this part quickly, including running the workflow on the Open Science Grid. Steps to Take \u00b6 Note: While one person in your group works on step 1, someone else can work on step 2. 1. Finish optimizing job components \u00b6 In the previous exercise we ran some tests to find the optimal values for the permutation and qtl job steps. Now we want to implement these values and confirm that they work. The steps to take are as follows: Based on your previous tests, modify the permutation submit file for each trait according to: the optimal number of job processes (how many jobs to queue ) number of permutations-per-process (one of the arguments to the run_perm.R script) requests for memory and disk Submit one of these newly modified permutation submit files in preparation for an optimized QTL test. If your estimate of the necessary permutations per process (for a ~30-minute run time) was not close enough, modify and test the permutation jobs again. Once the permutation jobs complete successfully, finish preparing for an optimized QTL step by using tarit.sh to package the output from the test permutation step just above. Then run the QTL job. As above, you will only need to submit one of the QTL submit files to confirm the approximate memory and disk needed. Make sure all of the desired output files for the QTL step are created to confirm success. Don't forget to test taritall.sh after a successful QTL job, to make sure it works as expected. After the optimized permutation and QTL tests, copy all output, error, and log files to a new directory to prepare for the production workflow. 2. Create a DAG \u00b6 Write a single DAG file for the workflow, including: JOB lines for each submit file PARENT x CHILD y lines as necessary SCRIPT PRE and/or SCRIPT POST lines for the tar steps Note: You may need to think about how each tar step works for deciding on \"PRE\" or \"POST\" scripts for each. If you need a refresher on what a DAG looks like, see this exercise from Monday or the HTCondor manual To quickly check that you've got the details of the DAG correct, modify your permutation submit files to submit only a few jobs, and with each job performing a much smaller number of permutations (say, ~10?). 3. Run a production workflow \u00b6 Once you have run a quick test of the DAG and you know all the steps are working together, you can submit a full-scale run of the DAG! To do so, modify the permutation submit files to the values you chose when optimizing the permutation step (remember, this should be about 100,000 total permutations for each trait) and then submit the DAG. If you have any issues, consult the log and out files for the DAG and jobs, and modify your approach at any of the previous steps. While the full-scale DAG is running, you may wish to further detail your drawn workflow, including information regarding resource usage. Share all submit and DAG files with one another so everyone has a copy. If you have time (even while step 3 is running smoothly), move on to the Bonus Tasks in Exercise 1.4","title":"1.3. Execute workflow"},{"location":"materials/day5/part2-ex1-execute-workflow/#friday-exercise-13-execute-a-production-workflow","text":"In this exercise, you will: finish testing workflow steps, write the DAG for the workflow you planned, and submit this workflow on learn.chtc.wisc.edu There are bonus tasks in Exercise 1.4 , if you get through this part quickly, including running the workflow on the Open Science Grid.","title":"Friday Exercise 1.3: Execute a Production Workflow"},{"location":"materials/day5/part2-ex1-execute-workflow/#steps-to-take","text":"Note: While one person in your group works on step 1, someone else can work on step 2.","title":"Steps to Take"},{"location":"materials/day5/part2-ex1-execute-workflow/#1-finish-optimizing-job-components","text":"In the previous exercise we ran some tests to find the optimal values for the permutation and qtl job steps. Now we want to implement these values and confirm that they work. The steps to take are as follows: Based on your previous tests, modify the permutation submit file for each trait according to: the optimal number of job processes (how many jobs to queue ) number of permutations-per-process (one of the arguments to the run_perm.R script) requests for memory and disk Submit one of these newly modified permutation submit files in preparation for an optimized QTL test. If your estimate of the necessary permutations per process (for a ~30-minute run time) was not close enough, modify and test the permutation jobs again. Once the permutation jobs complete successfully, finish preparing for an optimized QTL step by using tarit.sh to package the output from the test permutation step just above. Then run the QTL job. As above, you will only need to submit one of the QTL submit files to confirm the approximate memory and disk needed. Make sure all of the desired output files for the QTL step are created to confirm success. Don't forget to test taritall.sh after a successful QTL job, to make sure it works as expected. After the optimized permutation and QTL tests, copy all output, error, and log files to a new directory to prepare for the production workflow.","title":"1. Finish optimizing job components"},{"location":"materials/day5/part2-ex1-execute-workflow/#2-create-a-dag","text":"Write a single DAG file for the workflow, including: JOB lines for each submit file PARENT x CHILD y lines as necessary SCRIPT PRE and/or SCRIPT POST lines for the tar steps Note: You may need to think about how each tar step works for deciding on \"PRE\" or \"POST\" scripts for each. If you need a refresher on what a DAG looks like, see this exercise from Monday or the HTCondor manual To quickly check that you've got the details of the DAG correct, modify your permutation submit files to submit only a few jobs, and with each job performing a much smaller number of permutations (say, ~10?).","title":"2. Create a DAG"},{"location":"materials/day5/part2-ex1-execute-workflow/#3-run-a-production-workflow","text":"Once you have run a quick test of the DAG and you know all the steps are working together, you can submit a full-scale run of the DAG! To do so, modify the permutation submit files to the values you chose when optimizing the permutation step (remember, this should be about 100,000 total permutations for each trait) and then submit the DAG. If you have any issues, consult the log and out files for the DAG and jobs, and modify your approach at any of the previous steps. While the full-scale DAG is running, you may wish to further detail your drawn workflow, including information regarding resource usage. Share all submit and DAG files with one another so everyone has a copy. If you have time (even while step 3 is running smoothly), move on to the Bonus Tasks in Exercise 1.4","title":"3. Run a production workflow"},{"location":"materials/day5/part2-ex2-workflow-tuning/","text":"pre em { font-style: normal; background-color: yellow; } pre strong { font-style: normal; font-weight: bold; color: \\#008; } Friday Exercise 1.4: Workflow Optimization and Scaling \u00b6 If you finish the entire workflow and are thirsty for more, try any of the following in whatever order you like: Bonus 1 \u00b6 Rerun the DAG again with four times the permutations per job (but fewer processes, keeping a total of 100,000 permutations per trait). Which DAG finished in an overall faster time? Why? Bonus 2 \u00b6 You probably noticed that the job processes from the permutation step create many log, out, and error files. Modify the permutation submit files to better organize these files into subdirectories (check out HTCondor's IntitialDir feature and/or DAG's DIR features). You may wish to always test the DAG using fewer permutations and permutations processes for a quick turnaround. Bonus 3 \u00b6 Take the workflow to the submit server for the Open Science Grid ( osg-learn.chtc.wisc.edu ), and run it there. What happens? Do all of the jobs complete successfully? (Check for the desired output files and examine the runR_*.out files, which are created by the runR.pl wrapper and indicate any errors from the R script, even if HTCondor thought the job was successful.) If there are any errors (likely related to machine differences and/or software dependencies), implement a RETRY for jobs that fail, accounting for the fact that a DAG RETRY statement applies to an entire submit file ( JOB ), while you might actually need to account for process-specific errors.","title":"1.4. (opt) Tune workflow"},{"location":"materials/day5/part2-ex2-workflow-tuning/#friday-exercise-14-workflow-optimization-and-scaling","text":"If you finish the entire workflow and are thirsty for more, try any of the following in whatever order you like:","title":"Friday Exercise 1.4: Workflow Optimization and Scaling"},{"location":"materials/day5/part2-ex2-workflow-tuning/#bonus-1","text":"Rerun the DAG again with four times the permutations per job (but fewer processes, keeping a total of 100,000 permutations per trait). Which DAG finished in an overall faster time? Why?","title":"Bonus 1"},{"location":"materials/day5/part2-ex2-workflow-tuning/#bonus-2","text":"You probably noticed that the job processes from the permutation step create many log, out, and error files. Modify the permutation submit files to better organize these files into subdirectories (check out HTCondor's IntitialDir feature and/or DAG's DIR features). You may wish to always test the DAG using fewer permutations and permutations processes for a quick turnaround.","title":"Bonus 2"},{"location":"materials/day5/part2-ex2-workflow-tuning/#bonus-3","text":"Take the workflow to the submit server for the Open Science Grid ( osg-learn.chtc.wisc.edu ), and run it there. What happens? Do all of the jobs complete successfully? (Check for the desired output files and examine the runR_*.out files, which are created by the runR.pl wrapper and indicate any errors from the R script, even if HTCondor thought the job was successful.) If there are any errors (likely related to machine differences and/or software dependencies), implement a RETRY for jobs that fail, accounting for the fact that a DAG RETRY statement applies to an entire submit file ( JOB ), while you might actually need to account for process-specific errors.","title":"Bonus 3"}]}